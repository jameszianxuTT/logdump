WARNING:root:Defaulting to PJRT_DEVICE=CPU
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.4.1, pluggy-1.6.0 -- /localdev/jameszianxu/gen_mc/tt-torch/env/venv/bin/python3.10
cachedir: .pytest_cache
rootdir: /localdev/jameszianxu/gen_mc/tt-torch
configfile: pyproject.toml
plugins: cov-6.2.1, forked-1.6.0, xdist-3.8.0, split-0.10.0
collecting ... collected 1 item

tests/models/llama/test_llama3_generative.py::test_llama3_generate Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 52.71it/s]
2025-08-13 23:25:01.921 (   0.000s) [        CA99B1C0]      dylib_platform.cc:47       1| DylibPlatform::SubclassInitialize
2025-08-13 23:25:01.923 (   0.001s) [        CA99B1C0]     client_instance.cc:39       1| ClientInstance::ClientInstance
2025-08-13 23:25:01.923 (   0.001s) [        CA99B1C0]              client.cc:18       1| TTClientInstance::TTClientInstance
2025-08-13 23:25:01.923 (   0.001s) [        CA99B1C0]     client_instance.cc:60       1| ClientInstance::Initialize
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]              stubs.inc:112   WARN| STUB: PJRT_Client_TopologyDescription
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     client_instance.cc:383      1| ClientInstance::PJRT_Client_PlatformVersion
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     client_instance.cc:363      1| ClientInstance::PJRT_Client_PlatformName
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     client_instance.cc:395      1| ClientInstance::PJRT_Client_Devices
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     device_instance.cc:44       1| DeviceInstance::PJRT_Device_GetDescription
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]  device_description.cc:63       1| DeviceDescription::PJRT_DeviceDescription_Attributes
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     client_instance.cc:408      1| ClientInstance::PJRT_Client_AddressableDevices
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     client_instance.cc:458      1| ClientInstance::PJRT_Client_AddressableMemories
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     device_instance.cc:71       1| DeviceInstance::PJRT_Device_AddressableMemories
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]     memory_instance.cc:124      1| MemoryInstance::PJRT_Memory_AddressableByDevices
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]        api_bindings.cc:76       1| PJRT_Plugin_Attributes
2025-08-13 23:25:05.179669: W torch_xla/csrc/runtime/profiler.cpp:88] Profiler API not found for PJRT plugin
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:05.179 (   3.257s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:05.179 (   3.258s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:05.179 (   3.258s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:05.179 (   3.258s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:05.179 (   3.258s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:05.179 (   3.258s) [        CA99B1C0]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-08-13 23:25:05.179 (   3.258s) [        CA99B1C0]  device_description.cc:99       1| DeviceDescription::PJRT_DeviceDescription_ToString
2025-08-13 23:25:05.179 (   3.258s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:05.179 (   3.258s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:05.179 (   3.258s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:05.179 (   3.258s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
[James] Manually forwarding attention mask
Initial prompt: '<|begin_of_text|>I like taking walks in the'
Setting up XLA environment...
XLA environment configured.
Created device mesh: (1, 2) with 2 devices
[SHARD_DEBUG] ADDED tensor: id=140716026138592, shape=[1, 7], dtype=torch.int64, device=cpu
[SHARD_DEBUG]   shard_spec: (None, None)
[SHARD_DEBUG] ADDED tensor: id=140716025109200, shape=[7], dtype=torch.int64, device=cpu
[SHARD_DEBUG]   shard_spec: None
[SHARD_DEBUG] ADDED tensor: id=140716026146992, shape=[1, 8, 128, 128], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'batch', None, None)
[SHARD_DEBUG] ADDED tensor: id=140716026145072, shape=[1, 8, 128, 128], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'batch', None, None)
[SHARD_DEBUG] ADDED tensor: id=140715232194416, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232193056, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232192496, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232198496, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232196096, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232191216, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232190016, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232195136, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232190576, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232189216, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232065184, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232197616, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232067264, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232063024, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715244815760, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232164928, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232153168, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715244818320, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715244818560, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231705696, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715244818160, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715231700016, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231699856, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231698656, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715231706336, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231699696, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231698496, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231706256, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715231698176, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231698016, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231703936, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715231698336, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231700096, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231698976, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231702016, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715231706576, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231703856, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231699776, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140716026996640, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716026996720, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716026995520, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716026999840, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140716023643200, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023625056, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716026996240, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140716023638480, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023643520, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023630480, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023642320, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140716023640480, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023633520, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715230277568, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140716023638000, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023640240, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716024700960, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231698816, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232190656, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023639920, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023615136, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140716023641280, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023641440, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231702096, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023634320, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140716023640320, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023639840, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023632080, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140716023644000, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023641520, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023640640, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023643920, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232065104, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232053344, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232060384, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232056064, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232066384, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232059664, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232062464, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232055504, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232056224, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232053104, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232057424, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232053664, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232055904, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232061584, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232055424, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232059984, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232051824, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715233137008, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232051744, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232061024, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232057744, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232062624, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232064544, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232062304, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232059904, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232061264, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232063664, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232065824, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232057264, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716025215504, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232058944, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232059344, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232059104, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232157648, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232057104, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232165408, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232158368, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232165728, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232165248, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232154368, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232150128, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232191056, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232156288, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232150208, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232149568, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232153648, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232154768, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232155008, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232161088, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232157488, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232151648, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232150688, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232164608, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232162768, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232161648, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232155328, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232160448, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232158208, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232153888, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232158528, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232163808, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232155968, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232163168, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232163568, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232163328, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232165328, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232851696, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232051504, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232154288, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232161248, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715231699136, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231699536, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232236128, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715231698256, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231706176, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231706976, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231698736, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232245808, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232237648, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232234288, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232242608, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232246448, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232134704, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232247728, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232136464, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232143504, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232148544, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232145424, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232143024, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232140944, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232134784, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232148064, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232138864, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232134304, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232146464, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232136304, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232138624, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232137424, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232136784, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232143424, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232146624, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232135344, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232145664, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232143824, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232144544, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232139584, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232142784, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140712235723296, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232137584, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232133344, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232143104, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232149424, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232135584, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232144384, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715232142304, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140716023626176, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023622096, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716024832992, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716023616816, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140716026892016, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716024828032, shape=[8192, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140716024827152, shape=[3072, 8192], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140716024550624, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231682512, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231674992, shape=[1024, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
[SHARD_DEBUG] ADDED tensor: id=140715231683312, shape=[3072, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: (None, 'model')
[SHARD_DEBUG] ADDED tensor: id=140715232188816, shape=[128256, 3072], dtype=torch.bfloat16, device=cpu
[SHARD_DEBUG]   shard_spec: ('model', None)
<|begin_of_text|>I like taking walks in theNote: Using experimental XLA backend.
[James] disable rectify buffer inplace copy
[SHARD_DEBUG] GET tensor: id=140715232198416, shape=[3072], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for L__self___model_layers__modules__0___input_layernorm_weight
[SHARD_DEBUG] GET tensor: id=140715232183856, shape=[3072], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for L__self___model_layers__modules__0___post_attention_layernorm_weight
[SHARD_DEBUG] GET tensor: id=140715231682672, shape=[3072], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for L__self___model_norm_weight
[SHARD_DEBUG] GET tensor: id=140715232188816, shape=[128256, 3072], dtype=torch.bfloat16 -> shard_spec: ('model', None)
2025-08-13 23:25:07.132 (   5.210s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.132 (   5.210s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.216 (   5.294s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.216 (   5.294s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.216 (   5.294s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.216 (   5.295s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [64128, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.217 (   5.295s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.217 (   5.295s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.217 (   5.295s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.217 (   5.295s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.217 (   5.295s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.217 (   5.295s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [64128, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.217 (   5.295s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.217 (   5.295s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying cached shard_spec ('model', None) to L__self___lm_head.weight
[Cache] Cached new tensor for L__self___lm_head.weight
[SHARD_DEBUG] GET tensor: id=140711049670192, shape=[128], dtype=torch.int64 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.0
[SHARD_DEBUG] GET tensor: id=140711049670352, shape=[7, 128], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.1
[SHARD_DEBUG] GET tensor: id=140711049674512, shape=[1, 64, 1], dtype=torch.float32 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.2
[SHARD_DEBUG] GET tensor: id=140711049674672, shape=[3072, 3072], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.3
[SHARD_DEBUG] GET tensor: id=140711049668032, shape=[3072, 1024], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.4
[SHARD_DEBUG] GET tensor: id=140711049670752, shape=[3072, 1024], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.5
[SHARD_DEBUG] GET tensor: id=140711049667872, shape=[3072, 3072], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.6
[SHARD_DEBUG] GET tensor: id=140711049670912, shape=[3072, 8192], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.7
[SHARD_DEBUG] GET tensor: id=140711049674112, shape=[3072, 8192], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.8
[SHARD_DEBUG] GET tensor: id=140711049674272, shape=[8192, 3072], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.9
[SHARD_DEBUG] GET tensor: id=140711049673152, shape=[3072, 128256], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.10
[SHARD_DEBUG] GET tensor: id=140715232198496, shape=[3072, 3072], dtype=torch.bfloat16 -> shard_spec: ('model', None)
2025-08-13 23:25:07.375 (   5.453s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.375 (   5.453s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.378 (   5.456s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.378 (   5.456s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.378 (   5.456s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.378 (   5.457s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1536, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.378 (   5.457s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.378 (   5.457s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.379 (   5.457s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.379 (   5.457s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.379 (   5.457s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.379 (   5.457s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1536, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.379 (   5.457s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.379 (   5.457s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying cached shard_spec ('model', None) to const_subgraph_module.L__self___model_layers__modules__0___self_attn_q_proj.weight
[Cache] Cached new tensor for const_subgraph_module.L__self___model_layers__modules__0___self_attn_q_proj.weight
[SHARD_DEBUG] GET tensor: id=140715232196096, shape=[1024, 3072], dtype=torch.bfloat16 -> shard_spec: ('model', None)
2025-08-13 23:25:07.379 (   5.458s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.379 (   5.458s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.380 (   5.458s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.380 (   5.459s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.380 (   5.459s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.380 (   5.459s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [512, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.380 (   5.459s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.380 (   5.459s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.380 (   5.459s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.380 (   5.459s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.380 (   5.459s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.381 (   5.459s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [512, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.381 (   5.459s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.381 (   5.459s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying cached shard_spec ('model', None) to const_subgraph_module.L__self___model_layers__modules__0___self_attn_k_proj.weight
[Cache] Cached new tensor for const_subgraph_module.L__self___model_layers__modules__0___self_attn_k_proj.weight
[SHARD_DEBUG] GET tensor: id=140715232191216, shape=[1024, 3072], dtype=torch.bfloat16 -> shard_spec: ('model', None)
2025-08-13 23:25:07.381 (   5.460s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.381 (   5.460s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [512, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [512, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.383 (   5.461s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying cached shard_spec ('model', None) to const_subgraph_module.L__self___model_layers__modules__0___self_attn_v_proj.weight
[Cache] Cached new tensor for const_subgraph_module.L__self___model_layers__modules__0___self_attn_v_proj.weight
[SHARD_DEBUG] GET tensor: id=140715232190016, shape=[3072, 3072], dtype=torch.bfloat16 -> shard_spec: (None, 'model')
2025-08-13 23:25:07.385 (   5.463s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.385 (   5.463s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.388 (   5.467s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.388 (   5.467s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.388 (   5.467s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.389 (   5.467s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1536] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.389 (   5.467s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.389 (   5.467s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.389 (   5.467s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.389 (   5.467s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.389 (   5.467s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.389 (   5.467s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1536] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.389 (   5.467s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.389 (   5.467s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying cached shard_spec (None, 'model') to const_subgraph_module.L__self___model_layers__modules__0___self_attn_o_proj.weight
[Cache] Cached new tensor for const_subgraph_module.L__self___model_layers__modules__0___self_attn_o_proj.weight
[SHARD_DEBUG] GET tensor: id=140715232193056, shape=[8192, 3072], dtype=torch.bfloat16 -> shard_spec: ('model', None)
2025-08-13 23:25:07.392 (   5.471s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.392 (   5.471s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.399 (   5.477s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.399 (   5.478s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.399 (   5.478s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.399 (   5.478s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [4096, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.400 (   5.478s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.400 (   5.478s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.400 (   5.478s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.400 (   5.478s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.400 (   5.478s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.400 (   5.478s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [4096, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.400 (   5.478s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.400 (   5.478s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying cached shard_spec ('model', None) to const_subgraph_module.L__self___model_layers__modules__0___mlp_gate_proj.weight
[Cache] Cached new tensor for const_subgraph_module.L__self___model_layers__modules__0___mlp_gate_proj.weight
[SHARD_DEBUG] GET tensor: id=140715232194416, shape=[8192, 3072], dtype=torch.bfloat16 -> shard_spec: ('model', None)
2025-08-13 23:25:07.412 (   5.490s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.412 (   5.490s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.418 (   5.496s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.418 (   5.496s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.418 (   5.496s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.418 (   5.497s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [4096, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.418 (   5.497s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.418 (   5.497s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.418 (   5.497s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.419 (   5.497s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.419 (   5.497s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.419 (   5.497s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [4096, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.419 (   5.497s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.419 (   5.497s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying cached shard_spec ('model', None) to const_subgraph_module.L__self___model_layers__modules__0___mlp_up_proj.weight
[Cache] Cached new tensor for const_subgraph_module.L__self___model_layers__modules__0___mlp_up_proj.weight
[SHARD_DEBUG] GET tensor: id=140715232192496, shape=[3072, 8192], dtype=torch.bfloat16 -> shard_spec: (None, 'model')
2025-08-13 23:25:07.427 (   5.505s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.427 (   5.505s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.436 (   5.514s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.436 (   5.514s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.436 (   5.514s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.436 (   5.515s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 4096] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.436 (   5.515s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.436 (   5.515s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.436 (   5.515s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.436 (   5.515s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.437 (   5.515s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.437 (   5.515s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 4096] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.437 (   5.515s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.437 (   5.515s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying cached shard_spec (None, 'model') to const_subgraph_module.L__self___model_layers__modules__0___mlp_down_proj.weight
[Cache] Cached new tensor for const_subgraph_module.L__self___model_layers__modules__0___mlp_down_proj.weight
[SHARD_DEBUG] GET tensor: id=140715232188816, shape=[128256, 3072], dtype=torch.bfloat16 -> shard_spec: ('model', None)
2025-08-13 23:25:07.485 (   5.563s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.485 (   5.563s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.575 (   5.653s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.575 (   5.653s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.575 (   5.653s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.575 (   5.654s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [64128, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.575 (   5.654s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.575 (   5.654s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.576 (   5.654s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.576 (   5.654s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.576 (   5.654s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.576 (   5.654s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [64128, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.576 (   5.654s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.576 (   5.654s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying cached shard_spec ('model', None) to const_subgraph_module.L__self___lm_head.weight
[Cache] Cached new tensor for const_subgraph_module.L__self___lm_head.weight
[SHARD_DEBUG] GET tensor: id=140716026146992, shape=[1, 8, 128, 128], dtype=torch.bfloat16 -> shard_spec: (None, 'batch', None, None)
2025-08-13 23:25:07.671 (   5.750s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.671 (   5.750s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.672 (   5.750s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.672 (   5.750s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.672 (   5.750s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.672 (   5.750s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.672 (   5.750s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.672 (   5.750s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.672 (   5.751s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.672 (   5.751s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.672 (   5.751s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.672 (   5.751s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.672 (   5.751s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.672 (   5.751s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying cached shard_spec (None, 'batch', None, None) to kwargs____past_key_values___key_cache_0
[Cache] Cached new tensor for kwargs____past_key_values___key_cache_0
[SHARD_DEBUG] GET tensor: id=140716026145072, shape=[1, 8, 128, 128], dtype=torch.bfloat16 -> shard_spec: (None, 'batch', None, None)
2025-08-13 23:25:07.673 (   5.751s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.673 (   5.751s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.673 (   5.751s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.673 (   5.751s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.673 (   5.751s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.673 (   5.751s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.673 (   5.752s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.673 (   5.752s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.673 (   5.752s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.673 (   5.752s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.673 (   5.752s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.673 (   5.752s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 8, 128, 128] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.673 (   5.752s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.673 (   5.752s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying cached shard_spec (None, 'batch', None, None) to kwargs____past_key_values___value_cache_0
[Cache] Cached new tensor for kwargs____past_key_values___value_cache_0
[SHARD_DEBUG] GET tensor: id=140716026202448, shape=[64], dtype=torch.float32 -> NOT FOUND
[Cache] Cached new tensor for const_subgraph_module.L__self___model_rotary_emb_inv_freq
attempting retrieval of shard spec for inputs tensor([[128000,     40,   1093,   4737,  23291,    304,    279]])
[SHARD_DEBUG] GET tensor: id=140716026138592, shape=[1, 7], dtype=torch.int64 -> shard_spec: (None, None)
2025-08-13 23:25:07.719 (   5.797s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.719 (   5.797s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.719 (   5.797s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.719 (   5.797s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.719 (   5.797s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.719 (   5.797s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 7] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:07.719 (   5.798s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.719 (   5.798s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.719 (   5.798s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.719 (   5.798s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.719 (   5.798s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.719 (   5.798s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 7] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:07.719 (   5.798s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.719 (   5.798s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
[Sharding] Applying runtime shard_spec (None, None) to tensor shape torch.Size([1, 7])
attempting retrieval of shard spec for inputs tensor([0, 1, 2, 3, 4, 5, 6])
[SHARD_DEBUG] GET tensor: id=140716025109200, shape=[7], dtype=torch.int64 -> NOT FOUND
[Sharding] No shard_spec found for runtime tensor shape torch.Size([7]), skipping sharding

[XLA Cache] === DETAILED CACHE ANALYSIS ===
[XLA Cache] Total inputs to analyze: 28
[XLA Cache] Current cache contains 0 unique tensor keys
[XLA Cache] Input 0: input_0_shape_[3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140711054734448, shape=(3072,), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140711054734448
[XLA Cache]   XLA tensor id: 1
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 1: input_1_shape_[3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140711048747168, shape=(3072,), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140711048747168
[XLA Cache]   XLA tensor id: 2
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 2: input_2_shape_[3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140711052554416, shape=(3072,), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140711052554416
[XLA Cache]   XLA tensor id: 3
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 3: input_3_shape_[128256, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140711051624368, shape=(128256, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140711051624368
[XLA Cache]   XLA tensor id: 4
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 4: input_4_shape_[128]_dtype_torch.int64
[XLA Cache]   Cache key: id=140709176378128, shape=(128,), dtype=torch.int64, device=xla:0
[XLA Cache]   Tensor object id: 140709176378128
[XLA Cache]   XLA tensor id: 5
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 5: input_5_shape_[7, 128]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140709170807248, shape=(7, 128), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140709170807248
[XLA Cache]   XLA tensor id: 6
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 6: input_6_shape_[1, 64, 1]_dtype_torch.float32
[XLA Cache]   Cache key: id=140711051624208, shape=(1, 64, 1), dtype=torch.float32, device=xla:0
[XLA Cache]   Tensor object id: 140711051624208
[XLA Cache]   XLA tensor id: 7
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 7: input_7_shape_[3072, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366995584, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366995584
[XLA Cache]   XLA tensor id: 8
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 8: input_8_shape_[3072, 1024]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366995504, shape=(3072, 1024), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366995504
[XLA Cache]   XLA tensor id: 9
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 9: input_9_shape_[3072, 1024]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366995664, shape=(3072, 1024), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366995664
[XLA Cache]   XLA tensor id: 10
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 10: input_10_shape_[3072, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366995744, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366995744
[XLA Cache]   XLA tensor id: 11
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 11: input_11_shape_[3072, 8192]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366995824, shape=(3072, 8192), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366995824
[XLA Cache]   XLA tensor id: 12
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 12: input_12_shape_[3072, 8192]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366995904, shape=(3072, 8192), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366995904
[XLA Cache]   XLA tensor id: 13
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 13: input_13_shape_[8192, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366995984, shape=(8192, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366995984
[XLA Cache]   XLA tensor id: 14
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 14: input_14_shape_[3072, 128256]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996064, shape=(3072, 128256), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996064
[XLA Cache]   XLA tensor id: 15
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 15: input_15_shape_[3072, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996224, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996224
[XLA Cache]   XLA tensor id: 16
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 16: input_16_shape_[1024, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996144, shape=(1024, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996144
[XLA Cache]   XLA tensor id: 17
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 17: input_17_shape_[1024, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996304, shape=(1024, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996304
[XLA Cache]   XLA tensor id: 18
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 18: input_18_shape_[3072, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996384, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996384
[XLA Cache]   XLA tensor id: 19
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 19: input_19_shape_[8192, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996464, shape=(8192, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996464
[XLA Cache]   XLA tensor id: 20
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 20: input_20_shape_[8192, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996544, shape=(8192, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996544
[XLA Cache]   XLA tensor id: 21
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 21: input_21_shape_[3072, 8192]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996624, shape=(3072, 8192), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996624
[XLA Cache]   XLA tensor id: 22
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 22: input_22_shape_[128256, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996704, shape=(128256, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996704
[XLA Cache]   XLA tensor id: 23
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 23: input_23_shape_[1, 8, 128, 128]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996864, shape=(1, 8, 128, 128), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996864
[XLA Cache]   XLA tensor id: 24
[XLA Cache]   *** STATIC CACHE TENSOR DETECTED ***
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   *** CACHE MISS ANALYSIS FOR STATIC CACHE ***
[XLA Cache]   Reason: Each tensor object has unique id() = 140708366996864
[XLA Cache]   Even identical static cache tensors get different cache keys due to object identity
[XLA Cache]   Total static cache tensors in cache: 1
2025-08-13 23:25:07.720 (   5.799s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.720 (   5.799s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.720 (   5.799s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.720 (   5.799s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.720 (   5.799s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.720 (   5.799s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [7] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:07.720 (   5.799s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.720 (   5.799s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.720 (   5.799s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.720 (   5.799s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.720 (   5.799s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [7] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [128] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [128] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.721 (   5.799s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [7, 128] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [7, 128] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.721 (   5.800s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 64, 1] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 64, 1] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.722 (   5.800s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.723 (   5.801s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.723 (   5.801s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.723 (   5.801s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.723 (   5.801s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.723 (   5.801s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.723 (   5.801s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.723 (   5.802s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.723 (   5.802s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.723 (   5.802s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.723 (   5.802s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.723 (   5.802s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.723 (   5.802s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.723 (   5.802s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.723 (   5.802s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.724 (   5.802s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.746 (   5.825s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.747 (   5.825s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.747 (   5.825s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.747 (   5.825s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.747 (   5.825s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.747 (   5.825s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.747 (   5.825s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.747 (   5.825s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.747 (   5.825s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.747 (   5.825s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.747 (   5.825s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.747 (   5.825s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.747 (   5.826s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.747 (   5.826s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.757 (   5.835s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.757 (   5.835s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.757 (   5.835s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.757 (   5.836s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.757 (   5.836s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.757 (   5.836s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.757 (   5.836s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.757 (   5.836s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.757 (   5.836s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.757 (   5.836s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.758 (   5.836s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.758 (   5.836s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.758 (   5.836s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.758 (   5.836s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.768 (   5.846s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.769 (   5.847s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.769 (   5.847s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.769 (   5.847s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.769 (   5.847s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.769 (   5.847s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.769 (   5.848s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:07.769 (   5.848s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.769 (   5.848s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.769 (   5.848s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.769 (   5.848s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.769 (   5.848s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.769 (   5.848s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:07.769 (   5.848s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.769 (   5.848s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.770 (   5.848s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.770 (   5.848s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.770 (   5.848s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.770 (   5.848s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.770 (   5.848s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.770 (   5.849s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.770 (   5.849s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.770 (   5.849s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.770 (   5.849s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.770 (   5.849s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.770 (   5.849s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.770 (   5.849s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.770 (   5.849s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.770 (   5.849s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.771 (   5.849s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.771 (   5.849s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.794 (   5.873s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.794 (   5.873s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.794 (   5.873s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.795 (   5.873s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.795 (   5.873s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.795 (   5.873s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.795 (   5.873s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.795 (   5.873s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.795 (   5.873s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.795 (   5.873s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.795 (   5.873s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.795 (   5.873s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.796 (   5.874s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.796 (   5.874s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.796 (   5.874s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.796 (   5.874s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.796 (   5.874s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.796 (   5.874s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.796 (   5.874s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.796 (   5.874s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.796 (   5.874s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.796 (   5.874s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.796 (   5.875s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.796 (   5.875s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.796 (   5.875s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.796 (   5.875s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.796 (   5.875s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.796 (   5.875s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.889 (   5.968s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.889 (   5.968s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.889 (   5.968s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.890 (   5.968s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.890 (   5.968s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.890 (   5.968s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.890 (   5.968s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.890 (   5.968s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.890 (   5.968s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.890 (   5.968s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.890 (   5.968s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.890 (   5.968s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.894 (   5.972s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.894 (   5.972s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.988 (   6.066s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.988 (   6.066s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.988 (   6.066s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.988 (   6.067s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.988 (   6.067s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.988 (   6.067s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.988 (   6.067s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:07.988 (   6.067s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:07.988 (   6.067s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:07.988 (   6.067s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-13 23:25:07.988 (   6.067s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:07.988 (   6.067s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:07.992 (   6.070s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:07.992 (   6.070s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:08.074 (   6.152s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:08.074 (   6.152s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:08.074 (   6.152s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:08.074 (   6.152s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:08.074 (   6.153s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:08.074 (   6.153s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:08.074 (   6.153s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:08.074 (   6.153s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:08.074 (   6.153s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:08.074 (   6.153s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:08.074 (   6.153s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:08.074 (   6.153s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:08.079 (   6.157s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:08.079 (   6.158s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:08.079 (   6.158s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 128256] (semantics: ZeroCopy/other)
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 128256] (semantics: ZeroCopy/other)
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:09.529 (   7.607s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:09.573 (   7.651s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.651s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.651s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.651s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.651s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.651s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.651s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.651s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.651s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.651s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.573 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.574 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.574 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.574 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.574 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.574 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.574 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.574 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.574 (   7.652s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.574 (   7.653s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.574 (   7.653s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.574 (   7.653s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
[XLA Cache]   Similar cache keys with same shape/dtype/device:
[XLA Cache]     id=140708366996864 (different from current 140708366996864)
[XLA Cache]   ---
[XLA Cache] Input 24: input_24_shape_[1, 8, 128, 128]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366997104, shape=(1, 8, 128, 128), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366997104
[XLA Cache]   XLA tensor id: 25
[XLA Cache]   *** STATIC CACHE TENSOR DETECTED ***
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   *** CACHE MISS ANALYSIS FOR STATIC CACHE ***
[XLA Cache]   Reason: Each tensor object has unique id() = 140708366997104
[XLA Cache]   Even identical static cache tensors get different cache keys due to object identity
[XLA Cache]   Total static cache tensors in cache: 2
[XLA Cache]   Similar cache keys with same shape/dtype/device:
[XLA Cache]     id=140708366996864 (different from current 140708366997104)
[XLA Cache]     id=140708366997104 (different from current 140708366997104)
[XLA Cache]   ---
[XLA Cache] Input 25: input_25_shape_[64]_dtype_torch.float32
[XLA Cache]   Cache key: id=140708366997184, shape=(64,), dtype=torch.float32, device=xla:0
[XLA Cache]   Tensor object id: 140708366997184
[XLA Cache]   XLA tensor id: 26
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 26: input_26_shape_[1, 7]_dtype_torch.int64
[XLA Cache]   Cache key: id=140716024407088, shape=(1, 7), dtype=torch.int64, device=xla:0
[XLA Cache]   Tensor object id: 140716024407088
[XLA Cache]   XLA tensor id: 27
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 27: input_27_shape_[7]_dtype_torch.int64
[XLA Cache]   Cache key: id=140711052837008, shape=(7,), dtype=torch.int64, device=xla:0
[XLA Cache]   Tensor object id: 140711052837008
[XLA Cache]   XLA tensor id: 28
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] CACHE MISSES (28): ['input_0_shape_[3072]_dtype_torch.bfloat16 (idx=0)', 'input_1_shape_[3072]_dtype_torch.bfloat16 (idx=1)', 'input_2_shape_[3072]_dtype_torch.bfloat16 (idx=2)', 'input_3_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=3)', 'input_4_shape_[128]_dtype_torch.int64 (idx=4)', 'input_5_shape_[7, 128]_dtype_torch.bfloat16 (idx=5)', 'input_6_shape_[1, 64, 1]_dtype_torch.float32 (idx=6)', 'input_7_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=7)', 'input_8_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=8)', 'input_9_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=9)', 'input_10_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=10)', 'input_11_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=11)', 'input_12_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=12)', 'input_13_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=13)', 'input_14_shape_[3072, 128256]_dtype_torch.bfloat16 (idx=14)', 'input_15_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=15)', 'input_16_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=16)', 'input_17_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=17)', 'input_18_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=18)', 'input_19_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=19)', 'input_20_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=20)', 'input_21_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=21)', 'input_22_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=22)', 'input_23_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=23)', 'input_24_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=24)', 'input_25_shape_[64]_dtype_torch.float32 (idx=25)', 'input_26_shape_[1, 7]_dtype_torch.int64 (idx=26)', 'input_27_shape_[7]_dtype_torch.int64 (idx=27)']
[XLA Cache] STATIC CACHE TENSORS FOUND (2):
[XLA Cache]   input_23_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 -> HIT (tensor_id=140708366996864, xla_id=24)
[XLA Cache]   input_24_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 -> HIT (tensor_id=140708366997104, xla_id=25)
[XLA Cache] Updated cache size: 28 unique tensors
[XLA Cache] === END CACHE ANALYSIS ===

[JAMES] StableHLO IR:
 module @IrToHlo.360 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<3072x128256xbf16> {mhlo.sharding = "{replicated}"}, %arg1: tensor<f32> {mhlo.sharding = "{replicated}"}, %arg2: tensor<8192x3072xbf16> {mhlo.sharding = "{replicated}"}, %arg3: tensor<3072x8192xbf16> {mhlo.sharding = "{replicated}"}, %arg4: tensor<3072x3072xbf16> {mhlo.sharding = "{replicated}"}, %arg5: tensor<3072x1024xbf16> {mhlo.sharding = "{replicated}"}, %arg6: tensor<1x7xi64> {mhlo.sharding = "{replicated}"}, %arg7: tensor<128256x3072xbf16> {mhlo.sharding = "{devices=[2,1]<=[2]}"}, %arg8: tensor<3072xbf16> {mhlo.sharding = "{replicated}"}, %arg9: tensor<7xi64> {mhlo.sharding = "{replicated}"}, %arg10: tensor<i64> {mhlo.sharding = "{replicated}"}, %arg11: tensor<1x8x128x128xbf16> {mhlo.sharding = "{replicated}"}, %arg12: tensor<128xi64> {mhlo.sharding = "{replicated}"}, %arg13: tensor<7x128xbf16> {mhlo.sharding = "{replicated}"}, %arg14: tensor<f32> {mhlo.sharding = "{replicated}"}, %arg15: tensor<1x64x1xf32> {mhlo.sharding = "{replicated}"}, %arg16: tensor<3072x1024xbf16> {mhlo.sharding = "{replicated}"}, %arg17: tensor<1x8x128x128xbf16> {mhlo.sharding = "{replicated}"}, %arg18: tensor<3072x3072xbf16> {mhlo.sharding = "{replicated}"}, %arg19: tensor<3072xbf16> {mhlo.sharding = "{replicated}"}, %arg20: tensor<3072x8192xbf16> {mhlo.sharding = "{replicated}"}, %arg21: tensor<3072xbf16> {mhlo.sharding = "{replicated}"}) -> tensor<1x7x128256xbf16> {
    %c = stablehlo.constant dense<0> : tensor<7xi64>
    %cst = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<1x7x3072xf32>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %0 = stablehlo.convert %arg21 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %1 = stablehlo.broadcast_in_dim %0, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %2 = stablehlo.reshape %arg6 : (tensor<1x7xi64>) -> tensor<7xi64>
    %3 = stablehlo.convert %2 : (tensor<7xi64>) -> tensor<7xui32>
    %4 = "stablehlo.gather"(%arg7, %3) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %5 = stablehlo.reshape %4 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %6 = stablehlo.convert %arg8 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %8 = stablehlo.convert %5 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %9 = stablehlo.power %8, %cst_0 : tensor<1x7x3072xf32>
    %10 = stablehlo.reduce(%9 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %11 = stablehlo.multiply %10, %cst : tensor<1x7xf32>
    %12 = stablehlo.reshape %11 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %13 = stablehlo.broadcast_in_dim %arg1, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %14 = stablehlo.add %12, %13 : tensor<1x7x1xf32>
    %15 = stablehlo.rsqrt %14 : tensor<1x7x1xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %17 = stablehlo.broadcast_in_dim %16, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %18 = stablehlo.multiply %8, %17 : tensor<1x7x3072xf32>
    %19 = stablehlo.convert %18 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %20 = stablehlo.convert %19 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %21 = stablehlo.multiply %7, %20 : tensor<1x7x3072xf32>
    %22 = stablehlo.convert %21 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %23 = stablehlo.reshape %22 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %24 = stablehlo.dot_general %23, %arg18, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %25 = stablehlo.reshape %24 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %26 = stablehlo.transpose %25, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %27 = stablehlo.convert %26 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %28 = stablehlo.reshape %arg9 : (tensor<7xi64>) -> tensor<1x1x7xi64>
    %29 = stablehlo.convert %28 : (tensor<1x1x7xi64>) -> tensor<1x1x7xf32>
    %30 = stablehlo.dot_general %arg15, %29, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %31 = stablehlo.transpose %30, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %32 = stablehlo.concatenate %31, %31, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %33 = stablehlo.cosine %32 : tensor<1x7x128xf32>
    %34 = stablehlo.convert %33 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %35 = stablehlo.reshape %34 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %36 = stablehlo.convert %35 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %37 = stablehlo.reshape %36 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %38 = stablehlo.broadcast_in_dim %37, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %39 = stablehlo.multiply %27, %38 : tensor<1x24x7x128xf32>
    %40 = stablehlo.convert %39 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %41 = stablehlo.slice %26 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %42 = stablehlo.negate %41 : tensor<1x24x7x64xbf16>
    %43 = stablehlo.slice %26 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %44 = stablehlo.concatenate %42, %43, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %45 = stablehlo.convert %44 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %46 = stablehlo.sine %32 : tensor<1x7x128xf32>
    %47 = stablehlo.convert %46 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %48 = stablehlo.reshape %47 : (tensor<1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xf32>
    %50 = stablehlo.reshape %49 : (tensor<1x1x7x128xf32>) -> tensor<1x7x128xf32>
    %51 = stablehlo.broadcast_in_dim %50, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %52 = stablehlo.multiply %45, %51 : tensor<1x24x7x128xf32>
    %53 = stablehlo.convert %52 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %54 = stablehlo.add %40, %53 : tensor<1x24x7x128xbf16>
    %55 = stablehlo.reshape %54 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %56 = stablehlo.compare  LT, %arg9, %c : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %57 = stablehlo.broadcast_in_dim %arg10, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %58 = stablehlo.add %arg9, %57 : tensor<7xi64>
    %59 = stablehlo.select %56, %58, %arg9 : tensor<7xi1>, tensor<7xi64>
    %60 = stablehlo.reshape %59 : (tensor<7xi64>) -> tensor<7x1xi64>
    %61 = stablehlo.dot_general %23, %arg16, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %62 = stablehlo.reshape %61 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %63 = stablehlo.transpose %62, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %64 = stablehlo.convert %63 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %65 = stablehlo.broadcast_in_dim %37, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %66 = stablehlo.multiply %64, %65 : tensor<1x8x7x128xf32>
    %67 = stablehlo.convert %66 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %68 = stablehlo.slice %63 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %69 = stablehlo.negate %68 : tensor<1x8x7x64xbf16>
    %70 = stablehlo.slice %63 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %71 = stablehlo.concatenate %69, %70, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %72 = stablehlo.convert %71 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %73 = stablehlo.broadcast_in_dim %50, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %74 = stablehlo.multiply %72, %73 : tensor<1x8x7x128xf32>
    %75 = stablehlo.convert %74 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %76 = stablehlo.add %67, %75 : tensor<1x8x7x128xbf16>
    %77 = "stablehlo.scatter"(%arg17, %60, %76) <{indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>, unique_indices = false}> ({
    ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
      stablehlo.return %arg23 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %78 = stablehlo.broadcast_in_dim %77, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %81 = stablehlo.reshape %80 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %82 = stablehlo.dot_general %55, %81, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %83 = stablehlo.reshape %82 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %84 = stablehlo.convert %83 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %85 = stablehlo.broadcast_in_dim %arg14, dims = [] : (tensor<f32>) -> tensor<1x24x7x128xf32>
    %86 = stablehlo.multiply %84, %85 : tensor<1x24x7x128xf32>
    %87 = stablehlo.convert %86 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %88 = stablehlo.convert %arg13 : (tensor<7x128xbf16>) -> tensor<7x128xf32>
    %89 = stablehlo.broadcast_in_dim %arg12, dims = [1] : (tensor<128xi64>) -> tensor<7x128xi64>
    %90 = stablehlo.broadcast_in_dim %arg9, dims = [0] : (tensor<7xi64>) -> tensor<7x128xi64>
    %91 = stablehlo.compare  GT, %89, %90 : (tensor<7x128xi64>, tensor<7x128xi64>) -> tensor<7x128xi1>
    %92 = stablehlo.convert %91 : (tensor<7x128xi1>) -> tensor<7x128xf32>
    %93 = stablehlo.multiply %88, %92 : tensor<7x128xf32>
    %94 = stablehlo.convert %93 : (tensor<7x128xf32>) -> tensor<7x128xbf16>
    %95 = stablehlo.reshape %94 : (tensor<7x128xbf16>) -> tensor<1x7x128xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %97 = stablehlo.add %87, %96 : tensor<1x24x7x128xbf16>
    %98 = stablehlo.convert %97 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %99 = stablehlo.reduce(%98 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %101 = stablehlo.subtract %98, %100 : tensor<1x24x7x128xf32>
    %102 = stablehlo.exponential %101 : tensor<1x24x7x128xf32>
    %103 = stablehlo.reduce(%102 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %105 = stablehlo.divide %102, %104 : tensor<1x24x7x128xf32>
    %106 = stablehlo.convert %105 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %108 = stablehlo.dot_general %23, %arg5, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %109 = stablehlo.reshape %108 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %110 = stablehlo.transpose %109, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %111 = "stablehlo.scatter"(%arg11, %60, %110) <{indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>, unique_indices = false}> ({
    ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
      stablehlo.return %arg23 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %112 = stablehlo.broadcast_in_dim %111, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %114 = stablehlo.dot_general %107, %113, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %115 = stablehlo.reshape %114 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %116 = stablehlo.transpose %115, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %117 = stablehlo.reshape %116 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %118 = stablehlo.dot_general %117, %arg4, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %119 = stablehlo.reshape %118 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %120 = stablehlo.add %5, %119 : tensor<1x7x3072xbf16>
    %121 = stablehlo.convert %arg19 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %122 = stablehlo.broadcast_in_dim %121, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %123 = stablehlo.convert %120 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %124 = stablehlo.power %123, %cst_0 : tensor<1x7x3072xf32>
    %125 = stablehlo.reduce(%124 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %126 = stablehlo.multiply %125, %cst : tensor<1x7xf32>
    %127 = stablehlo.reshape %126 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %128 = stablehlo.add %127, %13 : tensor<1x7x1xf32>
    %129 = stablehlo.rsqrt %128 : tensor<1x7x1xf32>
    %130 = stablehlo.reshape %129 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %131 = stablehlo.broadcast_in_dim %130, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %132 = stablehlo.multiply %123, %131 : tensor<1x7x3072xf32>
    %133 = stablehlo.convert %132 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %134 = stablehlo.convert %133 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %135 = stablehlo.multiply %122, %134 : tensor<1x7x3072xf32>
    %136 = stablehlo.convert %135 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %137 = stablehlo.reshape %136 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %138 = stablehlo.dot_general %137, %arg20, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %139 = stablehlo.reshape %138 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %140 = stablehlo.convert %139 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %141 = stablehlo.logistic %139 : tensor<1x7x8192xbf16>
    %142 = stablehlo.convert %141 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %143 = stablehlo.multiply %140, %142 : tensor<1x7x8192xf32>
    %144 = stablehlo.convert %143 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %145 = stablehlo.convert %144 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %146 = stablehlo.dot_general %137, %arg3, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %147 = stablehlo.reshape %146 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %148 = stablehlo.convert %147 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %149 = stablehlo.multiply %145, %148 : tensor<1x7x8192xf32>
    %150 = stablehlo.convert %149 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %151 = stablehlo.reshape %150 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %152 = stablehlo.dot_general %151, %arg2, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %153 = stablehlo.reshape %152 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %154 = stablehlo.add %120, %153 : tensor<1x7x3072xbf16>
    %155 = stablehlo.convert %154 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %156 = stablehlo.power %155, %cst_0 : tensor<1x7x3072xf32>
    %157 = stablehlo.reduce(%156 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %158 = stablehlo.multiply %157, %cst : tensor<1x7xf32>
    %159 = stablehlo.reshape %158 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %160 = stablehlo.add %159, %13 : tensor<1x7x1xf32>
    %161 = stablehlo.rsqrt %160 : tensor<1x7x1xf32>
    %162 = stablehlo.reshape %161 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %163 = stablehlo.broadcast_in_dim %162, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %164 = stablehlo.multiply %155, %163 : tensor<1x7x3072xf32>
    %165 = stablehlo.convert %164 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %166 = stablehlo.convert %165 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %167 = stablehlo.multiply %1, %166 : tensor<1x7x3072xf32>
    %168 = stablehlo.convert %167 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %169 = stablehlo.reshape %168 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %170 = stablehlo.dot_general %169, %arg0, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %171 = stablehlo.reshape %170 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %171 : tensor<1x7x128256xbf16>
  }
}

2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.590 (   7.668s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
Hlo input positions pre normalization [15, 219, 14, 13, 11, 10, 27, 23, 1, 28, -1, 25, 5, 6, 151, 7, 9, 24, 8, 2, 12, 3]
Hlo input positions post normalization [16, 220, 15, 14, 12, 11, 28, 24, 2, 29, 0, 26, 6, 7, 152, 8, 10, 25, 9, 3, 13, 4]
[JAMES] setting arg ref map to  refs=140715232196096,constant_unknown,140715232198496,140711049673152,140711049674112,140711049670912,constant_unknown,140716026145072,140715231682672,constant_unknown,140715232198416,user_input,140711049674512,140711049674672,constant_unknown,140711049668032,140711049667872,140716026202448,140711049670752,140715232188816,140711049674272,140711049670192
2025-08-13 23:25:09.590 (   7.669s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:09.590 (   7.669s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:09.590 (   7.669s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:09.590 (   7.669s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:09.590 (   7.669s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [64] (semantics: ZeroCopy/other)
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [64] (semantics: ZeroCopy/other)
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.669s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.591 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.670s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.671s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.592 (   7.671s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.593 (   7.671s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:09.617 (   7.696s) [        CA99B1C0]     client_instance.cc:471      1| ClientInstance::PJRT_Client_Compile
2025-08-13 23:25:09.617 (   7.696s) [        CA99B1C0]      module_builder.cc:101      1| ModuleBuilder::buildModule
2025-08-13 23:25:09.619 (   7.697s) [        CA99B1C0]      module_builder.cc:155      1| VHLO Module:
module @SyncTensorsGraph.362 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<7x!vhlo.i64_v1>, %arg1: !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg4: !vhlo.tensor_v1<1x7x!vhlo.i64_v1>, %arg5: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg8: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<128x!vhlo.i64_v1>, %arg16: !vhlo.tensor_v1<7x128x!vhlo.bf16_v1>, %arg17: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg18: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg21: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<7xi64>>}> : () -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x7xf32>>}> : () -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.broadcast_in_dim_v1"(%4) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %6 = "vhlo.compare_v1"(%arg0, %0) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.bool_v1>
    %7 = "vhlo.broadcast_in_dim_v1"(%arg7) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %8 = "vhlo.add_v1"(%arg0, %7) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %9 = "vhlo.select_v1"(%6, %8, %arg0) : (!vhlo.tensor_v1<7x!vhlo.bool_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>, !vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %10 = "vhlo.reshape_v1"(%9) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x1x!vhlo.i64_v1>
    %11 = "vhlo.convert_v1"(%arg6) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %12 = "vhlo.broadcast_in_dim_v1"(%11) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %13 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.i64_v1>
    %14 = "vhlo.convert_v1"(%13) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x!vhlo.ui32_v1>
    %15 = "vhlo.gather_v2"(%arg5, %14) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %16 = "vhlo.reshape_v1"(%15) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %17 = "vhlo.convert_v1"(%16) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %18 = "vhlo.power_v1"(%17, %5) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %19 = "vhlo.reduce_v1"(%18, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg23: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %180 = "vhlo.add_v1"(%arg22, %arg23) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%180) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %20 = "vhlo.multiply_v1"(%19, %3) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %22 = "vhlo.broadcast_in_dim_v1"(%arg3) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %23 = "vhlo.add_v1"(%21, %22) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %24 = "vhlo.rsqrt_v2"(%23) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %25 = "vhlo.reshape_v1"(%24) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %26 = "vhlo.broadcast_in_dim_v1"(%25) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %27 = "vhlo.multiply_v1"(%17, %26) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %28 = "vhlo.convert_v1"(%27) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %29 = "vhlo.convert_v1"(%28) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %30 = "vhlo.multiply_v1"(%12, %29) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %31 = "vhlo.convert_v1"(%30) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %32 = "vhlo.reshape_v1"(%31) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %33 = "vhlo.dot_general_v2"(%32, %arg2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %34 = "vhlo.reshape_v1"(%33) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %35 = "vhlo.transpose_v1"(%34) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %36 = "vhlo.convert_v1"(%35) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %37 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>
    %38 = "vhlo.convert_v1"(%37) : (!vhlo.tensor_v1<1x1x7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>
    %39 = "vhlo.dot_general_v2"(%arg1, %38) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>
    %40 = "vhlo.transpose_v1"(%39) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1]> : tensor<3xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[1, 2, 0]> : tensor<3xindex>>, xla_shape = #vhlo.string_v1<"f32[1,7,64]{1,2,0}">} : (!vhlo.tensor_v1<1x64x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>
    %41 = "vhlo.concatenate_v1"(%40, %40) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %42 = "vhlo.cosine_v2"(%41) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %43 = "vhlo.convert_v1"(%42) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %44 = "vhlo.reshape_v1"(%43) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %45 = "vhlo.convert_v1"(%44) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %46 = "vhlo.reshape_v1"(%45) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %47 = "vhlo.broadcast_in_dim_v1"(%46) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %48 = "vhlo.multiply_v1"(%36, %47) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %49 = "vhlo.convert_v1"(%48) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %50 = "vhlo.slice_v1"(%35) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %51 = "vhlo.negate_v1"(%50) : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %52 = "vhlo.slice_v1"(%35) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>
    %53 = "vhlo.concatenate_v1"(%51, %52) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %54 = "vhlo.convert_v1"(%53) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %55 = "vhlo.sine_v2"(%41) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %56 = "vhlo.convert_v1"(%55) : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %57 = "vhlo.reshape_v1"(%56) : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>
    %58 = "vhlo.convert_v1"(%57) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>
    %59 = "vhlo.reshape_v1"(%58) : (!vhlo.tensor_v1<1x1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>
    %60 = "vhlo.broadcast_in_dim_v1"(%59) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %61 = "vhlo.multiply_v1"(%54, %60) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>
    %62 = "vhlo.convert_v1"(%61) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %63 = "vhlo.add_v1"(%49, %62) : (!vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %64 = "vhlo.scatter_v2"(%arg8, %10, %63) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg23) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %65 = "vhlo.custom_call_v1"(%64) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %66 = "vhlo.dot_general_v2"(%32, %arg9) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>
    %67 = "vhlo.reshape_v1"(%66) : (!vhlo.tensor_v1<7x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>
    %68 = "vhlo.transpose_v1"(%67) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,8,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x8x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>
    %69 = "vhlo.scatter_v2"(%arg10, %10, %68) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg23) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %70 = "vhlo.custom_call_v1"(%69) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %71 = "vhlo.convert_v1"(%arg21) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %72 = "vhlo.broadcast_in_dim_v1"(%71) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %73 = "vhlo.dot_general_v2"(%32, %arg18) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %74 = "vhlo.reshape_v1"(%73) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %75 = "vhlo.transpose_v1"(%74) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %76 = "vhlo.convert_v1"(%75) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,7,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %77 = "vhlo.broadcast_in_dim_v1"(%46) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %78 = "vhlo.multiply_v1"(%76, %77) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %79 = "vhlo.convert_v1"(%78) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %80 = "vhlo.slice_v1"(%75) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %81 = "vhlo.negate_v1"(%80) : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %82 = "vhlo.slice_v1"(%75) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 7, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>
    %83 = "vhlo.concatenate_v1"(%81, %82) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %84 = "vhlo.convert_v1"(%83) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %85 = "vhlo.broadcast_in_dim_v1"(%59) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %86 = "vhlo.multiply_v1"(%84, %85) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %87 = "vhlo.convert_v1"(%86) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %88 = "vhlo.add_v1"(%79, %87) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %89 = "vhlo.reshape_v1"(%88) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %90 = "vhlo.broadcast_in_dim_v1"(%64) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %91 = "vhlo.reshape_v1"(%90) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %92 = "vhlo.transpose_v1"(%91) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,128]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %93 = "vhlo.reshape_v1"(%92) : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %94 = "vhlo.dot_general_v2"(%89, %93) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %95 = "vhlo.reshape_v1"(%94) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %96 = "vhlo.convert_v1"(%95) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %97 = "vhlo.broadcast_in_dim_v1"(%arg17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %98 = "vhlo.multiply_v1"(%96, %97) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %99 = "vhlo.convert_v1"(%98) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %100 = "vhlo.convert_v1"(%arg16) : (!vhlo.tensor_v1<7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.f32_v1>
    %101 = "vhlo.broadcast_in_dim_v1"(%arg15) <{broadcast_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.i64_v1>
    %102 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<7x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.i64_v1>
    %103 = "vhlo.compare_v1"(%101, %102) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<7x128x!vhlo.i64_v1>, !vhlo.tensor_v1<7x128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.bool_v1>
    %104 = "vhlo.convert_v1"(%103) : (!vhlo.tensor_v1<7x128x!vhlo.bool_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.f32_v1>
    %105 = "vhlo.multiply_v1"(%100, %104) : (!vhlo.tensor_v1<7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.f32_v1>
    %106 = "vhlo.convert_v1"(%105) : (!vhlo.tensor_v1<7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<7x128x!vhlo.bf16_v1>
    %107 = "vhlo.reshape_v1"(%106) : (!vhlo.tensor_v1<7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>
    %108 = "vhlo.broadcast_in_dim_v1"(%107) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %109 = "vhlo.add_v1"(%99, %108) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %110 = "vhlo.convert_v1"(%109) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %111 = "vhlo.reduce_v1"(%110, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg23: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %180 = "vhlo.maximum_v1"(%arg22, %arg23) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%180) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %112 = "vhlo.broadcast_in_dim_v1"(%111) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %113 = "vhlo.subtract_v1"(%110, %112) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %114 = "vhlo.exponential_v2"(%113) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %115 = "vhlo.reduce_v1"(%114, %1) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg23: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %180 = "vhlo.add_v1"(%arg22, %arg23) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%180) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>
    %116 = "vhlo.broadcast_in_dim_v1"(%115) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %117 = "vhlo.divide_v1"(%114, %116) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>
    %118 = "vhlo.convert_v1"(%117) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %119 = "vhlo.reshape_v1"(%118) : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %120 = "vhlo.broadcast_in_dim_v1"(%69) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %121 = "vhlo.reshape_v1"(%120) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %122 = "vhlo.dot_general_v2"(%119, %121) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>
    %123 = "vhlo.reshape_v1"(%122) : (!vhlo.tensor_v1<24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>
    %124 = "vhlo.transpose_v1"(%123) <{permutation = #vhlo.tensor_v1<dense<[0, 2, 1, 3]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,7,24,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x7x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<1x7x24x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %126 = "vhlo.dot_general_v2"(%125, %arg14) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %127 = "vhlo.reshape_v1"(%126) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %128 = "vhlo.add_v1"(%16, %127) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %129 = "vhlo.convert_v1"(%arg19) : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %130 = "vhlo.broadcast_in_dim_v1"(%129) <{broadcast_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %131 = "vhlo.convert_v1"(%128) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %132 = "vhlo.power_v1"(%131, %5) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %133 = "vhlo.reduce_v1"(%132, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg23: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %180 = "vhlo.add_v1"(%arg22, %arg23) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%180) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %134 = "vhlo.multiply_v1"(%133, %3) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %135 = "vhlo.reshape_v1"(%134) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %136 = "vhlo.add_v1"(%135, %22) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %137 = "vhlo.rsqrt_v2"(%136) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %138 = "vhlo.reshape_v1"(%137) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %139 = "vhlo.broadcast_in_dim_v1"(%138) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %140 = "vhlo.multiply_v1"(%131, %139) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %141 = "vhlo.convert_v1"(%140) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %142 = "vhlo.convert_v1"(%141) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %143 = "vhlo.multiply_v1"(%130, %142) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %144 = "vhlo.convert_v1"(%143) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %145 = "vhlo.reshape_v1"(%144) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %146 = "vhlo.dot_general_v2"(%145, %arg20) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %147 = "vhlo.reshape_v1"(%146) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %148 = "vhlo.convert_v1"(%147) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %149 = "vhlo.logistic_v2"(%147) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %150 = "vhlo.convert_v1"(%149) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %151 = "vhlo.multiply_v1"(%148, %150) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %152 = "vhlo.convert_v1"(%151) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %153 = "vhlo.convert_v1"(%152) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %154 = "vhlo.dot_general_v2"(%145, %arg13) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %155 = "vhlo.reshape_v1"(%154) : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %156 = "vhlo.convert_v1"(%155) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %157 = "vhlo.multiply_v1"(%153, %156) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>
    %158 = "vhlo.convert_v1"(%157) : (!vhlo.tensor_v1<1x7x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>
    %159 = "vhlo.reshape_v1"(%158) : (!vhlo.tensor_v1<1x7x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>
    %160 = "vhlo.dot_general_v2"(%159, %arg12) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %161 = "vhlo.reshape_v1"(%160) : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %162 = "vhlo.add_v1"(%128, %161) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %163 = "vhlo.convert_v1"(%162) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %164 = "vhlo.power_v1"(%163, %5) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %165 = "vhlo.reduce_v1"(%164, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg23: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %180 = "vhlo.add_v1"(%arg22, %arg23) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%180) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %166 = "vhlo.multiply_v1"(%165, %3) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %167 = "vhlo.reshape_v1"(%166) : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %168 = "vhlo.add_v1"(%167, %22) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %169 = "vhlo.rsqrt_v2"(%168) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>
    %170 = "vhlo.reshape_v1"(%169) : (!vhlo.tensor_v1<1x7x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x!vhlo.f32_v1>
    %171 = "vhlo.broadcast_in_dim_v1"(%170) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x7x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %172 = "vhlo.multiply_v1"(%163, %171) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %173 = "vhlo.convert_v1"(%172) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %174 = "vhlo.convert_v1"(%173) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %175 = "vhlo.multiply_v1"(%72, %174) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>
    %176 = "vhlo.convert_v1"(%175) : (!vhlo.tensor_v1<1x7x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>
    %177 = "vhlo.reshape_v1"(%176) : (!vhlo.tensor_v1<1x7x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>
    %178 = "vhlo.dot_general_v2"(%177, %arg11) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<7x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>
    %179 = "vhlo.reshape_v1"(%178) : (!vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%65, %70, %178, %179) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<7x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x7x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-08-13 23:25:09.632 (   7.710s) [        CA99B1C0]      module_builder.cc:188      1| SHLO Module:
module @SyncTensorsGraph.362 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<1x64x1xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}]>}, %arg2: tensor<3072x1024xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg3: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg4: tensor<1x7xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg7: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg8: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}, %arg9: tensor<3072x1024xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg10: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}, %arg11: tensor<3072x128256xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg12: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg13: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg15: tensor<128xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg16: tensor<7x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg17: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg18: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg19: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg20: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg21: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) {
    %c = stablehlo.constant dense<0> : tensor<7xi64>
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
    %1 = stablehlo.compare  LT, %arg0, %c : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
    %2 = stablehlo.broadcast_in_dim %arg7, dims = [] : (tensor<i64>) -> tensor<7xi64>
    %3 = stablehlo.add %arg0, %2 : tensor<7xi64>
    %4 = stablehlo.select %1, %3, %arg0 : tensor<7xi1>, tensor<7xi64>
    %5 = stablehlo.reshape %4 : (tensor<7xi64>) -> tensor<7x1xi64>
    %6 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %7 = stablehlo.broadcast_in_dim %6, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %8 = stablehlo.convert %arg4 : (tensor<1x7xi64>) -> tensor<1x7xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x7xui32>) -> tensor<7xui32>
    %10 = "stablehlo.gather"(%arg5, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %13 = stablehlo.power %12, %0 : tensor<1x7x3072xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %15 = stablehlo.multiply %14, %cst_1 : tensor<1x7xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %17 = stablehlo.broadcast_in_dim %arg3, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x7x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x7x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x7x3072xf32>
    %23 = stablehlo.convert %22 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %24 = stablehlo.convert %23 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %25 = stablehlo.multiply %7, %24 : tensor<1x7x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %27 = stablehlo.reshape %26 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %28 = stablehlo.dot_general %27, %arg2, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %29 = stablehlo.reshape %28 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %30 = stablehlo.transpose %29, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %31 = stablehlo.convert %30 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %32 = stablehlo.convert %arg0 : (tensor<7xi64>) -> tensor<7xf32>
    %33 = stablehlo.reshape %32 : (tensor<7xf32>) -> tensor<1x1x7xf32>
    %34 = stablehlo.dot_general %arg1, %33, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %35 = stablehlo.transpose %34, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
    %36 = stablehlo.concatenate %35, %35, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
    %37 = stablehlo.cosine %36 : tensor<1x7x128xf32>
    %38 = stablehlo.convert %37 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %39 = stablehlo.convert %38 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %40 = stablehlo.broadcast_in_dim %39, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %41 = stablehlo.multiply %31, %40 : tensor<1x8x7x128xf32>
    %42 = stablehlo.convert %41 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %43 = stablehlo.slice %30 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %44 = stablehlo.negate %43 : tensor<1x8x7x64xbf16>
    %45 = stablehlo.slice %30 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
    %46 = stablehlo.concatenate %44, %45, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
    %47 = stablehlo.convert %46 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
    %48 = stablehlo.sine %36 : tensor<1x7x128xf32>
    %49 = stablehlo.convert %48 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
    %50 = stablehlo.convert %49 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
    %51 = stablehlo.broadcast_in_dim %50, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
    %52 = stablehlo.multiply %47, %51 : tensor<1x8x7x128xf32>
    %53 = stablehlo.convert %52 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
    %54 = stablehlo.add %42, %53 : tensor<1x8x7x128xbf16>
    %55 = "stablehlo.scatter"(%arg8, %5, %54) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
      stablehlo.return %arg23 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %56 = sdy.sharding_constraint %55 <@mesh, [{}, {}, {}, {}]> : tensor<1x8x128x128xbf16>
    %57 = stablehlo.dot_general %27, %arg9, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %58 = stablehlo.reshape %57 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
    %59 = stablehlo.transpose %58, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
    %60 = "stablehlo.scatter"(%arg10, %5, %59) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
      stablehlo.return %arg23 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
    %61 = sdy.sharding_constraint %60 <@mesh, [{}, {}, {}, {}]> : tensor<1x8x128x128xbf16>
    %62 = stablehlo.convert %arg21 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %63 = stablehlo.broadcast_in_dim %62, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %64 = stablehlo.dot_general %27, %arg18, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %65 = stablehlo.reshape %64 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
    %66 = stablehlo.transpose %65, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
    %67 = stablehlo.convert %66 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %68 = stablehlo.broadcast_in_dim %39, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %69 = stablehlo.multiply %67, %68 : tensor<1x24x7x128xf32>
    %70 = stablehlo.convert %69 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %71 = stablehlo.slice %66 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %72 = stablehlo.negate %71 : tensor<1x24x7x64xbf16>
    %73 = stablehlo.slice %66 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
    %74 = stablehlo.concatenate %72, %73, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
    %75 = stablehlo.convert %74 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %76 = stablehlo.broadcast_in_dim %50, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
    %77 = stablehlo.multiply %75, %76 : tensor<1x24x7x128xf32>
    %78 = stablehlo.convert %77 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %79 = stablehlo.add %70, %78 : tensor<1x24x7x128xbf16>
    %80 = stablehlo.reshape %79 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %81 = stablehlo.broadcast_in_dim %55, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %82 = stablehlo.reshape %81 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %83 = stablehlo.transpose %82, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %84 = stablehlo.reshape %83 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %85 = stablehlo.dot_general %80, %84, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %86 = stablehlo.convert %85 : (tensor<24x7x128xbf16>) -> tensor<24x7x128xf32>
    %87 = stablehlo.reshape %86 : (tensor<24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %88 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<f32>) -> tensor<1x24x7x128xf32>
    %89 = stablehlo.multiply %87, %88 : tensor<1x24x7x128xf32>
    %90 = stablehlo.convert %89 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %91 = stablehlo.convert %arg16 : (tensor<7x128xbf16>) -> tensor<7x128xf32>
    %92 = stablehlo.broadcast_in_dim %arg15, dims = [1] : (tensor<128xi64>) -> tensor<7x128xi64>
    %93 = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<7xi64>) -> tensor<7x128xi64>
    %94 = stablehlo.compare  GT, %92, %93 : (tensor<7x128xi64>, tensor<7x128xi64>) -> tensor<7x128xi1>
    %95 = stablehlo.convert %94 : (tensor<7x128xi1>) -> tensor<7x128xf32>
    %96 = stablehlo.multiply %91, %95 : tensor<7x128xf32>
    %97 = stablehlo.convert %96 : (tensor<7x128xf32>) -> tensor<7x128xbf16>
    %98 = stablehlo.reshape %97 : (tensor<7x128xbf16>) -> tensor<1x7x128xbf16>
    %99 = stablehlo.broadcast_in_dim %98, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %100 = stablehlo.add %90, %99 : tensor<1x24x7x128xbf16>
    %101 = stablehlo.convert %100 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
    %102 = stablehlo.reduce(%101 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %103 = stablehlo.broadcast_in_dim %102, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %104 = stablehlo.subtract %101, %103 : tensor<1x24x7x128xf32>
    %105 = stablehlo.exponential %104 : tensor<1x24x7x128xf32>
    %106 = stablehlo.reduce(%105 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
    %107 = stablehlo.broadcast_in_dim %106, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
    %108 = stablehlo.divide %105, %107 : tensor<1x24x7x128xf32>
    %109 = stablehlo.convert %108 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
    %110 = stablehlo.reshape %109 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %111 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %112 = stablehlo.reshape %111 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %113 = stablehlo.dot_general %110, %112, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %114 = stablehlo.reshape %113 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %115 = stablehlo.transpose %114, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
    %116 = stablehlo.reshape %115 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
    %117 = stablehlo.dot_general %116, %arg14, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %118 = stablehlo.reshape %117 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %119 = stablehlo.add %11, %118 : tensor<1x7x3072xbf16>
    %120 = stablehlo.convert %arg19 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %121 = stablehlo.broadcast_in_dim %120, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
    %122 = stablehlo.convert %119 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %123 = stablehlo.power %122, %0 : tensor<1x7x3072xf32>
    %124 = stablehlo.reduce(%123 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %125 = stablehlo.multiply %124, %cst_1 : tensor<1x7xf32>
    %126 = stablehlo.reshape %125 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %127 = stablehlo.add %126, %17 : tensor<1x7x1xf32>
    %128 = stablehlo.rsqrt %127 : tensor<1x7x1xf32>
    %129 = stablehlo.reshape %128 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %130 = stablehlo.broadcast_in_dim %129, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %131 = stablehlo.multiply %122, %130 : tensor<1x7x3072xf32>
    %132 = stablehlo.convert %131 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %133 = stablehlo.convert %132 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %134 = stablehlo.multiply %121, %133 : tensor<1x7x3072xf32>
    %135 = stablehlo.convert %134 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %136 = stablehlo.reshape %135 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %137 = stablehlo.dot_general %136, %arg20, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %138 = stablehlo.reshape %137 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %139 = stablehlo.convert %138 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %140 = stablehlo.logistic %138 : tensor<1x7x8192xbf16>
    %141 = stablehlo.convert %140 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %142 = stablehlo.multiply %139, %141 : tensor<1x7x8192xf32>
    %143 = stablehlo.convert %142 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %144 = stablehlo.convert %143 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
    %145 = stablehlo.dot_general %136, %arg13, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %146 = stablehlo.convert %145 : (tensor<7x8192xbf16>) -> tensor<7x8192xf32>
    %147 = stablehlo.reshape %146 : (tensor<7x8192xf32>) -> tensor<1x7x8192xf32>
    %148 = stablehlo.multiply %144, %147 : tensor<1x7x8192xf32>
    %149 = stablehlo.convert %148 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
    %150 = stablehlo.reshape %149 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
    %151 = stablehlo.dot_general %150, %arg12, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %152 = stablehlo.reshape %151 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %153 = stablehlo.add %119, %152 : tensor<1x7x3072xbf16>
    %154 = stablehlo.convert %153 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %155 = stablehlo.power %154, %0 : tensor<1x7x3072xf32>
    %156 = stablehlo.reduce(%155 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
    %157 = stablehlo.multiply %156, %cst_1 : tensor<1x7xf32>
    %158 = stablehlo.reshape %157 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
    %159 = stablehlo.add %158, %17 : tensor<1x7x1xf32>
    %160 = stablehlo.rsqrt %159 : tensor<1x7x1xf32>
    %161 = stablehlo.reshape %160 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
    %162 = stablehlo.broadcast_in_dim %161, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
    %163 = stablehlo.multiply %154, %162 : tensor<1x7x3072xf32>
    %164 = stablehlo.convert %163 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %165 = stablehlo.convert %164 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
    %166 = stablehlo.multiply %63, %165 : tensor<1x7x3072xf32>
    %167 = stablehlo.convert %166 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
    %168 = stablehlo.reshape %167 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
    %169 = stablehlo.dot_general %168, %arg11, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %170 = stablehlo.reshape %169 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %56, %61, %169, %170 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
2025-08-13 23:25:09.650 (   7.728s) [        CA99B1C0]      module_builder.cc:205      1| SHLO StableHLO Pipeline Module:
module @SyncTensorsGraph.362 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<7xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<1x64x1xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x7xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<128xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:4 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}, {}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>, <@mesh, [{}, {}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg22: tensor<7xi64>, %arg23: tensor<1x64x1xf32>, %arg24: tensor<3072x1024xbf16>, %arg25: tensor<f32>, %arg26: tensor<1x7xi64>, %arg27: tensor<64128x3072xbf16>, %arg28: tensor<3072xbf16>, %arg29: tensor<i64>, %arg30: tensor<1x8x128x128xbf16>, %arg31: tensor<3072x1024xbf16>, %arg32: tensor<1x8x128x128xbf16>, %arg33: tensor<3072x128256xbf16>, %arg34: tensor<8192x3072xbf16>, %arg35: tensor<3072x8192xbf16>, %arg36: tensor<3072x3072xbf16>, %arg37: tensor<128xi64>, %arg38: tensor<7x128xbf16>, %arg39: tensor<f32>, %arg40: tensor<3072x3072xbf16>, %arg41: tensor<3072xbf16>, %arg42: tensor<3072x8192xbf16>, %arg43: tensor<3072xbf16>) {
      %c = stablehlo.constant dense<0> : tensor<7xi64>
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x7xf32>
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x7x3072xf32>
      %2 = stablehlo.compare  LT, %arg22, %c : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi1>
      %3 = stablehlo.broadcast_in_dim %arg29, dims = [] : (tensor<i64>) -> tensor<7xi64>
      %4 = stablehlo.add %arg22, %3 : tensor<7xi64>
      %5 = stablehlo.select %2, %4, %arg22 : tensor<7xi1>, tensor<7xi64>
      %6 = stablehlo.reshape %5 : (tensor<7xi64>) -> tensor<7x1xi64>
      %7 = stablehlo.convert %arg28 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %8 = stablehlo.broadcast_in_dim %7, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %9 = stablehlo.convert %arg26 : (tensor<1x7xi64>) -> tensor<1x7xui32>
      %10 = stablehlo.reshape %9 : (tensor<1x7xui32>) -> tensor<7xui32>
      %11 = "stablehlo.gather"(%arg27, %10) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<64128x3072xbf16>, tensor<7xui32>) -> tensor<7x3072xbf16>
      %12 = stablehlo.reshape %11 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %13 = stablehlo.convert %12 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %14 = stablehlo.power %13, %1 : tensor<1x7x3072xf32>
      %15 = stablehlo.reduce(%14 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %16 = stablehlo.multiply %15, %cst_1 : tensor<1x7xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %18 = stablehlo.broadcast_in_dim %arg25, dims = [] : (tensor<f32>) -> tensor<1x7x1xf32>
      %19 = stablehlo.add %17, %18 : tensor<1x7x1xf32>
      %20 = stablehlo.rsqrt %19 : tensor<1x7x1xf32>
      %21 = stablehlo.reshape %20 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %23 = stablehlo.multiply %13, %22 : tensor<1x7x3072xf32>
      %24 = stablehlo.convert %23 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %25 = stablehlo.convert %24 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %26 = stablehlo.multiply %8, %25 : tensor<1x7x3072xf32>
      %27 = stablehlo.convert %26 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %28 = stablehlo.reshape %27 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %29 = stablehlo.dot_general %28, %arg24, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
      %30 = stablehlo.reshape %29 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
      %31 = stablehlo.transpose %30, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
      %32 = stablehlo.convert %31 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,7,128]{3,1,2,0}"} : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
      %33 = stablehlo.convert %arg22 : (tensor<7xi64>) -> tensor<7xf32>
      %34 = stablehlo.reshape %33 : (tensor<7xf32>) -> tensor<1x1x7xf32>
      %35 = stablehlo.dot_general %arg23, %34, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
      %36 = stablehlo.transpose %35, dims = [0, 2, 1] {result_layout = dense<[1, 2, 0]> : tensor<3xindex>, xla_shape = "f32[1,7,64]{1,2,0}"} : (tensor<1x64x7xf32>) -> tensor<1x7x64xf32>
      %37 = stablehlo.concatenate %36, %36, dim = 2 : (tensor<1x7x64xf32>, tensor<1x7x64xf32>) -> tensor<1x7x128xf32>
      %38 = stablehlo.cosine %37 : tensor<1x7x128xf32>
      %39 = stablehlo.convert %38 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %40 = stablehlo.convert %39 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
      %41 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
      %42 = stablehlo.multiply %32, %41 : tensor<1x8x7x128xf32>
      %43 = stablehlo.convert %42 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
      %44 = stablehlo.slice %31 [0:1, 0:8, 0:7, 64:128] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %45 = stablehlo.negate %44 : tensor<1x8x7x64xbf16>
      %46 = stablehlo.slice %31 [0:1, 0:8, 0:7, 0:64] : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x64xbf16>
      %47 = stablehlo.concatenate %45, %46, dim = 3 : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x128xbf16>
      %48 = stablehlo.convert %47 : (tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xf32>
      %49 = stablehlo.sine %37 : tensor<1x7x128xf32>
      %50 = stablehlo.convert %49 : (tensor<1x7x128xf32>) -> tensor<1x7x128xbf16>
      %51 = stablehlo.convert %50 : (tensor<1x7x128xbf16>) -> tensor<1x7x128xf32>
      %52 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x8x7x128xf32>
      %53 = stablehlo.multiply %48, %52 : tensor<1x8x7x128xf32>
      %54 = stablehlo.convert %53 : (tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xbf16>
      %55 = stablehlo.add %43, %54 : tensor<1x8x7x128xbf16>
      %56 = "stablehlo.scatter"(%arg30, %6, %55) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg44: tensor<bf16>, %arg45: tensor<bf16>):
        stablehlo.return %arg45 : tensor<bf16>
      }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
      %57 = stablehlo.dot_general %28, %arg31, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
      %58 = stablehlo.reshape %57 : (tensor<7x1024xbf16>) -> tensor<1x7x8x128xbf16>
      %59 = stablehlo.transpose %58, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,8,7,128]{3,1,2,0}"} : (tensor<1x7x8x128xbf16>) -> tensor<1x8x7x128xbf16>
      %60 = "stablehlo.scatter"(%arg32, %6, %59) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg44: tensor<bf16>, %arg45: tensor<bf16>):
        stablehlo.return %arg45 : tensor<bf16>
      }) : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>) -> tensor<1x8x128x128xbf16>
      %61 = stablehlo.convert %arg43 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %62 = stablehlo.broadcast_in_dim %61, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %63 = stablehlo.dot_general %28, %arg40, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
      %64 = stablehlo.reshape %63 : (tensor<7x3072xbf16>) -> tensor<1x7x24x128xbf16>
      %65 = stablehlo.transpose %64, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,7,128]{3,1,2,0}"} : (tensor<1x7x24x128xbf16>) -> tensor<1x24x7x128xbf16>
      %66 = stablehlo.convert %65 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,7,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
      %67 = stablehlo.broadcast_in_dim %40, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
      %68 = stablehlo.multiply %66, %67 : tensor<1x24x7x128xf32>
      %69 = stablehlo.convert %68 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
      %70 = stablehlo.slice %65 [0:1, 0:24, 0:7, 64:128] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
      %71 = stablehlo.negate %70 : tensor<1x24x7x64xbf16>
      %72 = stablehlo.slice %65 [0:1, 0:24, 0:7, 0:64] : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x64xbf16>
      %73 = stablehlo.concatenate %71, %72, dim = 3 : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x128xbf16>
      %74 = stablehlo.convert %73 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
      %75 = stablehlo.broadcast_in_dim %51, dims = [0, 2, 3] : (tensor<1x7x128xf32>) -> tensor<1x24x7x128xf32>
      %76 = stablehlo.multiply %74, %75 : tensor<1x24x7x128xf32>
      %77 = stablehlo.convert %76 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
      %78 = stablehlo.add %69, %77 : tensor<1x24x7x128xbf16>
      %79 = stablehlo.reshape %78 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
      %80 = stablehlo.broadcast_in_dim %56, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
      %81 = stablehlo.reshape %80 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
      %82 = stablehlo.transpose %81, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
      %83 = stablehlo.reshape %82 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
      %84 = stablehlo.dot_general %79, %83, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
      %85 = stablehlo.convert %84 : (tensor<24x7x128xbf16>) -> tensor<24x7x128xf32>
      %86 = stablehlo.reshape %85 : (tensor<24x7x128xf32>) -> tensor<1x24x7x128xf32>
      %87 = stablehlo.broadcast_in_dim %arg39, dims = [] : (tensor<f32>) -> tensor<1x24x7x128xf32>
      %88 = stablehlo.multiply %86, %87 : tensor<1x24x7x128xf32>
      %89 = stablehlo.convert %88 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
      %90 = stablehlo.convert %arg38 : (tensor<7x128xbf16>) -> tensor<7x128xf32>
      %91 = stablehlo.broadcast_in_dim %arg37, dims = [1] : (tensor<128xi64>) -> tensor<7x128xi64>
      %92 = stablehlo.broadcast_in_dim %arg22, dims = [0] : (tensor<7xi64>) -> tensor<7x128xi64>
      %93 = stablehlo.compare  GT, %91, %92 : (tensor<7x128xi64>, tensor<7x128xi64>) -> tensor<7x128xi1>
      %94 = stablehlo.convert %93 : (tensor<7x128xi1>) -> tensor<7x128xf32>
      %95 = stablehlo.multiply %90, %94 : tensor<7x128xf32>
      %96 = stablehlo.convert %95 : (tensor<7x128xf32>) -> tensor<7x128xbf16>
      %97 = stablehlo.reshape %96 : (tensor<7x128xbf16>) -> tensor<1x7x128xbf16>
      %98 = stablehlo.broadcast_in_dim %97, dims = [0, 2, 3] : (tensor<1x7x128xbf16>) -> tensor<1x24x7x128xbf16>
      %99 = stablehlo.add %89, %98 : tensor<1x24x7x128xbf16>
      %100 = stablehlo.convert %99 : (tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xf32>
      %101 = stablehlo.reduce(%100 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
      %102 = stablehlo.broadcast_in_dim %101, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
      %103 = stablehlo.subtract %100, %102 : tensor<1x24x7x128xf32>
      %104 = stablehlo.exponential %103 : tensor<1x24x7x128xf32>
      %105 = stablehlo.reduce(%104 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x7x128xf32>, tensor<f32>) -> tensor<1x24x7xf32>
      %106 = stablehlo.broadcast_in_dim %105, dims = [0, 1, 2] : (tensor<1x24x7xf32>) -> tensor<1x24x7x128xf32>
      %107 = stablehlo.divide %104, %106 : tensor<1x24x7x128xf32>
      %108 = stablehlo.convert %107 : (tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xbf16>
      %109 = stablehlo.reshape %108 : (tensor<1x24x7x128xbf16>) -> tensor<24x7x128xbf16>
      %110 = stablehlo.broadcast_in_dim %60, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
      %111 = stablehlo.reshape %110 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
      %112 = stablehlo.dot_general %109, %111, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
      %113 = stablehlo.reshape %112 : (tensor<24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
      %114 = stablehlo.transpose %113, dims = [0, 2, 1, 3] {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "bf16[1,7,24,128]{3,1,2,0}"} : (tensor<1x24x7x128xbf16>) -> tensor<1x7x24x128xbf16>
      %115 = stablehlo.reshape %114 : (tensor<1x7x24x128xbf16>) -> tensor<7x3072xbf16>
      %116 = stablehlo.dot_general %115, %arg36, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
      %117 = stablehlo.reshape %116 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %118 = stablehlo.add %12, %117 : tensor<1x7x3072xbf16>
      %119 = stablehlo.convert %arg41 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %120 = stablehlo.broadcast_in_dim %119, dims = [2] : (tensor<3072xf32>) -> tensor<1x7x3072xf32>
      %121 = stablehlo.convert %118 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %122 = stablehlo.power %121, %1 : tensor<1x7x3072xf32>
      %123 = stablehlo.reduce(%122 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %124 = stablehlo.multiply %123, %cst_1 : tensor<1x7xf32>
      %125 = stablehlo.reshape %124 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %126 = stablehlo.add %125, %18 : tensor<1x7x1xf32>
      %127 = stablehlo.rsqrt %126 : tensor<1x7x1xf32>
      %128 = stablehlo.reshape %127 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %129 = stablehlo.broadcast_in_dim %128, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %130 = stablehlo.multiply %121, %129 : tensor<1x7x3072xf32>
      %131 = stablehlo.convert %130 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %132 = stablehlo.convert %131 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %133 = stablehlo.multiply %120, %132 : tensor<1x7x3072xf32>
      %134 = stablehlo.convert %133 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %135 = stablehlo.reshape %134 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %136 = stablehlo.dot_general %135, %arg42, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
      %137 = stablehlo.reshape %136 : (tensor<7x8192xbf16>) -> tensor<1x7x8192xbf16>
      %138 = stablehlo.convert %137 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %139 = stablehlo.logistic %137 : tensor<1x7x8192xbf16>
      %140 = stablehlo.convert %139 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %141 = stablehlo.multiply %138, %140 : tensor<1x7x8192xf32>
      %142 = stablehlo.convert %141 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %143 = stablehlo.convert %142 : (tensor<1x7x8192xbf16>) -> tensor<1x7x8192xf32>
      %144 = stablehlo.dot_general %135, %arg35, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
      %145 = stablehlo.convert %144 : (tensor<7x8192xbf16>) -> tensor<7x8192xf32>
      %146 = stablehlo.reshape %145 : (tensor<7x8192xf32>) -> tensor<1x7x8192xf32>
      %147 = stablehlo.multiply %143, %146 : tensor<1x7x8192xf32>
      %148 = stablehlo.convert %147 : (tensor<1x7x8192xf32>) -> tensor<1x7x8192xbf16>
      %149 = stablehlo.reshape %148 : (tensor<1x7x8192xbf16>) -> tensor<7x8192xbf16>
      %150 = stablehlo.dot_general %149, %arg34, contracting_dims = [1] x [0] : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
      %151 = stablehlo.reshape %150 : (tensor<7x3072xbf16>) -> tensor<1x7x3072xbf16>
      %152 = stablehlo.add %118, %151 : tensor<1x7x3072xbf16>
      %153 = stablehlo.convert %152 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %154 = stablehlo.power %153, %1 : tensor<1x7x3072xf32>
      %155 = stablehlo.reduce(%154 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x7x3072xf32>, tensor<f32>) -> tensor<1x7xf32>
      %156 = stablehlo.multiply %155, %cst_1 : tensor<1x7xf32>
      %157 = stablehlo.reshape %156 : (tensor<1x7xf32>) -> tensor<1x7x1xf32>
      %158 = stablehlo.add %157, %18 : tensor<1x7x1xf32>
      %159 = stablehlo.rsqrt %158 : tensor<1x7x1xf32>
      %160 = stablehlo.reshape %159 : (tensor<1x7x1xf32>) -> tensor<1x7xf32>
      %161 = stablehlo.broadcast_in_dim %160, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<1x7x3072xf32>
      %162 = stablehlo.multiply %153, %161 : tensor<1x7x3072xf32>
      %163 = stablehlo.convert %162 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %164 = stablehlo.convert %163 : (tensor<1x7x3072xbf16>) -> tensor<1x7x3072xf32>
      %165 = stablehlo.multiply %62, %164 : tensor<1x7x3072xf32>
      %166 = stablehlo.convert %165 : (tensor<1x7x3072xf32>) -> tensor<1x7x3072xbf16>
      %167 = stablehlo.reshape %166 : (tensor<1x7x3072xbf16>) -> tensor<7x3072xbf16>
      %168 = stablehlo.dot_general %167, %arg33, contracting_dims = [1] x [0] : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
      %169 = stablehlo.reshape %168 : (tensor<7x128256xbf16>) -> tensor<1x7x128256xbf16>
      sdy.return %56, %60, %168, %169 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
    } : (tensor<7xi64>, tensor<1x64x1xf32>, tensor<3072x1024xbf16>, tensor<f32>, tensor<1x7xi64>, tensor<128256x3072xbf16>, tensor<3072xbf16>, tensor<i64>, tensor<1x8x128x128xbf16>, tensor<3072x1024xbf16>, tensor<1x8x128x128xbf16>, tensor<3072x128256xbf16>, tensor<8192x3072xbf16>, tensor<3072x8192xbf16>, tensor<3072x3072xbf16>, tensor<128xi64>, tensor<7x128xbf16>, tensor<f32>, tensor<3072x3072xbf16>, tensor<3072xbf16>, tensor<3072x8192xbf16>, tensor<3072xbf16>) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>)
    return %0#0, %0#1, %0#2, %0#3 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
2025-08-13 23:25:09.659 (   7.738s) [        CA99B1C0]      module_builder.cc:452      1| TTIR Module:
module @SyncTensorsGraph.362 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<7xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<1x64x1xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x7xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<128xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<7x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = ttir.empty() : tensor<7xi64>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %2 = ttir.empty() : tensor<1x64x1xf32>
    %3 = "ttir.mesh_shard"(%arg1, %2) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x64x1xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %4 = ttir.empty() : tensor<3072x1024xbf16>
    %5 = "ttir.mesh_shard"(%arg2, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x1024xbf16>, tensor<3072x1024xbf16>) -> tensor<3072x1024xbf16>
    %6 = ttir.empty() : tensor<f32>
    %7 = "ttir.mesh_shard"(%arg3, %6) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %8 = ttir.empty() : tensor<1x7xi64>
    %9 = "ttir.mesh_shard"(%arg4, %8) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x7xi64>, tensor<1x7xi64>) -> tensor<1x7xi64>
    %10 = ttir.empty() : tensor<64128x3072xbf16>
    %11 = "ttir.mesh_shard"(%arg5, %10) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<64128x3072xbf16>) -> tensor<64128x3072xbf16>
    %12 = ttir.empty() : tensor<3072xbf16>
    %13 = "ttir.mesh_shard"(%arg6, %12) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %14 = ttir.empty() : tensor<i64>
    %15 = "ttir.mesh_shard"(%arg7, %14) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %16 = ttir.empty() : tensor<1x8x128x128xbf16>
    %17 = "ttir.mesh_shard"(%arg8, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %18 = ttir.empty() : tensor<3072x1024xbf16>
    %19 = "ttir.mesh_shard"(%arg9, %18) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x1024xbf16>, tensor<3072x1024xbf16>) -> tensor<3072x1024xbf16>
    %20 = ttir.empty() : tensor<1x8x128x128xbf16>
    %21 = "ttir.mesh_shard"(%arg10, %20) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %22 = ttir.empty() : tensor<3072x128256xbf16>
    %23 = "ttir.mesh_shard"(%arg11, %22) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x128256xbf16>, tensor<3072x128256xbf16>) -> tensor<3072x128256xbf16>
    %24 = ttir.empty() : tensor<8192x3072xbf16>
    %25 = "ttir.mesh_shard"(%arg12, %24) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %26 = ttir.empty() : tensor<3072x8192xbf16>
    %27 = "ttir.mesh_shard"(%arg13, %26) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16>, tensor<3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %28 = ttir.empty() : tensor<3072x3072xbf16>
    %29 = "ttir.mesh_shard"(%arg14, %28) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %30 = ttir.empty() : tensor<128xi64>
    %31 = "ttir.mesh_shard"(%arg15, %30) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128xi64>, tensor<128xi64>) -> tensor<128xi64>
    %32 = ttir.empty() : tensor<7x128xbf16>
    %33 = "ttir.mesh_shard"(%arg16, %32) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7x128xbf16>, tensor<7x128xbf16>) -> tensor<7x128xbf16>
    %34 = ttir.empty() : tensor<f32>
    %35 = "ttir.mesh_shard"(%arg17, %34) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %36 = ttir.empty() : tensor<3072x3072xbf16>
    %37 = "ttir.mesh_shard"(%arg18, %36) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %38 = ttir.empty() : tensor<3072xbf16>
    %39 = "ttir.mesh_shard"(%arg19, %38) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %40 = ttir.empty() : tensor<3072x8192xbf16>
    %41 = "ttir.mesh_shard"(%arg20, %40) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16>, tensor<3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %42 = ttir.empty() : tensor<3072xbf16>
    %43 = "ttir.mesh_shard"(%arg21, %42) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %44 = "ttir.constant"() <{value = dense<0> : tensor<7xi64>}> : () -> tensor<7xi64>
    %45 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %46 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %47 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x7xf32>}> : () -> tensor<1x7xf32>
    %48 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %49 = ttir.empty() : tensor<1x1x1xf32>
    %50 = "ttir.reshape"(%48, %49) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %51 = ttir.empty() : tensor<1x7x3072xf32>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 7, 3072>}> : (tensor<1x1x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %53 = ttir.empty() : tensor<7xi1>
    %54 = "ttir.lt"(%1, %44, %53) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi1>) -> tensor<7xi1>
    %55 = ttir.empty() : tensor<1xi64>
    %56 = "ttir.reshape"(%15, %55) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %57 = ttir.empty() : tensor<7xi64>
    %58 = "ttir.broadcast"(%56, %57) <{broadcast_dimensions = array<i64: 7>}> : (tensor<1xi64>, tensor<7xi64>) -> tensor<7xi64>
    %59 = ttir.empty() : tensor<7xi64>
    %60 = "ttir.add"(%1, %58, %59) : (tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %61 = ttir.empty() : tensor<7xi64>
    %62 = "ttir.where"(%54, %60, %1, %61) : (tensor<7xi1>, tensor<7xi64>, tensor<7xi64>, tensor<7xi64>) -> tensor<7xi64>
    %63 = ttir.empty() : tensor<7x1xi64>
    %64 = "ttir.reshape"(%62, %63) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %65 = ttir.empty() : tensor<3072xf32>
    %66 = "ttir.typecast"(%13, %65) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %67 = ttir.empty() : tensor<1x1x3072xf32>
    %68 = "ttir.reshape"(%66, %67) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %69 = ttir.empty() : tensor<1x7x3072xf32>
    %70 = "ttir.broadcast"(%68, %69) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %71 = ttir.empty() : tensor<1x7xui32>
    %72 = "ttir.typecast"(%9, %71) <{conservative_folding = false}> : (tensor<1x7xi64>, tensor<1x7xui32>) -> tensor<1x7xui32>
    %73 = ttir.empty() : tensor<7xui32>
    %74 = "ttir.reshape"(%72, %73) <{shape = [7 : i32]}> : (tensor<1x7xui32>, tensor<7xui32>) -> tensor<7xui32>
    %75 = ttir.empty() : tensor<7x3072xbf16>
    %76 = "ttir.gather"(%11, %74, %75) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<64128x3072xbf16>, tensor<7xui32>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %77 = ttir.empty() : tensor<1x7x3072xbf16>
    %78 = "ttir.reshape"(%76, %77) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %79 = ttir.empty() : tensor<1x7x3072xf32>
    %80 = "ttir.typecast"(%78, %79) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %81 = ttir.empty() : tensor<1x7x3072xf32>
    %82 = "ttir.pow"(%80, %52, %81) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %83 = ttir.empty() : tensor<1x7xf32>
    %84 = "ttir.sum"(%82, %83) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %85 = ttir.empty() : tensor<1x7xf32>
    %86 = "ttir.multiply"(%84, %47, %85) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %87 = ttir.empty() : tensor<1x7x1xf32>
    %88 = "ttir.reshape"(%86, %87) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %89 = ttir.empty() : tensor<1x1x1xf32>
    %90 = "ttir.reshape"(%7, %89) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %91 = ttir.empty() : tensor<1x7x1xf32>
    %92 = "ttir.broadcast"(%90, %91) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %93 = ttir.empty() : tensor<1x7x1xf32>
    %94 = "ttir.add"(%88, %92, %93) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %95 = ttir.empty() : tensor<1x7x1xf32>
    %96 = "ttir.rsqrt"(%94, %95) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %97 = ttir.empty() : tensor<1x7xf32>
    %98 = "ttir.reshape"(%96, %97) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %99 = ttir.empty() : tensor<1x7x1xf32>
    %100 = "ttir.reshape"(%98, %99) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %101 = ttir.empty() : tensor<1x7x3072xf32>
    %102 = "ttir.broadcast"(%100, %101) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %103 = ttir.empty() : tensor<1x7x3072xf32>
    %104 = "ttir.multiply"(%80, %102, %103) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %105 = ttir.empty() : tensor<1x7x3072xbf16>
    %106 = "ttir.typecast"(%104, %105) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %107 = ttir.empty() : tensor<1x7x3072xf32>
    %108 = "ttir.typecast"(%106, %107) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %109 = ttir.empty() : tensor<1x7x3072xf32>
    %110 = "ttir.multiply"(%70, %108, %109) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %111 = ttir.empty() : tensor<1x7x3072xbf16>
    %112 = "ttir.typecast"(%110, %111) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %113 = ttir.empty() : tensor<7x3072xbf16>
    %114 = "ttir.reshape"(%112, %113) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %115 = "ttir.dot_general"(%114, %5) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %116 = ttir.empty() : tensor<1x7x8x128xbf16>
    %117 = "ttir.reshape"(%115, %116) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16>, tensor<1x7x8x128xbf16>) -> tensor<1x7x8x128xbf16>
    %118 = ttir.empty() : tensor<1x8x7x128xbf16>
    %119 = "ttir.permute"(%117, %118) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %120 = ttir.empty() : tensor<1x8x7x128xf32>
    %121 = "ttir.typecast"(%119, %120) <{conservative_folding = false}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %122 = ttir.empty() : tensor<7xf32>
    %123 = "ttir.typecast"(%1, %122) <{conservative_folding = false}> : (tensor<7xi64>, tensor<7xf32>) -> tensor<7xf32>
    %124 = ttir.empty() : tensor<1x1x7xf32>
    %125 = "ttir.reshape"(%123, %124) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xf32>, tensor<1x1x7xf32>) -> tensor<1x1x7xf32>
    %126 = "ttir.dot_general"(%3, %125) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x7xf32>) -> tensor<1x64x7xf32>
    %127 = ttir.empty() : tensor<1x7x64xf32>
    %128 = "ttir.permute"(%126, %127) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32>, tensor<1x7x64xf32>) -> tensor<1x7x64xf32>
    %129 = ttir.empty() : tensor<1x7x128xf32>
    %130 = "ttir.concat"(%128, %128, %129) <{dim = 2 : si32}> : (tensor<1x7x64xf32>, tensor<1x7x64xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %131 = ttir.empty() : tensor<1x7x128xf32>
    %132 = "ttir.cos"(%130, %131) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %133 = ttir.empty() : tensor<1x7x128xbf16>
    %134 = "ttir.typecast"(%132, %133) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %135 = ttir.empty() : tensor<1x7x128xf32>
    %136 = "ttir.typecast"(%134, %135) <{conservative_folding = false}> : (tensor<1x7x128xbf16>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %137 = ttir.empty() : tensor<1x1x7x128xf32>
    %138 = "ttir.reshape"(%136, %137) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %139 = ttir.empty() : tensor<1x8x7x128xf32>
    %140 = "ttir.broadcast"(%138, %139) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %141 = ttir.empty() : tensor<1x8x7x128xf32>
    %142 = "ttir.multiply"(%121, %140, %141) : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %143 = ttir.empty() : tensor<1x8x7x128xbf16>
    %144 = "ttir.typecast"(%142, %143) <{conservative_folding = false}> : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %145 = ttir.empty() : tensor<1x8x7x64xbf16>
    %146 = "ttir.slice"(%119, %145) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %147 = ttir.empty() : tensor<1x8x7x64xbf16>
    %148 = "ttir.neg"(%146, %147) : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %149 = ttir.empty() : tensor<1x8x7x64xbf16>
    %150 = "ttir.slice"(%119, %149) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x64xbf16>) -> tensor<1x8x7x64xbf16>
    %151 = ttir.empty() : tensor<1x8x7x128xbf16>
    %152 = "ttir.concat"(%148, %150, %151) <{dim = 3 : si32}> : (tensor<1x8x7x64xbf16>, tensor<1x8x7x64xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %153 = ttir.empty() : tensor<1x8x7x128xf32>
    %154 = "ttir.typecast"(%152, %153) <{conservative_folding = false}> : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %155 = ttir.empty() : tensor<1x7x128xf32>
    %156 = "ttir.sin"(%130, %155) : (tensor<1x7x128xf32>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %157 = ttir.empty() : tensor<1x7x128xbf16>
    %158 = "ttir.typecast"(%156, %157) <{conservative_folding = false}> : (tensor<1x7x128xf32>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %159 = ttir.empty() : tensor<1x7x128xf32>
    %160 = "ttir.typecast"(%158, %159) <{conservative_folding = false}> : (tensor<1x7x128xbf16>, tensor<1x7x128xf32>) -> tensor<1x7x128xf32>
    %161 = ttir.empty() : tensor<1x1x7x128xf32>
    %162 = "ttir.reshape"(%160, %161) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %163 = ttir.empty() : tensor<1x8x7x128xf32>
    %164 = "ttir.broadcast"(%162, %163) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %165 = ttir.empty() : tensor<1x8x7x128xf32>
    %166 = "ttir.multiply"(%154, %164, %165) : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>, tensor<1x8x7x128xf32>) -> tensor<1x8x7x128xf32>
    %167 = ttir.empty() : tensor<1x8x7x128xbf16>
    %168 = "ttir.typecast"(%166, %167) <{conservative_folding = false}> : (tensor<1x8x7x128xf32>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %169 = ttir.empty() : tensor<1x8x7x128xbf16>
    %170 = "ttir.add"(%144, %168, %169) : (tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %171 = ttir.empty() : tensor<1x8x128x128xbf16>
    %172 = "ttir.scatter"(%17, %64, %170, %171) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %173 = "ttir.dot_general"(%114, %19) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<7x1024xbf16>
    %174 = ttir.empty() : tensor<1x7x8x128xbf16>
    %175 = "ttir.reshape"(%173, %174) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16>, tensor<1x7x8x128xbf16>) -> tensor<1x7x8x128xbf16>
    %176 = ttir.empty() : tensor<1x8x7x128xbf16>
    %177 = "ttir.permute"(%175, %176) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16>, tensor<1x8x7x128xbf16>) -> tensor<1x8x7x128xbf16>
    %178 = ttir.empty() : tensor<1x8x128x128xbf16>
    %179 = "ttir.scatter"(%21, %64, %177, %178) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<7x1xi64>, tensor<1x8x7x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %180 = ttir.empty() : tensor<3072xf32>
    %181 = "ttir.typecast"(%43, %180) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %182 = ttir.empty() : tensor<1x1x3072xf32>
    %183 = "ttir.reshape"(%181, %182) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %184 = ttir.empty() : tensor<1x7x3072xf32>
    %185 = "ttir.broadcast"(%183, %184) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %186 = "ttir.dot_general"(%114, %37) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %187 = ttir.empty() : tensor<1x7x24x128xbf16>
    %188 = "ttir.reshape"(%186, %187) <{shape = [1 : i32, 7 : i32, 24 : i32, 128 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x24x128xbf16>) -> tensor<1x7x24x128xbf16>
    %189 = ttir.empty() : tensor<1x24x7x128xbf16>
    %190 = "ttir.permute"(%188, %189) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x24x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %191 = ttir.empty() : tensor<1x24x7x128xf32>
    %192 = "ttir.typecast"(%190, %191) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %193 = ttir.empty() : tensor<1x1x7x128xf32>
    %194 = "ttir.reshape"(%136, %193) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %195 = ttir.empty() : tensor<1x24x7x128xf32>
    %196 = "ttir.broadcast"(%194, %195) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %197 = ttir.empty() : tensor<1x24x7x128xf32>
    %198 = "ttir.multiply"(%192, %196, %197) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %199 = ttir.empty() : tensor<1x24x7x128xbf16>
    %200 = "ttir.typecast"(%198, %199) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %201 = ttir.empty() : tensor<1x24x7x64xbf16>
    %202 = "ttir.slice"(%190, %201) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %203 = ttir.empty() : tensor<1x24x7x64xbf16>
    %204 = "ttir.neg"(%202, %203) : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %205 = ttir.empty() : tensor<1x24x7x64xbf16>
    %206 = "ttir.slice"(%190, %205) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x64xbf16>) -> tensor<1x24x7x64xbf16>
    %207 = ttir.empty() : tensor<1x24x7x128xbf16>
    %208 = "ttir.concat"(%204, %206, %207) <{dim = 3 : si32}> : (tensor<1x24x7x64xbf16>, tensor<1x24x7x64xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %209 = ttir.empty() : tensor<1x24x7x128xf32>
    %210 = "ttir.typecast"(%208, %209) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %211 = ttir.empty() : tensor<1x1x7x128xf32>
    %212 = "ttir.reshape"(%160, %211) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32>, tensor<1x1x7x128xf32>) -> tensor<1x1x7x128xf32>
    %213 = ttir.empty() : tensor<1x24x7x128xf32>
    %214 = "ttir.broadcast"(%212, %213) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %215 = ttir.empty() : tensor<1x24x7x128xf32>
    %216 = "ttir.multiply"(%210, %214, %215) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %217 = ttir.empty() : tensor<1x24x7x128xbf16>
    %218 = "ttir.typecast"(%216, %217) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %219 = ttir.empty() : tensor<1x24x7x128xbf16>
    %220 = "ttir.add"(%200, %218, %219) : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %221 = ttir.empty() : tensor<24x7x128xbf16>
    %222 = "ttir.reshape"(%220, %221) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %223 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %224 = "ttir.reshape"(%172, %223) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %225 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %226 = "ttir.broadcast"(%224, %225) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %227 = ttir.empty() : tensor<1x24x128x128xbf16>
    %228 = "ttir.reshape"(%226, %227) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %229 = ttir.empty() : tensor<1x24x128x128xbf16>
    %230 = "ttir.permute"(%228, %229) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %231 = ttir.empty() : tensor<24x128x128xbf16>
    %232 = "ttir.reshape"(%230, %231) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %233 = "ttir.dot_general"(%222, %232) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %234 = ttir.empty() : tensor<24x7x128xf32>
    %235 = "ttir.typecast"(%233, %234) <{conservative_folding = false}> : (tensor<24x7x128xbf16>, tensor<24x7x128xf32>) -> tensor<24x7x128xf32>
    %236 = ttir.empty() : tensor<1x24x7x128xf32>
    %237 = "ttir.reshape"(%235, %236) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %238 = ttir.empty() : tensor<1x1x1x1xf32>
    %239 = "ttir.reshape"(%35, %238) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %240 = ttir.empty() : tensor<1x24x7x128xf32>
    %241 = "ttir.broadcast"(%239, %240) <{broadcast_dimensions = array<i64: 1, 24, 7, 128>}> : (tensor<1x1x1x1xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %242 = ttir.empty() : tensor<1x24x7x128xf32>
    %243 = "ttir.multiply"(%237, %241, %242) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %244 = ttir.empty() : tensor<1x24x7x128xbf16>
    %245 = "ttir.typecast"(%243, %244) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %246 = ttir.empty() : tensor<7x128xf32>
    %247 = "ttir.typecast"(%33, %246) <{conservative_folding = false}> : (tensor<7x128xbf16>, tensor<7x128xf32>) -> tensor<7x128xf32>
    %248 = ttir.empty() : tensor<1x128xi64>
    %249 = "ttir.reshape"(%31, %248) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xi64>, tensor<1x128xi64>) -> tensor<1x128xi64>
    %250 = ttir.empty() : tensor<7x128xi64>
    %251 = "ttir.broadcast"(%249, %250) <{broadcast_dimensions = array<i64: 7, 1>}> : (tensor<1x128xi64>, tensor<7x128xi64>) -> tensor<7x128xi64>
    %252 = ttir.empty() : tensor<7x1xi64>
    %253 = "ttir.reshape"(%1, %252) <{shape = [7 : i32, 1 : i32]}> : (tensor<7xi64>, tensor<7x1xi64>) -> tensor<7x1xi64>
    %254 = ttir.empty() : tensor<7x128xi64>
    %255 = "ttir.broadcast"(%253, %254) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<7x1xi64>, tensor<7x128xi64>) -> tensor<7x128xi64>
    %256 = ttir.empty() : tensor<7x128xi1>
    %257 = "ttir.gt"(%251, %255, %256) : (tensor<7x128xi64>, tensor<7x128xi64>, tensor<7x128xi1>) -> tensor<7x128xi1>
    %258 = ttir.empty() : tensor<7x128xf32>
    %259 = "ttir.typecast"(%257, %258) <{conservative_folding = false}> : (tensor<7x128xi1>, tensor<7x128xf32>) -> tensor<7x128xf32>
    %260 = ttir.empty() : tensor<7x128xf32>
    %261 = "ttir.multiply"(%247, %259, %260) : (tensor<7x128xf32>, tensor<7x128xf32>, tensor<7x128xf32>) -> tensor<7x128xf32>
    %262 = ttir.empty() : tensor<7x128xbf16>
    %263 = "ttir.typecast"(%261, %262) <{conservative_folding = false}> : (tensor<7x128xf32>, tensor<7x128xbf16>) -> tensor<7x128xbf16>
    %264 = ttir.empty() : tensor<1x7x128xbf16>
    %265 = "ttir.reshape"(%263, %264) <{shape = [1 : i32, 7 : i32, 128 : i32]}> : (tensor<7x128xbf16>, tensor<1x7x128xbf16>) -> tensor<1x7x128xbf16>
    %266 = ttir.empty() : tensor<1x1x7x128xbf16>
    %267 = "ttir.reshape"(%265, %266) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xbf16>, tensor<1x1x7x128xbf16>) -> tensor<1x1x7x128xbf16>
    %268 = ttir.empty() : tensor<1x24x7x128xbf16>
    %269 = "ttir.broadcast"(%267, %268) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %270 = ttir.empty() : tensor<1x24x7x128xbf16>
    %271 = "ttir.add"(%245, %269, %270) : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %272 = ttir.empty() : tensor<1x24x7x128xf32>
    %273 = "ttir.typecast"(%271, %272) <{conservative_folding = false}> : (tensor<1x24x7x128xbf16>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %274 = ttir.empty() : tensor<1x24x7xf32>
    %275 = "ttir.max"(%273, %274) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7xf32>) -> tensor<1x24x7xf32>
    %276 = ttir.empty() : tensor<1x24x7x1xf32>
    %277 = "ttir.reshape"(%275, %276) <{shape = [1 : i32, 24 : i32, 7 : i32, 1 : i32]}> : (tensor<1x24x7xf32>, tensor<1x24x7x1xf32>) -> tensor<1x24x7x1xf32>
    %278 = ttir.empty() : tensor<1x24x7x128xf32>
    %279 = "ttir.broadcast"(%277, %278) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x7x1xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %280 = ttir.empty() : tensor<1x24x7x128xf32>
    %281 = "ttir.subtract"(%273, %279, %280) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %282 = ttir.empty() : tensor<1x24x7x128xf32>
    %283 = "ttir.exp"(%281, %282) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %284 = ttir.empty() : tensor<1x24x7xf32>
    %285 = "ttir.sum"(%283, %284) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7xf32>) -> tensor<1x24x7xf32>
    %286 = ttir.empty() : tensor<1x24x7x1xf32>
    %287 = "ttir.reshape"(%285, %286) <{shape = [1 : i32, 24 : i32, 7 : i32, 1 : i32]}> : (tensor<1x24x7xf32>, tensor<1x24x7x1xf32>) -> tensor<1x24x7x1xf32>
    %288 = ttir.empty() : tensor<1x24x7x128xf32>
    %289 = "ttir.broadcast"(%287, %288) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x7x1xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %290 = ttir.empty() : tensor<1x24x7x128xf32>
    %291 = "ttir.div"(%283, %289, %290) : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>, tensor<1x24x7x128xf32>) -> tensor<1x24x7x128xf32>
    %292 = ttir.empty() : tensor<1x24x7x128xbf16>
    %293 = "ttir.typecast"(%291, %292) <{conservative_folding = false}> : (tensor<1x24x7x128xf32>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %294 = ttir.empty() : tensor<24x7x128xbf16>
    %295 = "ttir.reshape"(%293, %294) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xbf16>, tensor<24x7x128xbf16>) -> tensor<24x7x128xbf16>
    %296 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %297 = "ttir.reshape"(%179, %296) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %298 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %299 = "ttir.broadcast"(%297, %298) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %300 = ttir.empty() : tensor<24x128x128xbf16>
    %301 = "ttir.reshape"(%299, %300) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %302 = "ttir.dot_general"(%295, %301) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x7x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x7x128xbf16>
    %303 = ttir.empty() : tensor<1x24x7x128xbf16>
    %304 = "ttir.reshape"(%302, %303) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xbf16>, tensor<1x24x7x128xbf16>) -> tensor<1x24x7x128xbf16>
    %305 = ttir.empty() : tensor<1x7x24x128xbf16>
    %306 = "ttir.permute"(%304, %305) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x24x7x128xbf16>, tensor<1x7x24x128xbf16>) -> tensor<1x7x24x128xbf16>
    %307 = ttir.empty() : tensor<7x3072xbf16>
    %308 = "ttir.reshape"(%306, %307) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x24x128xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %309 = "ttir.dot_general"(%308, %29) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<7x3072xbf16>
    %310 = ttir.empty() : tensor<1x7x3072xbf16>
    %311 = "ttir.reshape"(%309, %310) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %312 = ttir.empty() : tensor<1x7x3072xbf16>
    %313 = "ttir.add"(%78, %311, %312) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %314 = ttir.empty() : tensor<3072xf32>
    %315 = "ttir.typecast"(%39, %314) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %316 = ttir.empty() : tensor<1x1x3072xf32>
    %317 = "ttir.reshape"(%315, %316) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %318 = ttir.empty() : tensor<1x7x3072xf32>
    %319 = "ttir.broadcast"(%317, %318) <{broadcast_dimensions = array<i64: 1, 7, 1>}> : (tensor<1x1x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %320 = ttir.empty() : tensor<1x7x3072xf32>
    %321 = "ttir.typecast"(%313, %320) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %322 = ttir.empty() : tensor<1x7x3072xf32>
    %323 = "ttir.pow"(%321, %52, %322) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %324 = ttir.empty() : tensor<1x7xf32>
    %325 = "ttir.sum"(%323, %324) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %326 = ttir.empty() : tensor<1x7xf32>
    %327 = "ttir.multiply"(%325, %47, %326) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %328 = ttir.empty() : tensor<1x7x1xf32>
    %329 = "ttir.reshape"(%327, %328) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %330 = ttir.empty() : tensor<1x7x1xf32>
    %331 = "ttir.add"(%329, %92, %330) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %332 = ttir.empty() : tensor<1x7x1xf32>
    %333 = "ttir.rsqrt"(%331, %332) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %334 = ttir.empty() : tensor<1x7xf32>
    %335 = "ttir.reshape"(%333, %334) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %336 = ttir.empty() : tensor<1x7x1xf32>
    %337 = "ttir.reshape"(%335, %336) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %338 = ttir.empty() : tensor<1x7x3072xf32>
    %339 = "ttir.broadcast"(%337, %338) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %340 = ttir.empty() : tensor<1x7x3072xf32>
    %341 = "ttir.multiply"(%321, %339, %340) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %342 = ttir.empty() : tensor<1x7x3072xbf16>
    %343 = "ttir.typecast"(%341, %342) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %344 = ttir.empty() : tensor<1x7x3072xf32>
    %345 = "ttir.typecast"(%343, %344) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %346 = ttir.empty() : tensor<1x7x3072xf32>
    %347 = "ttir.multiply"(%319, %345, %346) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %348 = ttir.empty() : tensor<1x7x3072xbf16>
    %349 = "ttir.typecast"(%347, %348) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %350 = ttir.empty() : tensor<7x3072xbf16>
    %351 = "ttir.reshape"(%349, %350) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %352 = "ttir.dot_general"(%351, %41) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %353 = ttir.empty() : tensor<1x7x8192xbf16>
    %354 = "ttir.reshape"(%352, %353) <{shape = [1 : i32, 7 : i32, 8192 : i32]}> : (tensor<7x8192xbf16>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %355 = ttir.empty() : tensor<1x7x8192xf32>
    %356 = "ttir.typecast"(%354, %355) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %357 = ttir.empty() : tensor<1x7x8192xbf16>
    %358 = "ttir.sigmoid"(%354, %357) : (tensor<1x7x8192xbf16>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %359 = ttir.empty() : tensor<1x7x8192xf32>
    %360 = "ttir.typecast"(%358, %359) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %361 = ttir.empty() : tensor<1x7x8192xf32>
    %362 = "ttir.multiply"(%356, %360, %361) : (tensor<1x7x8192xf32>, tensor<1x7x8192xf32>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %363 = ttir.empty() : tensor<1x7x8192xbf16>
    %364 = "ttir.typecast"(%362, %363) <{conservative_folding = false}> : (tensor<1x7x8192xf32>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %365 = ttir.empty() : tensor<1x7x8192xf32>
    %366 = "ttir.typecast"(%364, %365) <{conservative_folding = false}> : (tensor<1x7x8192xbf16>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %367 = "ttir.dot_general"(%351, %27) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<7x8192xbf16>
    %368 = ttir.empty() : tensor<7x8192xf32>
    %369 = "ttir.typecast"(%367, %368) <{conservative_folding = false}> : (tensor<7x8192xbf16>, tensor<7x8192xf32>) -> tensor<7x8192xf32>
    %370 = ttir.empty() : tensor<1x7x8192xf32>
    %371 = "ttir.reshape"(%369, %370) <{shape = [1 : i32, 7 : i32, 8192 : i32]}> : (tensor<7x8192xf32>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %372 = ttir.empty() : tensor<1x7x8192xf32>
    %373 = "ttir.multiply"(%366, %371, %372) : (tensor<1x7x8192xf32>, tensor<1x7x8192xf32>, tensor<1x7x8192xf32>) -> tensor<1x7x8192xf32>
    %374 = ttir.empty() : tensor<1x7x8192xbf16>
    %375 = "ttir.typecast"(%373, %374) <{conservative_folding = false}> : (tensor<1x7x8192xf32>, tensor<1x7x8192xbf16>) -> tensor<1x7x8192xbf16>
    %376 = ttir.empty() : tensor<7x8192xbf16>
    %377 = "ttir.reshape"(%375, %376) <{shape = [7 : i32, 8192 : i32]}> : (tensor<1x7x8192xbf16>, tensor<7x8192xbf16>) -> tensor<7x8192xbf16>
    %378 = "ttir.dot_general"(%377, %25) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<7x3072xbf16>
    %379 = ttir.empty() : tensor<1x7x3072xbf16>
    %380 = "ttir.reshape"(%378, %379) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %381 = ttir.empty() : tensor<1x7x3072xbf16>
    %382 = "ttir.add"(%313, %380, %381) : (tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %383 = ttir.empty() : tensor<1x7x3072xf32>
    %384 = "ttir.typecast"(%382, %383) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %385 = ttir.empty() : tensor<1x7x3072xf32>
    %386 = "ttir.pow"(%384, %52, %385) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %387 = ttir.empty() : tensor<1x7xf32>
    %388 = "ttir.sum"(%386, %387) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %389 = ttir.empty() : tensor<1x7xf32>
    %390 = "ttir.multiply"(%388, %47, %389) : (tensor<1x7xf32>, tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %391 = ttir.empty() : tensor<1x7x1xf32>
    %392 = "ttir.reshape"(%390, %391) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %393 = ttir.empty() : tensor<1x7x1xf32>
    %394 = "ttir.add"(%392, %92, %393) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %395 = ttir.empty() : tensor<1x7x1xf32>
    %396 = "ttir.rsqrt"(%394, %395) : (tensor<1x7x1xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %397 = ttir.empty() : tensor<1x7xf32>
    %398 = "ttir.reshape"(%396, %397) <{shape = [1 : i32, 7 : i32]}> : (tensor<1x7x1xf32>, tensor<1x7xf32>) -> tensor<1x7xf32>
    %399 = ttir.empty() : tensor<1x7x1xf32>
    %400 = "ttir.reshape"(%398, %399) <{shape = [1 : i32, 7 : i32, 1 : i32]}> : (tensor<1x7xf32>, tensor<1x7x1xf32>) -> tensor<1x7x1xf32>
    %401 = ttir.empty() : tensor<1x7x3072xf32>
    %402 = "ttir.broadcast"(%400, %401) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x7x1xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %403 = ttir.empty() : tensor<1x7x3072xf32>
    %404 = "ttir.multiply"(%384, %402, %403) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %405 = ttir.empty() : tensor<1x7x3072xbf16>
    %406 = "ttir.typecast"(%404, %405) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %407 = ttir.empty() : tensor<1x7x3072xf32>
    %408 = "ttir.typecast"(%406, %407) <{conservative_folding = false}> : (tensor<1x7x3072xbf16>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %409 = ttir.empty() : tensor<1x7x3072xf32>
    %410 = "ttir.multiply"(%185, %408, %409) : (tensor<1x7x3072xf32>, tensor<1x7x3072xf32>, tensor<1x7x3072xf32>) -> tensor<1x7x3072xf32>
    %411 = ttir.empty() : tensor<1x7x3072xbf16>
    %412 = "ttir.typecast"(%410, %411) <{conservative_folding = false}> : (tensor<1x7x3072xf32>, tensor<1x7x3072xbf16>) -> tensor<1x7x3072xbf16>
    %413 = ttir.empty() : tensor<7x3072xbf16>
    %414 = "ttir.reshape"(%412, %413) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x3072xbf16>, tensor<7x3072xbf16>) -> tensor<7x3072xbf16>
    %415 = "ttir.dot_general"(%414, %23) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<7x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<7x128256xbf16>
    %416 = ttir.empty() : tensor<1x7x128256xbf16>
    %417 = "ttir.reshape"(%415, %416) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16>, tensor<1x7x128256xbf16>) -> tensor<1x7x128256xbf16>
    %418 = ttir.empty() : tensor<1x8x128x128xbf16>
    %419 = "ttir.mesh_shard"(%172, %418) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %420 = ttir.empty() : tensor<1x8x128x128xbf16>
    %421 = "ttir.mesh_shard"(%179, %420) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %422 = ttir.empty() : tensor<7x128256xbf16>
    %423 = "ttir.mesh_shard"(%415, %422) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<7x128256xbf16>, tensor<7x128256xbf16>) -> tensor<7x128256xbf16>
    %424 = ttir.empty() : tensor<1x7x128256xbf16>
    %425 = "ttir.mesh_shard"(%417, %424) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x7x128256xbf16>, tensor<1x7x128256xbf16>) -> tensor<1x7x128256xbf16>
    return %419, %421, %423, %425 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<7x128256xbf16>, tensor<1x7x128256xbf16>
  }
}
2025-08-13 23:25:09.667 (   7.745s) [        CA99B1C0]      module_builder.cc:506   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-08-13 23:25:09.667 (   7.745s) [        CA99B1C0]      module_builder.cc:520   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-08-13 23:25:09.667 (   7.745s) [        CA99B1C0]      module_builder.cc:528   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.111")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.111")
CacheFillUpdatePattern: Successfully fusing ScatterOp into FillCacheOp
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.136")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.136")
CacheFillUpdatePattern: Successfully fusing ScatterOp into FillCacheOp
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
2025-08-13 23:25:09.726 (   7.804s) [        CA99B1C0]      module_builder.cc:588      1| TTNN Module:
module @SyncTensorsGraph.362 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.362 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073184896, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193216, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2 : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main(%arg0: tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.full"(%1) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x7>}> : (!ttnn.device) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %3 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = "ttnn.mesh_shard"(%arg5, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64128x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2004x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %9 = "ttnn.mesh_shard"(%arg6, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %10 = "ttnn.mesh_shard"(%arg8, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11 = "ttnn.mesh_shard"(%arg9, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %12 = "ttnn.mesh_shard"(%arg10, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %13 = "ttnn.mesh_shard"(%arg11, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %14 = "ttnn.mesh_shard"(%arg12, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = "ttnn.mesh_shard"(%arg13, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %16 = "ttnn.mesh_shard"(%arg14, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %17 = "ttnn.mesh_shard"(%arg15, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %18 = "ttnn.mesh_shard"(%arg16, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %19 = "ttnn.mesh_shard"(%arg17, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.mesh_shard"(%arg18, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %21 = "ttnn.mesh_shard"(%arg19, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.mesh_shard"(%arg20, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.mesh_shard"(%arg21, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg21) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.typecast"(%9) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %26 = "ttnn.typecast"(%7) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x7xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %27 = "ttnn.reshape"(%26) <{shape = [7 : i32]}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x7xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %28 = "ttnn.from_device"(%27) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %29 = "ttnn.to_layout"(%28) <{layout = #ttnn.layout<row_major>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> ()
        %30 = "ttnn.to_device"(%29, %1) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<system_memory>>>>) -> ()
        %31 = "ttnn.embedding"(%30, %8) : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<64128x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2004x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<7xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x7xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<64128x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2004x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %32 = "ttnn.typecast"(%31) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %33 = "ttnn.reshape"(%32) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %34 = "ttnn.pow"(%33, %0) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %35 = "ttnn.sum"(%34) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %36 = "ttnn.multiply"(%35, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %37 = "ttnn.reshape"(%36) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %38 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %39 = "ttnn.add"(%37, %38) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %40 = "ttnn.rsqrt"(%39) : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %41 = "ttnn.multiply"(%32, %40) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %42 = "ttnn.multiply"(%25, %41) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %43 = "ttnn.typecast"(%42) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %44 = "ttnn.matmul"(%43, %5) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %45 = "ttnn.reshape"(%44) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %46 = "ttnn.permute"(%45) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %47 = "ttnn.typecast"(%46) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %48 = "ttnn.typecast"(%3) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %49 = "ttnn.reshape"(%48) <{shape = [1 : i32, 1 : i32, 7 : i32]}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %50 = "ttnn.matmul"(%4, %49) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %51 = "ttnn.permute"(%50) <{permutation = array<i64: 0, 2, 1>}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x64x7xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %52 = "ttnn.concat"(%51, %51) <{dim = 2 : si32}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x7x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %53 = "ttnn.cos"(%52) : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %54 = "ttnn.reshape"(%53) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %55 = "ttnn.multiply"(%47, %54) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %56 = "ttnn.typecast"(%55) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %57 = "ttnn.slice"(%46) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %58 = "ttnn.neg"(%57) : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %59 = "ttnn.slice"(%46) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %60 = "ttnn.concat"(%58, %59) <{dim = 3 : si32}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x8x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %61 = "ttnn.typecast"(%60) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %62 = "ttnn.sin"(%52) : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %63 = "ttnn.reshape"(%62) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %64 = "ttnn.multiply"(%61, %63) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %65 = "ttnn.typecast"(%64) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x8x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %66 = "ttnn.add"(%56, %65) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%10, %66) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %67 = "ttnn.matmul"(%43, %11) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %68 = "ttnn.reshape"(%67) <{shape = [1 : i32, 7 : i32, 8 : i32, 128 : i32]}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<7x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %69 = "ttnn.permute"(%68) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1x7x8x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.fill_cache"(%12, %69) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x8x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %70 = "ttnn.typecast"(%23) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %71 = "ttnn.reshape"(%70) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %72 = "ttnn.matmul"(%43, %20) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %73 = "ttnn.reshape"(%72) <{shape = [1 : i32, 7 : i32, 24 : i32, 128 : i32]}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %74 = "ttnn.permute"(%73) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %75 = "ttnn.typecast"(%74) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %76 = "ttnn.reshape"(%75) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %77 = "ttnn.multiply"(%76, %53) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %78 = "ttnn.typecast"(%77) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %79 = "ttnn.slice"(%74) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %80 = "ttnn.neg"(%79) : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %81 = "ttnn.reshape"(%80) <{shape = [24 : i32, 7 : i32, 64 : i32]}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %82 = "ttnn.slice"(%74) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 7 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %83 = "ttnn.reshape"(%82) <{shape = [24 : i32, 7 : i32, 64 : i32]}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<1x24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %84 = "ttnn.concat"(%81, %83) <{dim = 2 : si32}> : (tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<24x7x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %85 = "ttnn.typecast"(%84) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %86 = "ttnn.multiply"(%85, %62) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %87 = "ttnn.typecast"(%86) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %88 = "ttnn.add"(%78, %87) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %89 = "ttnn.reshape"(%10) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %90 = "ttnn.repeat"(%89) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %91 = "ttnn.reshape"(%90) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %92 = "ttnn.permute"(%91) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %93 = "ttnn.reshape"(%92) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %94 = "ttnn.matmul"(%88, %93) <{transpose_a = false, transpose_b = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %95 = "ttnn.typecast"(%94) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %96 = "ttnn.reshape"(%95) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %97 = "ttnn.reshape"(%19) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %98 = "ttnn.multiply"(%96, %97) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %99 = "ttnn.typecast"(%98) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %100 = "ttnn.typecast"(%18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<7x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %101 = "ttnn.reshape"(%100) <{shape = [1 : i32, 1 : i32, 7 : i32, 128 : i32]}> : (tensor<7x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<7x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %102 = "ttnn.typecast"(%17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %103 = "ttnn.reshape"(%102) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %104 = "ttnn.typecast"(%3) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<7xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %105 = "ttnn.reshape"(%104) <{shape = [1 : i32, 1 : i32, 7 : i32, 1 : i32]}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<7xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %106 = "ttnn.typecast"(%103) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %107 = "ttnn.typecast"(%105) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x1x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %108 = "ttnn.gt"(%106, %107) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x1x7x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %109 = "ttnn.typecast"(%108) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %110 = "ttnn.multiply"(%101, %109) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %111 = "ttnn.typecast"(%110) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x1x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %112 = "ttnn.add"(%99, %111) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x1x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %113 = "ttnn.typecast"(%112) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %114 = "ttnn.max"(%113) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %115 = "ttnn.neg"(%114) : (tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %116 = "ttnn.add"(%113, %115) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x24x7x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %117 = "ttnn.softmax"(%116) <{dimension = 3 : si32}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %118 = "ttnn.typecast"(%117) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<1x24x7x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %119 = "ttnn.reshape"(%118) <{shape = [24 : i32, 7 : i32, 128 : i32]}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %120 = "ttnn.reshape"(%12) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %121 = "ttnn.repeat"(%120) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %122 = "ttnn.reshape"(%121) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %123 = "ttnn.matmul"(%119, %122) <{transpose_a = false, transpose_b = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %124 = "ttnn.reshape"(%123) <{shape = [1 : i32, 24 : i32, 7 : i32, 128 : i32]}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %125 = "ttnn.permute"(%124) <{permutation = array<i64: 0, 2, 1, 3>}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x24x7x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %126 = "ttnn.reshape"(%125) <{shape = [7 : i32, 3072 : i32]}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<1x7x24x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 224 + d1 * 32 + d2, d3), <1x1>, memref<7x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %127 = "ttnn.matmul"(%126, %16) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %128 = "ttnn.add"(%31, %127) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %129 = "ttnn.typecast"(%21) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %130 = "ttnn.reshape"(%129) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %131 = "ttnn.typecast"(%128) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %132 = "ttnn.reshape"(%131) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %133 = "ttnn.pow"(%132, %0) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %134 = "ttnn.sum"(%133) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %135 = "ttnn.multiply"(%134, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %136 = "ttnn.reshape"(%135) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %137 = "ttnn.add"(%136, %38) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %138 = "ttnn.rsqrt"(%137) : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %139 = "ttnn.multiply"(%131, %138) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %140 = "ttnn.multiply"(%130, %139) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %141 = "ttnn.typecast"(%140) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %142 = "ttnn.matmul"(%141, %22) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %143 = "ttnn.typecast"(%142) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %144 = "ttnn.sigmoid"(%142) : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %145 = "ttnn.typecast"(%144) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %146 = "ttnn.multiply"(%143, %145) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %147 = "ttnn.matmul"(%141, %15) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %148 = "ttnn.typecast"(%147) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %149 = "ttnn.multiply"(%146, %148) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %150 = "ttnn.typecast"(%149) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<7x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %151 = "ttnn.matmul"(%150, %14) <{transpose_a = false, transpose_b = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<7x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %152 = "ttnn.add"(%128, %151) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %153 = "ttnn.typecast"(%152) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %154 = "ttnn.reshape"(%153) <{shape = [1 : i32, 7 : i32, 3072 : i32]}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %155 = "ttnn.pow"(%154, %0) : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %156 = "ttnn.sum"(%155) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x7x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %157 = "ttnn.multiply"(%156, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %158 = "ttnn.reshape"(%157) <{shape = [7 : i32, 1 : i32]}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x7xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %159 = "ttnn.add"(%158, %38) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %160 = "ttnn.rsqrt"(%159) : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %161 = "ttnn.multiply"(%153, %160) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<7x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %162 = "ttnn.multiply"(%71, %161) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %163 = "ttnn.typecast"(%162) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<7x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %164 = "ttnn.matmul"(%163, %13) <{transpose_a = false, transpose_b = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<7x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %165 = "ttnn.reshape"(%164) <{shape = [1 : i32, 7 : i32, 128256 : i32]}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %166 = "ttnn.to_layout"(%10) <{layout = #ttnn.layout<row_major>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %167 = "ttnn.from_device"(%166) : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %168 = "ttnn.mesh_shard"(%167, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%167) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %169 = "ttnn.to_layout"(%12) <{layout = #ttnn.layout<row_major>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %170 = "ttnn.from_device"(%169) : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %171 = "ttnn.mesh_shard"(%170, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%170) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %172 = "ttnn.to_layout"(%164) <{layout = #ttnn.layout<row_major>}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%164) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %173 = "ttnn.from_device"(%172) : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %174 = "ttnn.mesh_shard"(%173, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%173) <{force = false}> : (tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %175 = "ttnn.to_layout"(%165) <{layout = #ttnn.layout<row_major>}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %176 = "ttnn.from_device"(%175) : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%175) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %177 = "ttnn.mesh_shard"(%176, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%176) <{force = false}> : (tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        return %168, %171, %174, %177 : tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<7x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x7x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 7 + d1, d2), <1x1>, memref<7x128256xbf16, #ttnn.buffer_type<system_memory>>>>
      }
    }
  }
}
2025-08-13 23:25:09.752 (   7.830s) [        CA99B1C0]loaded_executable_insta:98       1| [LIFECYCLE] LoadedExecutableInstance constructor - instance created: 0x5572dfa04780
2025-08-13 23:25:09.752 (   7.830s) [        CA99B1C0]loaded_executable_insta:516      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-08-13 23:25:09.752 (   7.830s) [        CA99B1C0]loaded_executable_insta:535      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-08-13 23:25:09.752 (   7.831s) [        CA99B1C0]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-08-13 23:25:09.752 (   7.831s) [        CA99B1C0]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-08-13 23:25:09.752 (   7.831s) [        CA99B1C0]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-08-13 23:25:09.752 (   7.831s) [        CA99B1C0]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-08-13 23:25:09.752 (   7.831s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:09.753 (   7.831s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:09.763 (   7.842s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:09.763 (   7.842s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:09.779 (   7.858s) [        8AFFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:09.779 (   7.858s) [        8AFFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        337FE640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        337FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        337FE640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.858s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.858s) [        337FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        337FE640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        337FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        337FE640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        337FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.780 (   7.859s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.780 (   7.859s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.781 (   7.859s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.781 (   7.859s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.781 (   7.859s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.781 (   7.859s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.781 (   7.859s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.781 (   7.859s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.781 (   7.859s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.781 (   7.859s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.781 (   7.859s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.781 (   7.859s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.781 (   7.859s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.781 (   7.859s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.781 (   7.859s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.781 (   7.859s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:09.781 (   7.859s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:09.781 (   7.859s) [        8AFFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-08-13 23:25:09.781 (   7.859s) [        8AFFD640]loaded_executable_insta:571      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-08-13 23:25:09.781 (   7.859s) [        8AFFD640]loaded_executable_insta:95       1| LoadedExecutableInstance::Execute
2025-08-13 23:25:09.781 (   7.859s) [        8AFFD640]loaded_executable_insta:114      1| [DEVICE] Runtime device not opened, opening devices...
2025-08-13 23:25:09.781 (   7.859s) [        8AFFD640]loaded_executable_insta:204      1| [DEVICE] Starting device opening process with 22 args on 2 devices
2025-08-13 23:25:09.781 (   7.859s) [        8AFFD640]loaded_executable_insta:207      1| [DEVICE] Found 2 unique device IDs from arguments
2025-08-13 23:25:09.781 (   7.859s) [        8AFFD640]loaded_executable_insta:246      1| [DEVICE] Opening mesh device with shape [1, 2]
2025-08-13 23:25:09.862 (   7.940s) [        8AFFD640]loaded_executable_insta:250      1| [DEVICE] Mesh device opened successfully
2025-08-13 23:25:09.862 (   7.940s) [        8AFFD640]loaded_executable_insta:121      1| [DEVICE] Successfully opened runtime device
2025-08-13 23:25:09.862 (   7.940s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cd4ca0 (arg 0)
2025-08-13 23:25:09.863 (   7.941s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 1)
2025-08-13 23:25:09.863 (   7.941s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 2)
2025-08-13 23:25:10.349 (   8.428s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 3)
2025-08-13 23:25:10.350 (   8.428s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 4)
2025-08-13 23:25:10.350 (   8.428s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 5)
2025-08-13 23:25:10.896 (   8.974s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 6)
2025-08-13 23:25:11.384 (   9.462s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 7)
2025-08-13 23:25:11.384 (   9.462s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 8)
2025-08-13 23:25:11.889 (   9.968s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 9)
2025-08-13 23:25:11.891 (   9.969s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 10)
2025-08-13 23:25:11.892 (   9.970s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 11)
2025-08-13 23:25:12.515 (  10.593s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 12)
2025-08-13 23:25:13.011 (  11.089s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 13)
2025-08-13 23:25:13.565 (  11.643s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 14)
2025-08-13 23:25:14.148 (  12.226s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 15)
2025-08-13 23:25:14.148 (  12.226s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 16)
2025-08-13 23:25:14.586 (  12.664s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 17)
2025-08-13 23:25:14.586 (  12.664s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 18)
2025-08-13 23:25:14.591 (  12.670s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 19)
2025-08-13 23:25:14.592 (  12.671s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 20)
2025-08-13 23:25:14.602 (  12.681s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb28d0 (arg 21)
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:466      1| BufferInstance::PJRT_Buffer_DynamicDimensionIndices
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-13 23:25:42.179 (  40.257s) [        8AFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-13 23:25:42.179 (  40.258s) [        8AFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-13 23:25:42.179 (  40.258s) [        8AFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-13 23:25:42.180 (  40.258s) [        8AFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-13 23:25:42.180 (  40.258s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:42.180 (  40.258s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:42.186 (  40.265s) [        CA99B1C0]     client_instance.cc:471      1| ClientInstance::PJRT_Client_Compile
2025-08-13 23:25:42.187 (  40.265s) [        CA99B1C0]      module_builder.cc:101      1| ModuleBuilder::buildModule
2025-08-13 23:25:42.187 (  40.265s) [        CA99B1C0]      module_builder.cc:155      1| VHLO Module:
module @SyncTensorsGraph.26 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<7.812500e-03> : tensor<bf16>>}> : () -> !vhlo.tensor_v1<!vhlo.bf16_v1>
    %2 = "vhlo.broadcast_in_dim_v1"(%1) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %3 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<8x128x128x!vhlo.bf16_v1>
    %4 = "vhlo.slice_v1"(%3) <{limit_indices = #vhlo.tensor_v1<dense<[1, 128, 128]> : tensor<3xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<3xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<3xi64>>}> : (!vhlo.tensor_v1<8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x128x!vhlo.bf16_v1>
    %5 = "vhlo.reshape_v1"(%4) : (!vhlo.tensor_v1<1x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x128x!vhlo.bf16_v1>
    %6 = "vhlo.reduce_v1"(%5, %0) <{dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>}> ({
    ^bb0(%arg1: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg2: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      %8 = "vhlo.add_v1"(%arg1, %arg2) : (!vhlo.tensor_v1<!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<!vhlo.bf16_v1>
      "vhlo.return_v1"(%8) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    %7 = "vhlo.multiply_v1"(%6, %2) : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>, !vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<128x!vhlo.bf16_v1>
    "vhlo.return_v1"(%7) : (!vhlo.tensor_v1<128x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-08-13 23:25:42.188 (  40.266s) [        CA99B1C0]      module_builder.cc:188      1| SHLO Module:
module @SyncTensorsGraph.26 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<1x8x128x128xbf16> {mhlo.sharding = "{replicated}"}) -> tensor<128xbf16> {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<7.812500e-03> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128xbf16>
    %1 = stablehlo.reshape %arg0 : (tensor<1x8x128x128xbf16>) -> tensor<8x128x128xbf16>
    %2 = stablehlo.slice %1 [0:1, 0:128, 0:128] : (tensor<8x128x128xbf16>) -> tensor<1x128x128xbf16>
    %3 = stablehlo.reshape %2 : (tensor<1x128x128xbf16>) -> tensor<128x128xbf16>
    %4 = stablehlo.reduce(%3 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x128xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %5 = stablehlo.multiply %4, %0 : tensor<128xbf16>
    return %5 : tensor<128xbf16>
  }
}
2025-08-13 23:25:42.191 (  40.269s) [        CA99B1C0]      module_builder.cc:205      1| SHLO StableHLO Pipeline Module:
module @SyncTensorsGraph.26 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["x"=1, "y"=1]>
  func.func @main(%arg0: tensor<1x8x128x128xbf16> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<bf16>
    %cst_0 = stablehlo.constant dense<7.812500e-03> : tensor<bf16>
    %0 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<bf16>) -> tensor<128xbf16>
    %1 = stablehlo.reshape %arg0 : (tensor<1x8x128x128xbf16>) -> tensor<8x128x128xbf16>
    %2 = stablehlo.slice %1 [0:1, 0:128, 0:128] : (tensor<8x128x128xbf16>) -> tensor<1x128x128xbf16>
    %3 = stablehlo.reshape %2 : (tensor<1x128x128xbf16>) -> tensor<128x128xbf16>
    %4 = stablehlo.reduce(%3 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x128xbf16>, tensor<bf16>) -> tensor<128xbf16>
    %5 = stablehlo.multiply %4, %0 : tensor<128xbf16>
    return %5 : tensor<128xbf16>
  }
}
2025-08-13 23:25:42.192 (  40.270s) [        CA99B1C0]      module_builder.cc:452      1| TTIR Module:
module @SyncTensorsGraph.26 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  func.func @main(%arg0: tensor<1x8x128x128xbf16> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<bf16>}> : () -> tensor<bf16>
    %1 = "ttir.constant"() <{value = dense<7.812500e-03> : tensor<bf16>}> : () -> tensor<bf16>
    %2 = ttir.empty() : tensor<1xbf16>
    %3 = "ttir.reshape"(%1, %2) <{shape = [1 : i32]}> : (tensor<bf16>, tensor<1xbf16>) -> tensor<1xbf16>
    %4 = ttir.empty() : tensor<128xbf16>
    %5 = "ttir.broadcast"(%3, %4) <{broadcast_dimensions = array<i64: 128>}> : (tensor<1xbf16>, tensor<128xbf16>) -> tensor<128xbf16>
    %6 = ttir.empty() : tensor<8x128x128xbf16>
    %7 = "ttir.reshape"(%arg0, %6) <{shape = [8 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<8x128x128xbf16>) -> tensor<8x128x128xbf16>
    %8 = ttir.empty() : tensor<1x128x128xbf16>
    %9 = "ttir.slice"(%7, %8) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 128 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<8x128x128xbf16>, tensor<1x128x128xbf16>) -> tensor<1x128x128xbf16>
    %10 = ttir.empty() : tensor<128x128xbf16>
    %11 = "ttir.reshape"(%9, %10) <{shape = [128 : i32, 128 : i32]}> : (tensor<1x128x128xbf16>, tensor<128x128xbf16>) -> tensor<128x128xbf16>
    %12 = ttir.empty() : tensor<128xbf16>
    %13 = "ttir.sum"(%11, %12) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<128x128xbf16>, tensor<128xbf16>) -> tensor<128xbf16>
    %14 = ttir.empty() : tensor<128xbf16>
    %15 = "ttir.multiply"(%13, %5, %14) : (tensor<128xbf16>, tensor<128xbf16>, tensor<128xbf16>) -> tensor<128xbf16>
    return %15 : tensor<128xbf16>
  }
}
2025-08-13 23:25:42.192 (  40.271s) [        CA99B1C0]      module_builder.cc:506   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-08-13 23:25:42.192 (  40.271s) [        CA99B1C0]      module_builder.cc:520   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-08-13 23:25:42.192 (  40.271s) [        CA99B1C0]      module_builder.cc:528   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
2025-08-13 23:25:42.210 (  40.289s) [        CA99B1C0]      module_builder.cc:588      1| TTNN Module:
module @SyncTensorsGraph.26 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.26 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x1>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073184896, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193216, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1, d2)[s0] -> (0, d0, d1, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x1, chipIds = [0]>
      func.func @main_const_eval_0() -> tensor<1xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<bf16>, fill_value = 7.812500e-03 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32]}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<bf16, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2 : tensor<1xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main(%arg0: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {mhlo.sharding = "{replicated}", ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<128xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !ttnn.device
        %2 = "ttnn.reshape"(%arg0) <{shape = [8 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %3 = "ttnn.slice"(%2) <{begins = [0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 128 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %4 = "ttnn.reshape"(%3) <{shape = [128 : i32, 128 : i32]}> : (tensor<1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = "ttnn.sum"(%4) <{dim_arg = [1 : i32], keep_dim = false}> : (tensor<128x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<128x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6 = "ttnn.multiply"(%5, %0) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<128xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<128xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %6 : tensor<128xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
    }
  }
}
2025-08-13 23:25:42.216 (  40.294s) [        CA99B1C0]loaded_executable_insta:98       1| [LIFECYCLE] LoadedExecutableInstance constructor - instance created: 0x5572f44f71e0
2025-08-13 23:25:42.216 (  40.294s) [        CA99B1C0]loaded_executable_insta:516      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-08-13 23:25:42.216 (  40.294s) [        CA99B1C0]loaded_executable_insta:535      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-08-13 23:25:42.216 (  40.295s) [        CA99B1C0]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-08-13 23:25:42.216 (  40.295s) [        CA99B1C0]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-08-13 23:25:42.216 (  40.295s) [        CA99B1C0]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-08-13 23:25:42.216 (  40.295s) [        CA99B1C0]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-08-13 23:25:42.216 (  40.295s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:42.216 (  40.295s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:42.220 (  40.298s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:42.220 (  40.298s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]loaded_executable_insta:571      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]loaded_executable_insta:95       1| LoadedExecutableInstance::Execute
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]loaded_executable_insta:101    ERR| Requested number of devices to run the executable on (2) doesn't match the compiler estimated number of devices (1)
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-08-13 23:25:42.222 (  40.300s) [        8AFFD640]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-08-13 23:25:42.224 (  40.302s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:42.224 (  40.302s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:42.224 (  40.302s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:42.224 (  40.302s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:42.224 (  40.302s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:42.224 (  40.302s) [        CA99B1C0]     buffer_instance.cc:454      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-08-13 23:25:42.224 (  40.302s) [        CA99B1C0]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-13 23:25:42.224 (  40.302s) [        CA99B1C0]     buffer_instance.cc:443      1| BufferInstance::PJRT_Buffer_Dimensions
2025-08-13 23:25:42.224 (  40.302s) [        CA99B1C0]     buffer_instance.cc:476      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-08-13 23:25:42.224 (  40.302s) [        CA99B1C0]     buffer_instance.cc:497      1| [BUFFER_TO_HOST] Source buffer shape: [1, 7, 128256], data_type: 13, required_size: 1795584 bytes
2025-08-13 23:25:42.224 (  40.302s) [        CA99B1C0]     buffer_instance.cc:508      1| [BUFFER_TO_HOST] Destination buffer: dst_size=1795584 bytes, dst_ptr=0x5572bd258840
2025-08-13 23:25:42.224 (  40.303s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:42.225 (  40.303s) [        727FC640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
input 0: device = xla:0, shape torch.Size([3072])
input 1: device = xla:0, shape torch.Size([3072])
input 2: device = xla:0, shape torch.Size([3072])
input 3: device = xla:0, shape torch.Size([128256, 3072])
input 4: device = xla:0, shape torch.Size([128])
input 5: device = xla:0, shape torch.Size([7, 128])
input 6: device = xla:0, shape torch.Size([1, 64, 1])
input 7: device = xla:0, shape torch.Size([3072, 3072])
input 8: device = xla:0, shape torch.Size([3072, 1024])
input 9: device = xla:0, shape torch.Size([3072, 1024])
input 10: device = xla:0, shape torch.Size([3072, 3072])
input 11: device = xla:0, shape torch.Size([3072, 8192])
input 12: device = xla:0, shape torch.Size([3072, 8192])
input 13: device = xla:0, shape torch.Size([8192, 3072])
input 14: device = xla:0, shape torch.Size([3072, 128256])
input 15: device = xla:0, shape torch.Size([3072, 3072])
input 16: device = xla:0, shape torch.Size([1024, 3072])
input 17: device = xla:0, shape torch.Size([1024, 3072])
input 18: device = xla:0, shape torch.Size([3072, 3072])
input 19: device = xla:0, shape torch.Size([8192, 3072])
input 20: device = xla:0, shape torch.Size([8192, 3072])
input 21: device = xla:0, shape torch.Size([3072, 8192])
input 22: device = xla:0, shape torch.Size([128256, 3072])
input 23: device = xla:0, shape torch.Size([1, 8, 128, 128])
	Warning: Could not compute mean for static cache 23: Bad StatusOr access: INTERNAL: Error code: 13
	mean along seqlen for static cache @ input idx 23 and shape torch.Size([1, 8, 128, 128]): <error during computation>
input 24: device = xla:0, shape torch.Size([1, 8, 128, 128])
	Warning: Could not compute mean for static cache 24: Bad StatusOr access: INTERNAL: Error code: 13
	mean along seqlen for static cache @ input idx 24 and shape torch.Size([1, 8, 128, 128]): <error during computation>
input 25: device = xla:0, shape torch.Size([64])
input 26: device = xla:0, shape torch.Size([1, 7])
input 27: device = xla:0, shape torch.Size([7])
attempting retrieval of shard spec for inputs tensor([[[-3.9688e+00, -1.4766e+00, -4.8242e-01,  ...,  2.6875e+00,
           2.6875e+00,  2.6875e+00],
         [-1.9844e+00,  6.0547e-01, -1.7578e-01,  ...,  2.6406e+00,
           2.6406e+00,  2.6406e+00],
         [ 2.1057e-03, -2.3125e+00, -3.4180e-01,  ...,  9.4531e-01,
           9.4531e-01,  9.3359e-01],
         ...,
         [-1.6172e+00, -2.3438e+00, -1.2812e+00,  ...,  4.0000e+00,
           4.0000e+00,  3.9688e+00],
         [ 5.4297e-01, -5.0391e-01, -2.2430e-03,  ..., -1.0859e+00,
          -1.0859e+00, -1.0859e+00],
         [-1.5039e-01, -3.6328e-01, -1.7188e-01,  ..., -4.7070e-01,
          -4.7070e-01, -4.7070e-01]]], device='xla:0', dtype=torch.bfloat16)
[SHARD_DEBUG] GET tensor: id=140711053240384, shape=[1, 7, 128256], dtype=torch.bfloat16 -> NOT FOUND
2025-08-13 23:25:42.228 (  40.306s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:42.228 (  40.306s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:42.228 (  40.306s) [        CA99B1C0]     buffer_instance.cc:454      1| BufferInstance::PJRT_Buffer_UnpaddedDimensions
2025-08-13 23:25:42.228 (  40.306s) [        CA99B1C0]     buffer_instance.cc:435      1| BufferInstance::PJRT_Buffer_ElementType
2025-08-13 23:25:42.228 (  40.306s) [        CA99B1C0]     buffer_instance.cc:476      1| BufferInstance::PJRT_Buffer_ToHostBuffer
2025-08-13 23:25:42.228 (  40.306s) [        CA99B1C0]     buffer_instance.cc:497      1| [BUFFER_TO_HOST] Source buffer shape: [1, 7, 128256], data_type: 13, required_size: 1795584 bytes
2025-08-13 23:25:42.228 (  40.306s) [        CA99B1C0]     buffer_instance.cc:508      1| [BUFFER_TO_HOST] Destination buffer: dst_size=1795584 bytes, dst_ptr=0x5572c4018680
2025-08-13 23:25:42.228 (  40.306s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:42.228 (  40.307s) [        81FFB640]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:42.229 (  40.307s) [        CA99B1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-13 23:25:42.229 (  40.307s) [        CA99B1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-13 23:25:42.229 (  40.308s) [        CA99B1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-13 23:25:42.229 (  40.308s) [        CA99B1C0]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
[Sharding] No shard_spec found for runtime tensor shape torch.Size([1, 7, 128256]), skipping sharding
 theNote: Using experimental XLA backend.
[James] disable rectify buffer inplace copy
[Cache] Reusing cached tensor for L__self___model_layers__modules__0___input_layernorm_weight
[Cache] Reusing cached tensor for L__self___model_layers__modules__0___post_attention_layernorm_weight
[Cache] Reusing cached tensor for L__self___model_norm_weight
[Cache] Reusing cached tensor for L__self___lm_head.weight
[SHARD_DEBUG] GET tensor: id=140708367073504, shape=[128], dtype=torch.int64 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.0
[SHARD_DEBUG] GET tensor: id=140708367075584, shape=[1, 128], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.1
[SHARD_DEBUG] GET tensor: id=140708367073904, shape=[1, 64, 1], dtype=torch.float32 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.2
[SHARD_DEBUG] GET tensor: id=140708367074544, shape=[3072, 3072], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.3
[SHARD_DEBUG] GET tensor: id=140708367076304, shape=[3072, 1024], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.4
[SHARD_DEBUG] GET tensor: id=140708367073344, shape=[3072, 1024], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.5
[SHARD_DEBUG] GET tensor: id=140708367076464, shape=[3072, 3072], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.6
[SHARD_DEBUG] GET tensor: id=140708367074144, shape=[3072, 8192], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.7
[SHARD_DEBUG] GET tensor: id=140708367075264, shape=[3072, 8192], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.8
[SHARD_DEBUG] GET tensor: id=140708367076704, shape=[8192, 3072], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.9
[SHARD_DEBUG] GET tensor: id=140708367078384, shape=[3072, 128256], dtype=torch.bfloat16 -> NOT FOUND
[Cache] Cached new tensor for _FX_CONST_FOLDED_ATTRS.10
[Cache] Reusing cached tensor for const_subgraph_module.L__self___model_layers__modules__0___self_attn_q_proj.weight
[Cache] Reusing cached tensor for const_subgraph_module.L__self___model_layers__modules__0___self_attn_k_proj.weight
[Cache] Reusing cached tensor for const_subgraph_module.L__self___model_layers__modules__0___self_attn_v_proj.weight
[Cache] Reusing cached tensor for const_subgraph_module.L__self___model_layers__modules__0___self_attn_o_proj.weight
[Cache] Reusing cached tensor for const_subgraph_module.L__self___model_layers__modules__0___mlp_gate_proj.weight
[Cache] Reusing cached tensor for const_subgraph_module.L__self___model_layers__modules__0___mlp_up_proj.weight
[Cache] Reusing cached tensor for const_subgraph_module.L__self___model_layers__modules__0___mlp_down_proj.weight
[Cache] Reusing cached tensor for const_subgraph_module.L__self___lm_head.weight
[Cache] Reusing cached tensor for kwargs____past_key_values___key_cache_0
[Cache] Reusing cached tensor for kwargs____past_key_values___value_cache_0
[Cache] Reusing cached tensor for const_subgraph_module.L__self___model_rotary_emb_inv_freq
attempting retrieval of shard spec for inputs tensor([[279]])
[SHARD_DEBUG] GET tensor: id=140711051974736, shape=[1, 1], dtype=torch.int64 -> NOT FOUND
[Sharding] No shard_spec found for runtime tensor shape torch.Size([1, 1]), skipping sharding
attempting retrieval of shard spec for inputs tensor([7])
[SHARD_DEBUG] GET tensor: id=140711053240384, shape=[1], dtype=torch.int64 -> NOT FOUND
[Sharding] No shard_spec found for runtime tensor shape torch.Size([1]), skipping sharding

[XLA Cache] === DETAILED CACHE ANALYSIS ===
[XLA Cache] Total inputs to analyze: 28
[XLA Cache] Current cache contains 28 unique tensor keys
[XLA Cache] Existing cache keys:
[XLA Cache]    1. [REGULAR] id=140708366995504, shape=(3072, 1024), dtype=torch.bfloat16, device=xla:0
[XLA Cache]    2. [REGULAR] id=140708366995584, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]    3. [REGULAR] id=140708366995664, shape=(3072, 1024), dtype=torch.bfloat16, device=xla:0
[XLA Cache]    4. [REGULAR] id=140708366995744, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]    5. [REGULAR] id=140708366995824, shape=(3072, 8192), dtype=torch.bfloat16, device=xla:0
[XLA Cache]    6. [REGULAR] id=140708366995904, shape=(3072, 8192), dtype=torch.bfloat16, device=xla:0
[XLA Cache]    7. [REGULAR] id=140708366995984, shape=(8192, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]    8. [REGULAR] id=140708366996064, shape=(3072, 128256), dtype=torch.bfloat16, device=xla:0
[XLA Cache]    9. [REGULAR] id=140708366996144, shape=(1024, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   10. [REGULAR] id=140708366996224, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   11. [REGULAR] id=140708366996304, shape=(1024, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   12. [REGULAR] id=140708366996384, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   13. [REGULAR] id=140708366996464, shape=(8192, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   14. [REGULAR] id=140708366996544, shape=(8192, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   15. [REGULAR] id=140708366996624, shape=(3072, 8192), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   16. [REGULAR] id=140708366996704, shape=(128256, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   17. [STATIC CACHE] id=140708366996864, shape=(1, 8, 128, 128), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   18. [STATIC CACHE] id=140708366997104, shape=(1, 8, 128, 128), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   19. [REGULAR] id=140708366997184, shape=(64,), dtype=torch.float32, device=xla:0
[XLA Cache]   20. [REGULAR] id=140709170807248, shape=(7, 128), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   21. [REGULAR] id=140709176378128, shape=(128,), dtype=torch.int64, device=xla:0
[XLA Cache]   22. [REGULAR] id=140711048747168, shape=(3072,), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   23. [REGULAR] id=140711051624208, shape=(1, 64, 1), dtype=torch.float32, device=xla:0
[XLA Cache]   24. [REGULAR] id=140711051624368, shape=(128256, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   25. [REGULAR] id=140711052554416, shape=(3072,), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   26. [REGULAR] id=140711052837008, shape=(7,), dtype=torch.int64, device=xla:0
[XLA Cache]   27. [REGULAR] id=140711054734448, shape=(3072,), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   28. [REGULAR] id=140716024407088, shape=(1, 7), dtype=torch.int64, device=xla:0
[XLA Cache] Input 0: input_0_shape_[3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140711054734448, shape=(3072,), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140711054734448
[XLA Cache]   XLA tensor id: 1
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 1: input_1_shape_[3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140711048747168, shape=(3072,), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140711048747168
[XLA Cache]   XLA tensor id: 2
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 2: input_2_shape_[3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140711052554416, shape=(3072,), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140711052554416
[XLA Cache]   XLA tensor id: 3
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 3: input_3_shape_[128256, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140711051624368, shape=(128256, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140711051624368
[XLA Cache]   XLA tensor id: 4
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 4: input_4_shape_[128]_dtype_torch.int64
[XLA Cache]   Cache key: id=140708237238944, shape=(128,), dtype=torch.int64, device=xla:0
[XLA Cache]   Tensor object id: 140708237238944
[XLA Cache]   XLA tensor id: 283
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 5: input_5_shape_[1, 128]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708237238704, shape=(1, 128), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708237238704
[XLA Cache]   XLA tensor id: 284
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 6: input_6_shape_[1, 64, 1]_dtype_torch.float32
[XLA Cache]   Cache key: id=140708237238384, shape=(1, 64, 1), dtype=torch.float32, device=xla:0
[XLA Cache]   Tensor object id: 140708237238384
[XLA Cache]   XLA tensor id: 285
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 7: input_7_shape_[3072, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708237238304, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708237238304
[XLA Cache]   XLA tensor id: 286
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 8: input_8_shape_[3072, 1024]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708237237824, shape=(3072, 1024), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708237237824
[XLA Cache]   XLA tensor id: 287
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 9: input_9_shape_[3072, 1024]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708237238544, shape=(3072, 1024), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708237238544
[XLA Cache]   XLA tensor id: 288
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 10: input_10_shape_[3072, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708237237984, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708237237984
[XLA Cache]   XLA tensor id: 289
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 11: input_11_shape_[3072, 8192]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708237238144, shape=(3072, 8192), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708237238144
[XLA Cache]   XLA tensor id: 290
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 12: input_12_shape_[3072, 8192]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708237237904, shape=(3072, 8192), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708237237904
[XLA Cache]   XLA tensor id: 291
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 13: input_13_shape_[8192, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708237238224, shape=(8192, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708237238224
[XLA Cache]   XLA tensor id: 292
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 14: input_14_shape_[3072, 128256]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708237238064, shape=(3072, 128256), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708237238064
[XLA Cache]   XLA tensor id: 293
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 15: input_15_shape_[3072, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996224, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996224
[XLA Cache]   XLA tensor id: 16
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 16: input_16_shape_[1024, 3072]_dtype_torch.bfloat16
2025-08-13 23:25:43.695 (  41.773s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.695 (  41.773s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.695 (  41.773s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.695 (  41.773s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.695 (  41.773s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.695 (  41.773s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:43.695 (  41.773s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.695 (  41.773s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.695 (  41.774s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.695 (  41.774s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.695 (  41.774s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.695 (  41.774s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1, 1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:43.695 (  41.774s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.695 (  41.774s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [1] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [128] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.696 (  41.774s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.696 (  41.775s) [        CA99B1C0]     buffer_instance.cc:213      1| [BUFFER] Creating OWNED host tensor with shape [128] (semantics: other, supported_dtype: false)
[32m                 Always[0m | [1m[38;5;208m WARNING[0m | User provided a tensor of data type: Int64 which is not supported by runtime/ttnn. Casting to: Int32, this may impact throughput and the integrity of the data.
2025-08-13 23:25:43.696 (  41.775s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.696 (  41.775s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.696 (  41.775s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.696 (  41.775s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.696 (  41.775s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.696 (  41.775s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.696 (  41.775s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 128] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 128] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.697 (  41.775s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.697 (  41.776s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 64, 1] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.697 (  41.776s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.697 (  41.776s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.697 (  41.776s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.697 (  41.776s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.697 (  41.776s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.697 (  41.776s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [1, 64, 1] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.697 (  41.776s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.697 (  41.776s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.699 (  41.777s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.699 (  41.777s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.727 (  41.806s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.727 (  41.806s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.727 (  41.806s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.728 (  41.806s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.728 (  41.806s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.728 (  41.806s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.728 (  41.806s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.728 (  41.806s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.728 (  41.806s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.728 (  41.806s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.728 (  41.806s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.728 (  41.806s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.728 (  41.807s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.728 (  41.807s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.736 (  41.814s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.736 (  41.814s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.736 (  41.814s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.736 (  41.814s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.736 (  41.814s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.736 (  41.815s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.736 (  41.815s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.736 (  41.815s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.736 (  41.815s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.736 (  41.815s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.736 (  41.815s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.736 (  41.815s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.737 (  41.815s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.737 (  41.815s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.744 (  41.822s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.744 (  41.822s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.744 (  41.822s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.744 (  41.822s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.744 (  41.823s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.744 (  41.823s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.744 (  41.823s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.744 (  41.823s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.744 (  41.823s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.744 (  41.823s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 1024] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.744 (  41.823s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.744 (  41.823s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.747 (  41.825s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.747 (  41.825s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.775 (  41.854s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.776 (  41.854s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.776 (  41.854s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.776 (  41.854s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.776 (  41.854s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.776 (  41.854s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.776 (  41.854s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.776 (  41.854s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.776 (  41.854s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.776 (  41.854s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.776 (  41.854s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.776 (  41.854s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.777 (  41.856s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.777 (  41.856s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.865 (  41.943s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.870 (  41.948s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.870 (  41.948s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.957 (  42.036s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.957 (  42.036s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.957 (  42.036s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.958 (  42.036s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.958 (  42.036s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.958 (  42.036s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.958 (  42.036s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:43.958 (  42.036s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:43.958 (  42.036s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:43.958 (  42.036s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 8192] (semantics: ZeroCopy/other)
2025-08-13 23:25:43.958 (  42.036s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:43.958 (  42.036s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:43.961 (  42.040s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:43.961 (  42.040s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:44.043 (  42.122s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:44.044 (  42.122s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:44.044 (  42.122s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:44.044 (  42.122s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:44.044 (  42.122s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:44.044 (  42.122s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:44.044 (  42.122s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:44.044 (  42.122s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:44.044 (  42.122s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:44.044 (  42.122s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [8192, 3072] (semantics: ZeroCopy/other)
2025-08-13 23:25:44.044 (  42.122s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:44.044 (  42.122s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:44.049 (  42.127s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:44.049 (  42.127s) [        CA99B1C0]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:45.516 (  43.595s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:45.516 (  43.595s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:45.516 (  43.595s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:45.517 (  43.595s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 128256] (semantics: ZeroCopy/other)
2025-08-13 23:25:45.517 (  43.595s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:45.517 (  43.595s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:45.517 (  43.595s) [        CA99B1C0]     device_instance.cc:82       1| DeviceInstance::PJRT_Device_DefaultMemory
2025-08-13 23:25:45.517 (  43.595s) [        CA99B1C0]     client_instance.cc:509      1| ClientInstance::PJRT_Client_BufferFromHostBuffer
2025-08-13 23:25:45.517 (  43.595s) [        CA99B1C0]     memory_instance.cc:57       1| MemoryInstance::getDevice
2025-08-13 23:25:45.517 (  43.595s) [        CA99B1C0]     buffer_instance.cc:228      1| [BUFFER] Creating BORROWED host tensor with shape [3072, 128256] (semantics: ZeroCopy/other)
2025-08-13 23:25:45.517 (  43.595s) [        CA99B1C0]      event_instance.cc:222      1| EventInstance::PJRT_Event_OnReady
2025-08-13 23:25:45.517 (  43.595s) [        CA99B1C0]      event_instance.cc:171      1| EventInstance::PJRT_Event_Destroy
2025-08-13 23:25:45.561 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.561 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.561 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.561 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.561 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.561 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.561 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.561 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.561 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.561 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.640s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.641s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.641s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.562 (  43.641s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.563 (  43.641s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.563 (  43.641s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.563 (  43.641s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
[XLA Cache]   Cache key: id=140708366996144, shape=(1024, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996144
[XLA Cache]   XLA tensor id: 17
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 17: input_17_shape_[1024, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996304, shape=(1024, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996304
[XLA Cache]   XLA tensor id: 18
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 18: input_18_shape_[3072, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996384, shape=(3072, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996384
[XLA Cache]   XLA tensor id: 19
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 19: input_19_shape_[8192, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996464, shape=(8192, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996464
[XLA Cache]   XLA tensor id: 20
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 20: input_20_shape_[8192, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996544, shape=(8192, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996544
[XLA Cache]   XLA tensor id: 21
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 21: input_21_shape_[3072, 8192]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996624, shape=(3072, 8192), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996624
[XLA Cache]   XLA tensor id: 22
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 22: input_22_shape_[128256, 3072]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996704, shape=(128256, 3072), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996704
[XLA Cache]   XLA tensor id: 23
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 23: input_23_shape_[1, 8, 128, 128]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366996864, shape=(1, 8, 128, 128), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366996864
[XLA Cache]   XLA tensor id: 24
[XLA Cache]   *** STATIC CACHE TENSOR DETECTED ***
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 24: input_24_shape_[1, 8, 128, 128]_dtype_torch.bfloat16
[XLA Cache]   Cache key: id=140708366997104, shape=(1, 8, 128, 128), dtype=torch.bfloat16, device=xla:0
[XLA Cache]   Tensor object id: 140708366997104
[XLA Cache]   XLA tensor id: 25
[XLA Cache]   *** STATIC CACHE TENSOR DETECTED ***
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 25: input_25_shape_[64]_dtype_torch.float32
[XLA Cache]   Cache key: id=140708366997184, shape=(64,), dtype=torch.float32, device=xla:0
[XLA Cache]   Tensor object id: 140708366997184
[XLA Cache]   XLA tensor id: 26
[XLA Cache]   Result: CACHE HIT ✓
[XLA Cache]   ---
[XLA Cache] Input 26: input_26_shape_[1, 1]_dtype_torch.int64
[XLA Cache]   Cache key: id=140715232138384, shape=(1, 1), dtype=torch.int64, device=xla:0
[XLA Cache]   Tensor object id: 140715232138384
[XLA Cache]   XLA tensor id: 294
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] Input 27: input_27_shape_[1]_dtype_torch.int64
[XLA Cache]   Cache key: id=140708366810416, shape=(1,), dtype=torch.int64, device=xla:0
[XLA Cache]   Tensor object id: 140708366810416
[XLA Cache]   XLA tensor id: 295
[XLA Cache]   Result: CACHE MISS - added to cache
[XLA Cache]   ---
[XLA Cache] CACHE HITS (15): ['input_0_shape_[3072]_dtype_torch.bfloat16 (idx=0)', 'input_1_shape_[3072]_dtype_torch.bfloat16 (idx=1)', 'input_2_shape_[3072]_dtype_torch.bfloat16 (idx=2)', 'input_3_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=3)', 'input_15_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=15)', 'input_16_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=16)', 'input_17_shape_[1024, 3072]_dtype_torch.bfloat16 (idx=17)', 'input_18_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=18)', 'input_19_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=19)', 'input_20_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=20)', 'input_21_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=21)', 'input_22_shape_[128256, 3072]_dtype_torch.bfloat16 (idx=22)', 'input_23_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=23)', 'input_24_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 (idx=24)', 'input_25_shape_[64]_dtype_torch.float32 (idx=25)']
[XLA Cache] CACHE MISSES (13): ['input_4_shape_[128]_dtype_torch.int64 (idx=4)', 'input_5_shape_[1, 128]_dtype_torch.bfloat16 (idx=5)', 'input_6_shape_[1, 64, 1]_dtype_torch.float32 (idx=6)', 'input_7_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=7)', 'input_8_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=8)', 'input_9_shape_[3072, 1024]_dtype_torch.bfloat16 (idx=9)', 'input_10_shape_[3072, 3072]_dtype_torch.bfloat16 (idx=10)', 'input_11_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=11)', 'input_12_shape_[3072, 8192]_dtype_torch.bfloat16 (idx=12)', 'input_13_shape_[8192, 3072]_dtype_torch.bfloat16 (idx=13)', 'input_14_shape_[3072, 128256]_dtype_torch.bfloat16 (idx=14)', 'input_26_shape_[1, 1]_dtype_torch.int64 (idx=26)', 'input_27_shape_[1]_dtype_torch.int64 (idx=27)']
[XLA Cache] STATIC CACHE TENSORS FOUND (2):
[XLA Cache]   input_23_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 -> HIT (tensor_id=140708366996864, xla_id=24)
[XLA Cache]   input_24_shape_[1, 8, 128, 128]_dtype_torch.bfloat16 -> HIT (tensor_id=140708366997104, xla_id=25)
[XLA Cache] Updated cache size: 41 unique tensors
[XLA Cache] === END CACHE ANALYSIS ===

[JAMES] StableHLO IR:
 module @IrToHlo.356 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  func.func @main(%arg0: tensor<3072x128256xbf16> {mhlo.sharding = "{replicated}"}, %arg1: tensor<f32> {mhlo.sharding = "{replicated}"}, %arg2: tensor<8192x3072xbf16> {mhlo.sharding = "{replicated}"}, %arg3: tensor<3072x8192xbf16> {mhlo.sharding = "{replicated}"}, %arg4: tensor<3072x3072xbf16> {mhlo.sharding = "{replicated}"}, %arg5: tensor<3072x1024xbf16> {mhlo.sharding = "{replicated}"}, %arg6: tensor<1x1xi64> {mhlo.sharding = "{replicated}"}, %arg7: tensor<128256x3072xbf16> {mhlo.sharding = "{devices=[2,1]<=[2]}"}, %arg8: tensor<3072xbf16> {mhlo.sharding = "{replicated}"}, %arg9: tensor<1xi64> {mhlo.sharding = "{replicated}"}, %arg10: tensor<i64> {mhlo.sharding = "{replicated}"}, %arg11: tensor<1x8x128x128xbf16> {mhlo.sharding = "{replicated}"}, %arg12: tensor<128xi64> {mhlo.sharding = "{replicated}"}, %arg13: tensor<1x128xbf16> {mhlo.sharding = "{replicated}"}, %arg14: tensor<f32> {mhlo.sharding = "{replicated}"}, %arg15: tensor<1x64x1xf32> {mhlo.sharding = "{replicated}"}, %arg16: tensor<3072x1024xbf16> {mhlo.sharding = "{replicated}"}, %arg17: tensor<1x8x128x128xbf16> {mhlo.sharding = "{replicated}"}, %arg18: tensor<3072x3072xbf16> {mhlo.sharding = "{replicated}"}, %arg19: tensor<3072xbf16> {mhlo.sharding = "{replicated}"}, %arg20: tensor<3072x8192xbf16> {mhlo.sharding = "{replicated}"}, %arg21: tensor<3072xbf16> {mhlo.sharding = "{replicated}"}) -> tensor<1x1x128256xbf16> {
    %c = stablehlo.constant dense<0> : tensor<1xi64>
    %cst = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<1x1x3072xf32>
    %cst_1 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_2 = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %0 = stablehlo.convert %arg21 {mhlo.sharding = "{replicated}"} : (tensor<3072xbf16>) -> tensor<3072xf32>
    %1 = stablehlo.reshape %0 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %2 = stablehlo.reshape %arg6 : (tensor<1x1xi64>) -> tensor<1xi64>
    %3 = stablehlo.convert %2 : (tensor<1xi64>) -> tensor<1xui32>
    %4 = "stablehlo.gather"(%arg7, %3) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, indices_are_sorted = false, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %5 = stablehlo.reshape %4 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %6 = stablehlo.convert %arg8 {mhlo.sharding = "{replicated}"} : (tensor<3072xbf16>) -> tensor<3072xf32>
    %7 = stablehlo.reshape %6 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %8 = stablehlo.convert %5 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %9 = stablehlo.power %8, %cst_0 : tensor<1x1x3072xf32>
    %10 = stablehlo.reduce(%9 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %11 = stablehlo.multiply %10, %cst : tensor<1x1xf32>
    %12 = stablehlo.reshape %11 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %13 = stablehlo.reshape %arg1 : (tensor<f32>) -> tensor<1x1x1xf32>
    %14 = stablehlo.add %12, %13 : tensor<1x1x1xf32>
    %15 = stablehlo.rsqrt %14 : tensor<1x1x1xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %17 = stablehlo.broadcast_in_dim %16, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %18 = stablehlo.multiply %8, %17 : tensor<1x1x3072xf32>
    %19 = stablehlo.convert %18 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %20 = stablehlo.convert %19 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %21 = stablehlo.multiply %7, %20 : tensor<1x1x3072xf32>
    %22 = stablehlo.convert %21 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %23 = stablehlo.reshape %22 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %24 = stablehlo.dot_general %23, %arg18, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %25 = stablehlo.reshape %24 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %26 = stablehlo.convert %25 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %27 = stablehlo.reshape %arg9 : (tensor<1xi64>) -> tensor<1x1x1xi64>
    %28 = stablehlo.convert %27 : (tensor<1x1x1xi64>) -> tensor<1x1x1xf32>
    %29 = stablehlo.dot_general %arg15, %28, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %30 = stablehlo.reshape %29 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %31 = stablehlo.concatenate %30, %30, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %32 = stablehlo.cosine %31 : tensor<1x1x128xf32>
    %33 = stablehlo.convert %32 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %34 = stablehlo.reshape %33 : (tensor<1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %35 = stablehlo.convert %34 : (tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xf32>
    %36 = stablehlo.reshape %35 : (tensor<1x1x1x128xf32>) -> tensor<1x1x128xf32>
    %37 = stablehlo.broadcast_in_dim %36, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %38 = stablehlo.multiply %26, %37 : tensor<1x24x1x128xf32>
    %39 = stablehlo.convert %38 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %40 = stablehlo.slice %25 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %41 = stablehlo.negate %40 : tensor<1x24x1x64xbf16>
    %42 = stablehlo.slice %25 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %43 = stablehlo.concatenate %41, %42, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %44 = stablehlo.convert %43 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %45 = stablehlo.sine %31 : tensor<1x1x128xf32>
    %46 = stablehlo.convert %45 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %47 = stablehlo.reshape %46 : (tensor<1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %48 = stablehlo.convert %47 : (tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xf32>
    %49 = stablehlo.reshape %48 : (tensor<1x1x1x128xf32>) -> tensor<1x1x128xf32>
    %50 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %51 = stablehlo.multiply %44, %50 : tensor<1x24x1x128xf32>
    %52 = stablehlo.convert %51 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %53 = stablehlo.add %39, %52 : tensor<1x24x1x128xbf16>
    %54 = stablehlo.reshape %53 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %55 = stablehlo.compare  LT, %arg9, %c : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %56 = stablehlo.reshape %arg10 : (tensor<i64>) -> tensor<1xi64>
    %57 = stablehlo.add %arg9, %56 : tensor<1xi64>
    %58 = stablehlo.select %55, %57, %arg9 : tensor<1xi1>, tensor<1xi64>
    %59 = stablehlo.reshape %58 : (tensor<1xi64>) -> tensor<1x1xi64>
    %60 = stablehlo.dot_general %23, %arg16, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %61 = stablehlo.reshape %60 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %62 = stablehlo.convert %61 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %63 = stablehlo.broadcast_in_dim %36, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %64 = stablehlo.multiply %62, %63 : tensor<1x8x1x128xf32>
    %65 = stablehlo.convert %64 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %66 = stablehlo.slice %61 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %67 = stablehlo.negate %66 : tensor<1x8x1x64xbf16>
    %68 = stablehlo.slice %61 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %69 = stablehlo.concatenate %67, %68, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %70 = stablehlo.convert %69 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %71 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %72 = stablehlo.multiply %70, %71 : tensor<1x8x1x128xf32>
    %73 = stablehlo.convert %72 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %74 = stablehlo.add %65, %73 : tensor<1x8x1x128xbf16>
    %75 = "stablehlo.scatter"(%arg17, %59, %74) <{indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>, unique_indices = false}> ({
    ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
      stablehlo.return %arg23 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %76 = stablehlo.broadcast_in_dim %75, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %77 = stablehlo.reshape %76 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %78 = stablehlo.transpose %77, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %80 = stablehlo.dot_general %54, %79, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %81 = stablehlo.reshape %80 : (tensor<24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %82 = stablehlo.convert %81 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %83 = stablehlo.broadcast_in_dim %arg14, dims = [] : (tensor<f32>) -> tensor<1x24x1x128xf32>
    %84 = stablehlo.multiply %82, %83 : tensor<1x24x1x128xf32>
    %85 = stablehlo.convert %84 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %86 = stablehlo.convert %arg13 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %87 = stablehlo.reshape %arg12 : (tensor<128xi64>) -> tensor<1x128xi64>
    %88 = stablehlo.broadcast_in_dim %arg9, dims = [0] : (tensor<1xi64>) -> tensor<1x128xi64>
    %89 = stablehlo.compare  GT, %87, %88 : (tensor<1x128xi64>, tensor<1x128xi64>) -> tensor<1x128xi1>
    %90 = stablehlo.convert %89 : (tensor<1x128xi1>) -> tensor<1x128xf32>
    %91 = stablehlo.multiply %86, %90 : tensor<1x128xf32>
    %92 = stablehlo.convert %91 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %93 = stablehlo.reshape %92 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16>
    %94 = stablehlo.broadcast_in_dim %93, dims = [0, 2, 3] : (tensor<1x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %95 = stablehlo.add %85, %94 : tensor<1x24x1x128xbf16>
    %96 = stablehlo.convert %95 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %97 = stablehlo.reduce(%96 init: %cst_1) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %98 = stablehlo.broadcast_in_dim %97, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %99 = stablehlo.subtract %96, %98 : tensor<1x24x1x128xf32>
    %100 = stablehlo.exponential %99 : tensor<1x24x1x128xf32>
    %101 = stablehlo.reduce(%100 init: %cst_2) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %102 = stablehlo.broadcast_in_dim %101, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %103 = stablehlo.divide %100, %102 : tensor<1x24x1x128xf32>
    %104 = stablehlo.convert %103 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %105 = stablehlo.reshape %104 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %106 = stablehlo.dot_general %23, %arg5, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %108 = "stablehlo.scatter"(%arg11, %59, %107) <{indices_are_sorted = false, scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>, unique_indices = false}> ({
    ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
      stablehlo.return %arg23 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %109 = stablehlo.broadcast_in_dim %108, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %110 = stablehlo.reshape %109 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %111 = stablehlo.dot_general %105, %110, batching_dims = [0] x [0], contracting_dims = [2] x [1], precision = [DEFAULT, DEFAULT] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %112 = stablehlo.reshape %111 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %113 = stablehlo.dot_general %112, %arg4, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %114 = stablehlo.reshape %113 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %115 = stablehlo.add %5, %114 : tensor<1x1x3072xbf16>
    %116 = stablehlo.convert %arg19 {mhlo.sharding = "{replicated}"} : (tensor<3072xbf16>) -> tensor<3072xf32>
    %117 = stablehlo.reshape %116 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %118 = stablehlo.convert %115 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %119 = stablehlo.power %118, %cst_0 : tensor<1x1x3072xf32>
    %120 = stablehlo.reduce(%119 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %121 = stablehlo.multiply %120, %cst : tensor<1x1xf32>
    %122 = stablehlo.reshape %121 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %123 = stablehlo.add %122, %13 : tensor<1x1x1xf32>
    %124 = stablehlo.rsqrt %123 : tensor<1x1x1xf32>
    %125 = stablehlo.reshape %124 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %126 = stablehlo.broadcast_in_dim %125, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %127 = stablehlo.multiply %118, %126 : tensor<1x1x3072xf32>
    %128 = stablehlo.convert %127 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %129 = stablehlo.convert %128 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %130 = stablehlo.multiply %117, %129 : tensor<1x1x3072xf32>
    %131 = stablehlo.convert %130 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %132 = stablehlo.reshape %131 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %133 = stablehlo.dot_general %132, %arg20, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %134 = stablehlo.reshape %133 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %135 = stablehlo.convert %134 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %136 = stablehlo.logistic %134 : tensor<1x1x8192xbf16>
    %137 = stablehlo.convert %136 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %138 = stablehlo.multiply %135, %137 : tensor<1x1x8192xf32>
    %139 = stablehlo.convert %138 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %140 = stablehlo.convert %139 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %141 = stablehlo.dot_general %132, %arg3, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %142 = stablehlo.reshape %141 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %143 = stablehlo.convert %142 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %144 = stablehlo.multiply %140, %143 : tensor<1x1x8192xf32>
    %145 = stablehlo.convert %144 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %146 = stablehlo.reshape %145 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %147 = stablehlo.dot_general %146, %arg2, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %148 = stablehlo.reshape %147 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %149 = stablehlo.add %115, %148 : tensor<1x1x3072xbf16>
    %150 = stablehlo.convert %149 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %151 = stablehlo.power %150, %cst_0 : tensor<1x1x3072xf32>
    %152 = stablehlo.reduce(%151 init: %cst_2) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %153 = stablehlo.multiply %152, %cst : tensor<1x1xf32>
    %154 = stablehlo.reshape %153 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %155 = stablehlo.add %154, %13 : tensor<1x1x1xf32>
    %156 = stablehlo.rsqrt %155 : tensor<1x1x1xf32>
    %157 = stablehlo.reshape %156 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %158 = stablehlo.broadcast_in_dim %157, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %159 = stablehlo.multiply %150, %158 : tensor<1x1x3072xf32>
    %160 = stablehlo.convert %159 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %161 = stablehlo.convert %160 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %162 = stablehlo.multiply %1, %161 : tensor<1x1x3072xf32>
    %163 = stablehlo.convert %162 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %164 = stablehlo.reshape %163 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %165 = stablehlo.dot_general %164, %arg0, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %166 = stablehlo.reshape %165 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %166 : tensor<1x1x128256xbf16>
  }
}

2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.655s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
Hlo input positions pre normalization [293, 485, 292, 291, 289, 288, 294, 242, 1, 295, -1, 25, 283, 284, 418, 285, 287, 24, 286, 2, 290, 3]
Hlo input positions post normalization [294, 486, 293, 292, 290, 289, 295, 243, 2, 296, 0, 26, 284, 285, 419, 286, 288, 25, 287, 3, 291, 4]
[JAMES] setting arg ref map to  refs=constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,140715231682672,constant_unknown,140715232198416,user_input,constant_unknown,constant_unknown,constant_unknown,constant_unknown,constant_unknown,140716026202448,constant_unknown,140715232188816,constant_unknown,140708367073504
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.577 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.656s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.578 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.579 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.579 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.579 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.579 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.579 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.579 (  43.657s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.579 (  43.658s) [        CA99B1C0]     buffer_instance.cc:526      1| BufferInstance::PJRT_Buffer_IsDeleted
2025-08-13 23:25:45.603 (  43.681s) [        CA99B1C0]     client_instance.cc:471      1| ClientInstance::PJRT_Client_Compile
2025-08-13 23:25:45.603 (  43.681s) [        CA99B1C0]      module_builder.cc:101      1| ModuleBuilder::buildModule
2025-08-13 23:25:45.604 (  43.682s) [        CA99B1C0]      module_builder.cc:155      1| VHLO Module:
module @SyncTensorsGraph.358 attributes {mhlo.cross_program_prefetches = [], mhlo.frontend_attributes = {xla.sdy.meshes = "{mesh = #sdy.mesh<[\22_axis_0\22=2]>}"}, mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  vhlo.func_v1 @main(%arg0: !vhlo.tensor_v1<1x!vhlo.i64_v1>, %arg1: !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, %arg2: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg3: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg4: !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, %arg5: !vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, %arg6: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg7: !vhlo.tensor_v1<!vhlo.i64_v1>, %arg8: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg9: !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>, %arg10: !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, %arg11: !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>, %arg12: !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>, %arg13: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg14: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg15: !vhlo.tensor_v1<128x!vhlo.i64_v1>, %arg16: !vhlo.tensor_v1<1x128x!vhlo.bf16_v1>, %arg17: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg18: !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>, %arg19: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>, %arg20: !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>, %arg21: !vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) {
    %0 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : () -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %1 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %2 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<0xFF800000> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %3 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<3.25520843E-4> : tensor<1x1xf32>>}> : () -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %4 = "vhlo.constant_v1"() <{value = #vhlo.tensor_v1<dense<2.000000e+00> : tensor<f32>>}> : () -> !vhlo.tensor_v1<!vhlo.f32_v1>
    %5 = "vhlo.broadcast_in_dim_v1"(%4) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %6 = "vhlo.compare_v1"(%arg0, %0) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 LT>}> : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.bool_v1>
    %7 = "vhlo.reshape_v1"(%arg7) : (!vhlo.tensor_v1<!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %8 = "vhlo.add_v1"(%arg0, %7) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %9 = "vhlo.select_v1"(%6, %8, %arg0) : (!vhlo.tensor_v1<1x!vhlo.bool_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %10 = "vhlo.reshape_v1"(%9) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.i64_v1>
    %11 = "vhlo.convert_v1"(%arg6) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %12 = "vhlo.reshape_v1"(%11) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %13 = "vhlo.reshape_v1"(%arg4) : (!vhlo.tensor_v1<1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.i64_v1>
    %14 = "vhlo.convert_v1"(%13) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x!vhlo.ui32_v1>
    %15 = "vhlo.gather_v2"(%arg5, %14) <{collapsed_slice_dims = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, offset_dims = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, operand_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, slice_sizes = #vhlo.tensor_v1<dense<[1, 3072]> : tensor<2xi64>>, start_index_map = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, start_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<128256x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x!vhlo.ui32_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %16 = "vhlo.reshape_v1"(%15) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %17 = "vhlo.convert_v1"(%16) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %18 = "vhlo.power_v1"(%17, %5) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %19 = "vhlo.reduce_v1"(%18, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg23: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %175 = "vhlo.add_v1"(%arg22, %arg23) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%175) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %20 = "vhlo.multiply_v1"(%19, %3) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %21 = "vhlo.reshape_v1"(%20) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %22 = "vhlo.reshape_v1"(%arg3) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %23 = "vhlo.add_v1"(%21, %22) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %24 = "vhlo.rsqrt_v2"(%23) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %25 = "vhlo.reshape_v1"(%24) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %26 = "vhlo.broadcast_in_dim_v1"(%25) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %27 = "vhlo.multiply_v1"(%17, %26) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %28 = "vhlo.convert_v1"(%27) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %29 = "vhlo.convert_v1"(%28) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %30 = "vhlo.multiply_v1"(%12, %29) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %31 = "vhlo.convert_v1"(%30) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %32 = "vhlo.reshape_v1"(%31) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %33 = "vhlo.dot_general_v2"(%32, %arg2) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %34 = "vhlo.reshape_v1"(%33) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %35 = "vhlo.convert_v1"(%34) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,8,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %36 = "vhlo.reshape_v1"(%arg0) : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>
    %37 = "vhlo.convert_v1"(%36) : (!vhlo.tensor_v1<1x1x1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %38 = "vhlo.dot_general_v2"(%arg1, %37) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>
    %39 = "vhlo.reshape_v1"(%38) : (!vhlo.tensor_v1<1x64x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>
    %40 = "vhlo.concatenate_v1"(%39, %39) <{dimension = #vhlo.integer_v1<2 : i64>}> : (!vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x64x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %41 = "vhlo.cosine_v2"(%40) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %42 = "vhlo.convert_v1"(%41) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %43 = "vhlo.reshape_v1"(%42) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %44 = "vhlo.convert_v1"(%43) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %45 = "vhlo.reshape_v1"(%44) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %46 = "vhlo.broadcast_in_dim_v1"(%45) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %47 = "vhlo.multiply_v1"(%35, %46) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %48 = "vhlo.convert_v1"(%47) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %49 = "vhlo.slice_v1"(%34) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %50 = "vhlo.negate_v1"(%49) : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %51 = "vhlo.slice_v1"(%34) <{limit_indices = #vhlo.tensor_v1<dense<[1, 8, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>
    %52 = "vhlo.concatenate_v1"(%50, %51) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %53 = "vhlo.convert_v1"(%52) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %54 = "vhlo.sine_v2"(%40) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %55 = "vhlo.convert_v1"(%54) : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %56 = "vhlo.reshape_v1"(%55) : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>
    %57 = "vhlo.convert_v1"(%56) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>
    %58 = "vhlo.reshape_v1"(%57) : (!vhlo.tensor_v1<1x1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>
    %59 = "vhlo.broadcast_in_dim_v1"(%58) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %60 = "vhlo.multiply_v1"(%53, %59) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>
    %61 = "vhlo.convert_v1"(%60) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %62 = "vhlo.add_v1"(%48, %61) : (!vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %63 = "vhlo.scatter_v2"(%arg8, %10, %62) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg23) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %64 = "vhlo.custom_call_v1"(%63) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %65 = "vhlo.dot_general_v2"(%32, %arg9) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>
    %66 = "vhlo.reshape_v1"(%65) : (!vhlo.tensor_v1<1x1024x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>
    %67 = "vhlo.scatter_v2"(%arg10, %10, %66) <{index_vector_dim = #vhlo.integer_v1<1 : i64>, indices_are_sorted = #vhlo.bool_v1<false>, input_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, inserted_window_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_dims_to_operand_dims = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, scatter_indices_batching_dims = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, unique_indices = #vhlo.bool_v1<false>, update_window_dims = #vhlo.tensor_v1<dense<[0, 1, 3]> : tensor<3xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.bf16_v1>, %arg23: !vhlo.tensor_v1<!vhlo.bf16_v1>):
      "vhlo.return_v1"(%arg23) : (!vhlo.tensor_v1<!vhlo.bf16_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x!vhlo.i64_v1>, !vhlo.tensor_v1<1x8x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %68 = "vhlo.custom_call_v1"(%67) <{api_version = #vhlo<api_version_v1 API_VERSION_ORIGINAL>, backend_config = #vhlo.string_v1<"">, call_target_name = #vhlo.string_v1<"Sharding">, called_computations = #vhlo.array_v1<[]>, has_side_effect = #vhlo.bool_v1<false>, operand_layouts = #vhlo.array_v1<[]>, output_operand_aliases = #vhlo.array_v1<[]>, result_layouts = #vhlo.array_v1<[]>}> {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}, {}, {}, {}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>
    %69 = "vhlo.convert_v1"(%arg21) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %70 = "vhlo.reshape_v1"(%69) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %71 = "vhlo.dot_general_v2"(%32, %arg18) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %72 = "vhlo.reshape_v1"(%71) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %73 = "vhlo.convert_v1"(%72) {result_layout = #vhlo.tensor_v1<dense<[3, 1, 2, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"f32[1,24,1,128]{3,1,2,0}">} : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %74 = "vhlo.broadcast_in_dim_v1"(%45) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %75 = "vhlo.multiply_v1"(%73, %74) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %76 = "vhlo.convert_v1"(%75) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %77 = "vhlo.slice_v1"(%72) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 128]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<[0, 0, 0, 64]> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %78 = "vhlo.negate_v1"(%77) : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %79 = "vhlo.slice_v1"(%72) <{limit_indices = #vhlo.tensor_v1<dense<[1, 24, 1, 64]> : tensor<4xi64>>, start_indices = #vhlo.tensor_v1<dense<0> : tensor<4xi64>>, strides = #vhlo.tensor_v1<dense<1> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>
    %80 = "vhlo.concatenate_v1"(%78, %79) <{dimension = #vhlo.integer_v1<3 : i64>}> : (!vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x64x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %81 = "vhlo.convert_v1"(%80) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %82 = "vhlo.broadcast_in_dim_v1"(%58) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %83 = "vhlo.multiply_v1"(%81, %82) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %84 = "vhlo.convert_v1"(%83) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %85 = "vhlo.add_v1"(%76, %84) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %86 = "vhlo.reshape_v1"(%85) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %87 = "vhlo.broadcast_in_dim_v1"(%63) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %88 = "vhlo.reshape_v1"(%87) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %89 = "vhlo.transpose_v1"(%88) <{permutation = #vhlo.tensor_v1<dense<[0, 1, 3, 2]> : tensor<4xi64>>}> {result_layout = #vhlo.tensor_v1<dense<[2, 3, 1, 0]> : tensor<4xindex>>, xla_shape = #vhlo.string_v1<"bf16[1,24,128,128]{2,3,1,0}">} : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>
    %90 = "vhlo.reshape_v1"(%89) : (!vhlo.tensor_v1<1x24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %91 = "vhlo.dot_general_v2"(%86, %90) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %92 = "vhlo.reshape_v1"(%91) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %93 = "vhlo.convert_v1"(%92) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %94 = "vhlo.broadcast_in_dim_v1"(%arg17) <{broadcast_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>}> : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %95 = "vhlo.multiply_v1"(%93, %94) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %96 = "vhlo.convert_v1"(%95) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %97 = "vhlo.convert_v1"(%arg16) : (!vhlo.tensor_v1<1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.f32_v1>
    %98 = "vhlo.reshape_v1"(%arg15) : (!vhlo.tensor_v1<128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.i64_v1>
    %99 = "vhlo.broadcast_in_dim_v1"(%arg0) <{broadcast_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>}> : (!vhlo.tensor_v1<1x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.i64_v1>
    %100 = "vhlo.compare_v1"(%98, %99) <{compare_type = #vhlo<comparison_type_v1 NOTYPE>, comparison_direction = #vhlo<comparison_direction_v1 GT>}> : (!vhlo.tensor_v1<1x128x!vhlo.i64_v1>, !vhlo.tensor_v1<1x128x!vhlo.i64_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.bool_v1>
    %101 = "vhlo.convert_v1"(%100) : (!vhlo.tensor_v1<1x128x!vhlo.bool_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.f32_v1>
    %102 = "vhlo.multiply_v1"(%97, %101) : (!vhlo.tensor_v1<1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.f32_v1>
    %103 = "vhlo.convert_v1"(%102) : (!vhlo.tensor_v1<1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x128x!vhlo.bf16_v1>
    %104 = "vhlo.reshape_v1"(%103) : (!vhlo.tensor_v1<1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>
    %105 = "vhlo.broadcast_in_dim_v1"(%104) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 2, 3]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %106 = "vhlo.add_v1"(%96, %105) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %107 = "vhlo.convert_v1"(%106) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %108 = "vhlo.reduce_v1"(%107, %2) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg23: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %175 = "vhlo.maximum_v1"(%arg22, %arg23) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%175) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %109 = "vhlo.broadcast_in_dim_v1"(%108) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %110 = "vhlo.subtract_v1"(%107, %109) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %111 = "vhlo.exponential_v2"(%110) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %112 = "vhlo.reduce_v1"(%111, %1) <{dimensions = #vhlo.tensor_v1<dense<3> : tensor<1xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg23: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %175 = "vhlo.add_v1"(%arg22, %arg23) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%175) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>
    %113 = "vhlo.broadcast_in_dim_v1"(%112) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 2]> : tensor<3xi64>>}> : (!vhlo.tensor_v1<1x24x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %114 = "vhlo.divide_v1"(%111, %113) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>, !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>
    %115 = "vhlo.convert_v1"(%114) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>
    %116 = "vhlo.reshape_v1"(%115) : (!vhlo.tensor_v1<1x24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %117 = "vhlo.broadcast_in_dim_v1"(%67) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1, 3, 4]> : tensor<4xi64>>}> : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>
    %118 = "vhlo.reshape_v1"(%117) : (!vhlo.tensor_v1<1x8x3x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>
    %119 = "vhlo.dot_general_v2"(%116, %118) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<24x128x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>
    %120 = "vhlo.reshape_v1"(%119) : (!vhlo.tensor_v1<24x1x128x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %121 = "vhlo.dot_general_v2"(%120, %arg14) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %122 = "vhlo.reshape_v1"(%121) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %123 = "vhlo.add_v1"(%16, %122) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %124 = "vhlo.convert_v1"(%arg19) {mhlo.frontend_attributes = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding_per_value<[<@mesh, [{}]>]>">}>, mhlo.sharding = #vhlo.string_v1<"{replicated}">} : (!vhlo.tensor_v1<3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<3072x!vhlo.f32_v1>
    %125 = "vhlo.reshape_v1"(%124) : (!vhlo.tensor_v1<3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %126 = "vhlo.convert_v1"(%123) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %127 = "vhlo.power_v1"(%126, %5) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %128 = "vhlo.reduce_v1"(%127, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg23: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %175 = "vhlo.add_v1"(%arg22, %arg23) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%175) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %129 = "vhlo.multiply_v1"(%128, %3) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %130 = "vhlo.reshape_v1"(%129) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %131 = "vhlo.add_v1"(%130, %22) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %132 = "vhlo.rsqrt_v2"(%131) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %133 = "vhlo.reshape_v1"(%132) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %134 = "vhlo.broadcast_in_dim_v1"(%133) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %135 = "vhlo.multiply_v1"(%126, %134) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %136 = "vhlo.convert_v1"(%135) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %137 = "vhlo.convert_v1"(%136) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %138 = "vhlo.multiply_v1"(%125, %137) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %139 = "vhlo.convert_v1"(%138) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %140 = "vhlo.reshape_v1"(%139) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %141 = "vhlo.dot_general_v2"(%140, %arg20) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %142 = "vhlo.reshape_v1"(%141) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %143 = "vhlo.convert_v1"(%142) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %144 = "vhlo.logistic_v2"(%142) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %145 = "vhlo.convert_v1"(%144) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %146 = "vhlo.multiply_v1"(%143, %145) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %147 = "vhlo.convert_v1"(%146) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %148 = "vhlo.convert_v1"(%147) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %149 = "vhlo.dot_general_v2"(%140, %arg13) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %150 = "vhlo.reshape_v1"(%149) : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %151 = "vhlo.convert_v1"(%150) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %152 = "vhlo.multiply_v1"(%148, %151) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>
    %153 = "vhlo.convert_v1"(%152) : (!vhlo.tensor_v1<1x1x8192x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>
    %154 = "vhlo.reshape_v1"(%153) : (!vhlo.tensor_v1<1x1x8192x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>
    %155 = "vhlo.dot_general_v2"(%154, %arg12) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x8192x!vhlo.bf16_v1>, !vhlo.tensor_v1<8192x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %156 = "vhlo.reshape_v1"(%155) : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %157 = "vhlo.add_v1"(%123, %156) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %158 = "vhlo.convert_v1"(%157) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %159 = "vhlo.power_v1"(%158, %5) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %160 = "vhlo.reduce_v1"(%159, %1) <{dimensions = #vhlo.tensor_v1<dense<2> : tensor<1xi64>>}> ({
    ^bb0(%arg22: !vhlo.tensor_v1<!vhlo.f32_v1>, %arg23: !vhlo.tensor_v1<!vhlo.f32_v1>):
      %175 = "vhlo.add_v1"(%arg22, %arg23) : (!vhlo.tensor_v1<!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<!vhlo.f32_v1>
      "vhlo.return_v1"(%175) : (!vhlo.tensor_v1<!vhlo.f32_v1>) -> ()
    }) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %161 = "vhlo.multiply_v1"(%160, %3) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %162 = "vhlo.reshape_v1"(%161) : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %163 = "vhlo.add_v1"(%162, %22) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %164 = "vhlo.rsqrt_v2"(%163) <{result_accuracy = #vhlo.result_accuracy_v1<atol = 0.000000e+00, rtol = 0.000000e+00, ulps = 0, mode = #vhlo<result_accuracy_mode_v1 DEFAULT>>}> : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>
    %165 = "vhlo.reshape_v1"(%164) : (!vhlo.tensor_v1<1x1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x!vhlo.f32_v1>
    %166 = "vhlo.broadcast_in_dim_v1"(%165) <{broadcast_dimensions = #vhlo.tensor_v1<dense<[0, 1]> : tensor<2xi64>>}> : (!vhlo.tensor_v1<1x1x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %167 = "vhlo.multiply_v1"(%158, %166) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %168 = "vhlo.convert_v1"(%167) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %169 = "vhlo.convert_v1"(%168) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %170 = "vhlo.multiply_v1"(%70, %169) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>, !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>
    %171 = "vhlo.convert_v1"(%170) : (!vhlo.tensor_v1<1x1x3072x!vhlo.f32_v1>) -> !vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>
    %172 = "vhlo.reshape_v1"(%171) : (!vhlo.tensor_v1<1x1x3072x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>
    %173 = "vhlo.dot_general_v2"(%172, %arg11) <{accumulation_type = #vhlo.type_v1<!vhlo.none_v1>, allow_imprecise_accumulation = #vhlo.type_v1<!vhlo.none_v1>, lhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, lhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, lhs_contracting_dimensions = #vhlo.tensor_v1<dense<1> : tensor<1xi64>>, lhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>, num_primitive_operations = #vhlo.type_v1<!vhlo.none_v1>, precision_config = #vhlo.array_v1<[#vhlo<precision_v1 DEFAULT>, #vhlo<precision_v1 DEFAULT>]>, rhs_batching_dimensions = #vhlo.tensor_v1<dense<> : tensor<0xi64>>, rhs_component_count = #vhlo.type_v1<!vhlo.none_v1>, rhs_contracting_dimensions = #vhlo.tensor_v1<dense<0> : tensor<1xi64>>, rhs_precision_type = #vhlo.type_v1<!vhlo.none_v1>}> : (!vhlo.tensor_v1<1x3072x!vhlo.bf16_v1>, !vhlo.tensor_v1<3072x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>
    %174 = "vhlo.reshape_v1"(%173) : (!vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>) -> !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>
    "vhlo.return_v1"(%64, %68, %173, %174) : (!vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x8x128x128x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x128256x!vhlo.bf16_v1>, !vhlo.tensor_v1<1x1x128256x!vhlo.bf16_v1>) -> ()
  } {arg_attrs = #vhlo.array_v1<[#vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{\22_axis_0\22}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{devices=[2,1]<=[2]}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}, {}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, []>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}, {}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>, #vhlo.dict_v1<{#vhlo.string_v1<"mhlo.frontend_attributes"> = #vhlo.dict_v1<{#vhlo.string_v1<"xla.sdy.sharding"> = #vhlo.string_v1<"#sdy.sharding<@mesh, [{}]>">}>, #vhlo.string_v1<"mhlo.sharding"> = #vhlo.string_v1<"{replicated}">}>]>, res_attrs = #vhlo.array_v1<[]>, sym_visibility = #vhlo.string_v1<"">}
}
2025-08-13 23:25:45.618 (  43.696s) [        CA99B1C0]      module_builder.cc:188      1| SHLO Module:
module @SyncTensorsGraph.358 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0"=2]>
  func.func @main(%arg0: tensor<1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg1: tensor<1x64x1xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}]>}, %arg2: tensor<3072x1024xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg3: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg4: tensor<1x1xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg5: tensor<128256x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{"_axis_0"}, {}]>}, %arg6: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg7: tensor<i64> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg8: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}, %arg9: tensor<3072x1024xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg10: tensor<1x8x128x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}, {}, {}]>}, %arg11: tensor<3072x128256xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg12: tensor<8192x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg13: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg14: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg15: tensor<128xi64> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg16: tensor<1x128xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg17: tensor<f32> {sdy.sharding = #sdy.sharding<@mesh, []>}, %arg18: tensor<3072x3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg19: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}, %arg20: tensor<3072x8192xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}, {}]>}, %arg21: tensor<3072xbf16> {sdy.sharding = #sdy.sharding<@mesh, [{}]>}) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) {
    %c = stablehlo.constant dense<0> : tensor<1xi64>
    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
    %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
    %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
    %0 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
    %1 = stablehlo.compare  LT, %arg0, %c : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
    %2 = stablehlo.reshape %arg7 : (tensor<i64>) -> tensor<1xi64>
    %3 = stablehlo.add %arg0, %2 : tensor<1xi64>
    %4 = stablehlo.select %1, %3, %arg0 : tensor<1xi1>, tensor<1xi64>
    %5 = stablehlo.reshape %4 : (tensor<1xi64>) -> tensor<1x1xi64>
    %6 = stablehlo.convert %arg6 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %7 = stablehlo.reshape %6 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %8 = stablehlo.convert %arg4 : (tensor<1x1xi64>) -> tensor<1x1xui32>
    %9 = stablehlo.reshape %8 : (tensor<1x1xui32>) -> tensor<1xui32>
    %10 = "stablehlo.gather"(%arg5, %9) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<128256x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
    %11 = stablehlo.reshape %10 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %12 = stablehlo.convert %11 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %13 = stablehlo.power %12, %0 : tensor<1x1x3072xf32>
    %14 = stablehlo.reduce(%13 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %15 = stablehlo.multiply %14, %cst_1 : tensor<1x1xf32>
    %16 = stablehlo.reshape %15 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %17 = stablehlo.reshape %arg3 : (tensor<f32>) -> tensor<1x1x1xf32>
    %18 = stablehlo.add %16, %17 : tensor<1x1x1xf32>
    %19 = stablehlo.rsqrt %18 : tensor<1x1x1xf32>
    %20 = stablehlo.reshape %19 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %21 = stablehlo.broadcast_in_dim %20, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %22 = stablehlo.multiply %12, %21 : tensor<1x1x3072xf32>
    %23 = stablehlo.convert %22 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %24 = stablehlo.convert %23 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %25 = stablehlo.multiply %7, %24 : tensor<1x1x3072xf32>
    %26 = stablehlo.convert %25 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %27 = stablehlo.reshape %26 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %28 = stablehlo.dot_general %27, %arg2, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %29 = stablehlo.reshape %28 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %30 = stablehlo.convert %29 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %31 = stablehlo.convert %arg0 : (tensor<1xi64>) -> tensor<1xf32>
    %32 = stablehlo.reshape %31 : (tensor<1xf32>) -> tensor<1x1x1xf32>
    %33 = stablehlo.dot_general %arg1, %32, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %34 = stablehlo.reshape %33 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
    %35 = stablehlo.concatenate %34, %34, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
    %36 = stablehlo.cosine %35 : tensor<1x1x128xf32>
    %37 = stablehlo.convert %36 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %38 = stablehlo.convert %37 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %39 = stablehlo.broadcast_in_dim %38, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %40 = stablehlo.multiply %30, %39 : tensor<1x8x1x128xf32>
    %41 = stablehlo.convert %40 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %42 = stablehlo.slice %29 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %43 = stablehlo.negate %42 : tensor<1x8x1x64xbf16>
    %44 = stablehlo.slice %29 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
    %45 = stablehlo.concatenate %43, %44, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
    %46 = stablehlo.convert %45 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
    %47 = stablehlo.sine %35 : tensor<1x1x128xf32>
    %48 = stablehlo.convert %47 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
    %49 = stablehlo.convert %48 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
    %50 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
    %51 = stablehlo.multiply %46, %50 : tensor<1x8x1x128xf32>
    %52 = stablehlo.convert %51 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
    %53 = stablehlo.add %41, %52 : tensor<1x8x1x128xbf16>
    %54 = "stablehlo.scatter"(%arg8, %5, %53) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
      stablehlo.return %arg23 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %55 = sdy.sharding_constraint %54 <@mesh, [{}, {}, {}, {}]> : tensor<1x8x128x128xbf16>
    %56 = stablehlo.dot_general %27, %arg9, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %57 = stablehlo.reshape %56 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
    %58 = "stablehlo.scatter"(%arg10, %5, %57) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
    ^bb0(%arg22: tensor<bf16>, %arg23: tensor<bf16>):
      stablehlo.return %arg23 : tensor<bf16>
    }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
    %59 = sdy.sharding_constraint %58 <@mesh, [{}, {}, {}, {}]> : tensor<1x8x128x128xbf16>
    %60 = stablehlo.convert %arg21 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %61 = stablehlo.reshape %60 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %62 = stablehlo.dot_general %27, %arg18, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %63 = stablehlo.reshape %62 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
    %64 = stablehlo.convert %63 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %65 = stablehlo.broadcast_in_dim %38, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %66 = stablehlo.multiply %64, %65 : tensor<1x24x1x128xf32>
    %67 = stablehlo.convert %66 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %68 = stablehlo.slice %63 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %69 = stablehlo.negate %68 : tensor<1x24x1x64xbf16>
    %70 = stablehlo.slice %63 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
    %71 = stablehlo.concatenate %69, %70, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
    %72 = stablehlo.convert %71 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %73 = stablehlo.broadcast_in_dim %49, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
    %74 = stablehlo.multiply %72, %73 : tensor<1x24x1x128xf32>
    %75 = stablehlo.convert %74 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %76 = stablehlo.add %67, %75 : tensor<1x24x1x128xbf16>
    %77 = stablehlo.reshape %76 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %78 = stablehlo.broadcast_in_dim %54, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %79 = stablehlo.reshape %78 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %80 = stablehlo.transpose %79, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %81 = stablehlo.reshape %80 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %82 = stablehlo.dot_general %77, %81, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %83 = stablehlo.convert %82 : (tensor<24x1x128xbf16>) -> tensor<24x1x128xf32>
    %84 = stablehlo.reshape %83 : (tensor<24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %85 = stablehlo.broadcast_in_dim %arg17, dims = [] : (tensor<f32>) -> tensor<1x24x1x128xf32>
    %86 = stablehlo.multiply %84, %85 : tensor<1x24x1x128xf32>
    %87 = stablehlo.convert %86 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %88 = stablehlo.convert %arg16 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
    %89 = stablehlo.reshape %arg15 : (tensor<128xi64>) -> tensor<1x128xi64>
    %90 = stablehlo.broadcast_in_dim %arg0, dims = [0] : (tensor<1xi64>) -> tensor<1x128xi64>
    %91 = stablehlo.compare  GT, %89, %90 : (tensor<1x128xi64>, tensor<1x128xi64>) -> tensor<1x128xi1>
    %92 = stablehlo.convert %91 : (tensor<1x128xi1>) -> tensor<1x128xf32>
    %93 = stablehlo.multiply %88, %92 : tensor<1x128xf32>
    %94 = stablehlo.convert %93 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
    %95 = stablehlo.reshape %94 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16>
    %96 = stablehlo.broadcast_in_dim %95, dims = [0, 2, 3] : (tensor<1x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %97 = stablehlo.add %87, %96 : tensor<1x24x1x128xbf16>
    %98 = stablehlo.convert %97 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
    %99 = stablehlo.reduce(%98 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %100 = stablehlo.broadcast_in_dim %99, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %101 = stablehlo.subtract %98, %100 : tensor<1x24x1x128xf32>
    %102 = stablehlo.exponential %101 : tensor<1x24x1x128xf32>
    %103 = stablehlo.reduce(%102 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
    %104 = stablehlo.broadcast_in_dim %103, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
    %105 = stablehlo.divide %102, %104 : tensor<1x24x1x128xf32>
    %106 = stablehlo.convert %105 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
    %107 = stablehlo.reshape %106 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %108 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %109 = stablehlo.reshape %108 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
    %110 = stablehlo.dot_general %107, %109, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %111 = stablehlo.reshape %110 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
    %112 = stablehlo.dot_general %111, %arg14, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %113 = stablehlo.reshape %112 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %114 = stablehlo.add %11, %113 : tensor<1x1x3072xbf16>
    %115 = stablehlo.convert %arg19 : (tensor<3072xbf16>) -> tensor<3072xf32>
    %116 = stablehlo.reshape %115 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
    %117 = stablehlo.convert %114 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %118 = stablehlo.power %117, %0 : tensor<1x1x3072xf32>
    %119 = stablehlo.reduce(%118 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %120 = stablehlo.multiply %119, %cst_1 : tensor<1x1xf32>
    %121 = stablehlo.reshape %120 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %122 = stablehlo.add %121, %17 : tensor<1x1x1xf32>
    %123 = stablehlo.rsqrt %122 : tensor<1x1x1xf32>
    %124 = stablehlo.reshape %123 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %125 = stablehlo.broadcast_in_dim %124, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %126 = stablehlo.multiply %117, %125 : tensor<1x1x3072xf32>
    %127 = stablehlo.convert %126 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %128 = stablehlo.convert %127 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %129 = stablehlo.multiply %116, %128 : tensor<1x1x3072xf32>
    %130 = stablehlo.convert %129 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %131 = stablehlo.reshape %130 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %132 = stablehlo.dot_general %131, %arg20, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %133 = stablehlo.reshape %132 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %134 = stablehlo.convert %133 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %135 = stablehlo.logistic %133 : tensor<1x1x8192xbf16>
    %136 = stablehlo.convert %135 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %137 = stablehlo.multiply %134, %136 : tensor<1x1x8192xf32>
    %138 = stablehlo.convert %137 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %139 = stablehlo.convert %138 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
    %140 = stablehlo.dot_general %131, %arg13, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %141 = stablehlo.convert %140 : (tensor<1x8192xbf16>) -> tensor<1x8192xf32>
    %142 = stablehlo.reshape %141 : (tensor<1x8192xf32>) -> tensor<1x1x8192xf32>
    %143 = stablehlo.multiply %139, %142 : tensor<1x1x8192xf32>
    %144 = stablehlo.convert %143 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
    %145 = stablehlo.reshape %144 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
    %146 = stablehlo.dot_general %145, %arg12, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %147 = stablehlo.reshape %146 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %148 = stablehlo.add %114, %147 : tensor<1x1x3072xbf16>
    %149 = stablehlo.convert %148 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %150 = stablehlo.power %149, %0 : tensor<1x1x3072xf32>
    %151 = stablehlo.reduce(%150 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
    %152 = stablehlo.multiply %151, %cst_1 : tensor<1x1xf32>
    %153 = stablehlo.reshape %152 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
    %154 = stablehlo.add %153, %17 : tensor<1x1x1xf32>
    %155 = stablehlo.rsqrt %154 : tensor<1x1x1xf32>
    %156 = stablehlo.reshape %155 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
    %157 = stablehlo.broadcast_in_dim %156, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
    %158 = stablehlo.multiply %149, %157 : tensor<1x1x3072xf32>
    %159 = stablehlo.convert %158 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %160 = stablehlo.convert %159 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
    %161 = stablehlo.multiply %61, %160 : tensor<1x1x3072xf32>
    %162 = stablehlo.convert %161 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
    %163 = stablehlo.reshape %162 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
    %164 = stablehlo.dot_general %163, %arg11, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %165 = stablehlo.reshape %164 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %55, %59, %164, %165 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
2025-08-13 23:25:45.635 (  43.714s) [        CA99B1C0]      module_builder.cc:205      1| SHLO StableHLO Pipeline Module:
module @SyncTensorsGraph.358 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false} {
  sdy.mesh @mesh = <["_axis_0_updated"=1, "_axis_0"=2]>
  func.func @main(%arg0: tensor<1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<1x64x1xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<128xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0:4 = sdy.manual_computation(%arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21) in_shardings=[<@mesh, [{}]>, <@mesh, [{}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{"_axis_0"}, {}]>, <@mesh, [{}]>, <@mesh, []>, <@mesh, [{}, {}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, []>, <@mesh, [{}, {}]>, <@mesh, [{}]>, <@mesh, [{}, {}]>, <@mesh, [{}]>] out_shardings=[<@mesh, [{}, {}, {}, {}]>, <@mesh, [{}, {}, {}, {}]>, <@mesh, [{}, {}]>, <@mesh, [{}, {}, {}]>] manual_axes={"_axis_0_updated", "_axis_0"} (%arg22: tensor<1xi64>, %arg23: tensor<1x64x1xf32>, %arg24: tensor<3072x1024xbf16>, %arg25: tensor<f32>, %arg26: tensor<1x1xi64>, %arg27: tensor<64128x3072xbf16>, %arg28: tensor<3072xbf16>, %arg29: tensor<i64>, %arg30: tensor<1x8x128x128xbf16>, %arg31: tensor<3072x1024xbf16>, %arg32: tensor<1x8x128x128xbf16>, %arg33: tensor<3072x128256xbf16>, %arg34: tensor<8192x3072xbf16>, %arg35: tensor<3072x8192xbf16>, %arg36: tensor<3072x3072xbf16>, %arg37: tensor<128xi64>, %arg38: tensor<1x128xbf16>, %arg39: tensor<f32>, %arg40: tensor<3072x3072xbf16>, %arg41: tensor<3072xbf16>, %arg42: tensor<3072x8192xbf16>, %arg43: tensor<3072xbf16>) {
      %c = stablehlo.constant dense<0> : tensor<1xi64>
      %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>
      %cst_0 = stablehlo.constant dense<0xFF800000> : tensor<f32>
      %cst_1 = stablehlo.constant dense<3.25520843E-4> : tensor<1x1xf32>
      %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>
      %1 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1x1x3072xf32>
      %2 = stablehlo.compare  LT, %arg22, %c : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi1>
      %3 = stablehlo.reshape %arg29 : (tensor<i64>) -> tensor<1xi64>
      %4 = stablehlo.add %arg22, %3 : tensor<1xi64>
      %5 = stablehlo.select %2, %4, %arg22 : tensor<1xi1>, tensor<1xi64>
      %6 = stablehlo.reshape %5 : (tensor<1xi64>) -> tensor<1x1xi64>
      %7 = stablehlo.convert %arg28 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %8 = stablehlo.reshape %7 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
      %9 = stablehlo.convert %arg26 : (tensor<1x1xi64>) -> tensor<1x1xui32>
      %10 = stablehlo.reshape %9 : (tensor<1x1xui32>) -> tensor<1xui32>
      %11 = "stablehlo.gather"(%arg27, %10) <{dimension_numbers = #stablehlo.gather<offset_dims = [1], collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>, slice_sizes = array<i64: 1, 3072>}> : (tensor<64128x3072xbf16>, tensor<1xui32>) -> tensor<1x3072xbf16>
      %12 = stablehlo.reshape %11 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %13 = stablehlo.convert %12 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %14 = stablehlo.power %13, %1 : tensor<1x1x3072xf32>
      %15 = stablehlo.reduce(%14 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %16 = stablehlo.multiply %15, %cst_1 : tensor<1x1xf32>
      %17 = stablehlo.reshape %16 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %18 = stablehlo.reshape %arg25 : (tensor<f32>) -> tensor<1x1x1xf32>
      %19 = stablehlo.add %17, %18 : tensor<1x1x1xf32>
      %20 = stablehlo.rsqrt %19 : tensor<1x1x1xf32>
      %21 = stablehlo.reshape %20 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %22 = stablehlo.broadcast_in_dim %21, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %23 = stablehlo.multiply %13, %22 : tensor<1x1x3072xf32>
      %24 = stablehlo.convert %23 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %25 = stablehlo.convert %24 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %26 = stablehlo.multiply %8, %25 : tensor<1x1x3072xf32>
      %27 = stablehlo.convert %26 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %28 = stablehlo.reshape %27 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %29 = stablehlo.dot_general %28, %arg24, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
      %30 = stablehlo.reshape %29 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
      %31 = stablehlo.convert %30 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,8,1,128]{3,1,2,0}"} : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
      %32 = stablehlo.convert %arg22 : (tensor<1xi64>) -> tensor<1xf32>
      %33 = stablehlo.reshape %32 : (tensor<1xf32>) -> tensor<1x1x1xf32>
      %34 = stablehlo.dot_general %arg23, %33, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
      %35 = stablehlo.reshape %34 : (tensor<1x64x1xf32>) -> tensor<1x1x64xf32>
      %36 = stablehlo.concatenate %35, %35, dim = 2 : (tensor<1x1x64xf32>, tensor<1x1x64xf32>) -> tensor<1x1x128xf32>
      %37 = stablehlo.cosine %36 : tensor<1x1x128xf32>
      %38 = stablehlo.convert %37 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
      %39 = stablehlo.convert %38 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
      %40 = stablehlo.broadcast_in_dim %39, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
      %41 = stablehlo.multiply %31, %40 : tensor<1x8x1x128xf32>
      %42 = stablehlo.convert %41 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
      %43 = stablehlo.slice %30 [0:1, 0:8, 0:1, 64:128] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
      %44 = stablehlo.negate %43 : tensor<1x8x1x64xbf16>
      %45 = stablehlo.slice %30 [0:1, 0:8, 0:1, 0:64] : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x64xbf16>
      %46 = stablehlo.concatenate %44, %45, dim = 3 : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x128xbf16>
      %47 = stablehlo.convert %46 : (tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xf32>
      %48 = stablehlo.sine %36 : tensor<1x1x128xf32>
      %49 = stablehlo.convert %48 : (tensor<1x1x128xf32>) -> tensor<1x1x128xbf16>
      %50 = stablehlo.convert %49 : (tensor<1x1x128xbf16>) -> tensor<1x1x128xf32>
      %51 = stablehlo.broadcast_in_dim %50, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x8x1x128xf32>
      %52 = stablehlo.multiply %47, %51 : tensor<1x8x1x128xf32>
      %53 = stablehlo.convert %52 : (tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xbf16>
      %54 = stablehlo.add %42, %53 : tensor<1x8x1x128xbf16>
      %55 = "stablehlo.scatter"(%arg30, %6, %54) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg44: tensor<bf16>, %arg45: tensor<bf16>):
        stablehlo.return %arg45 : tensor<bf16>
      }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
      %56 = stablehlo.dot_general %28, %arg31, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
      %57 = stablehlo.reshape %56 : (tensor<1x1024xbf16>) -> tensor<1x8x1x128xbf16>
      %58 = "stablehlo.scatter"(%arg32, %6, %57) <{scatter_dimension_numbers = #stablehlo.scatter<update_window_dims = [0, 1, 3], inserted_window_dims = [2], scatter_dims_to_operand_dims = [2], index_vector_dim = 1>}> ({
      ^bb0(%arg44: tensor<bf16>, %arg45: tensor<bf16>):
        stablehlo.return %arg45 : tensor<bf16>
      }) : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>) -> tensor<1x8x128x128xbf16>
      %59 = stablehlo.convert %arg43 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %60 = stablehlo.reshape %59 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
      %61 = stablehlo.dot_general %28, %arg40, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
      %62 = stablehlo.reshape %61 : (tensor<1x3072xbf16>) -> tensor<1x24x1x128xbf16>
      %63 = stablehlo.convert %62 {result_layout = dense<[3, 1, 2, 0]> : tensor<4xindex>, xla_shape = "f32[1,24,1,128]{3,1,2,0}"} : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
      %64 = stablehlo.broadcast_in_dim %39, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
      %65 = stablehlo.multiply %63, %64 : tensor<1x24x1x128xf32>
      %66 = stablehlo.convert %65 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
      %67 = stablehlo.slice %62 [0:1, 0:24, 0:1, 64:128] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
      %68 = stablehlo.negate %67 : tensor<1x24x1x64xbf16>
      %69 = stablehlo.slice %62 [0:1, 0:24, 0:1, 0:64] : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x64xbf16>
      %70 = stablehlo.concatenate %68, %69, dim = 3 : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x128xbf16>
      %71 = stablehlo.convert %70 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
      %72 = stablehlo.broadcast_in_dim %50, dims = [0, 2, 3] : (tensor<1x1x128xf32>) -> tensor<1x24x1x128xf32>
      %73 = stablehlo.multiply %71, %72 : tensor<1x24x1x128xf32>
      %74 = stablehlo.convert %73 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
      %75 = stablehlo.add %66, %74 : tensor<1x24x1x128xbf16>
      %76 = stablehlo.reshape %75 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
      %77 = stablehlo.broadcast_in_dim %55, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
      %78 = stablehlo.reshape %77 : (tensor<1x8x3x128x128xbf16>) -> tensor<1x24x128x128xbf16>
      %79 = stablehlo.transpose %78, dims = [0, 1, 3, 2] {result_layout = dense<[2, 3, 1, 0]> : tensor<4xindex>, xla_shape = "bf16[1,24,128,128]{2,3,1,0}"} : (tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
      %80 = stablehlo.reshape %79 : (tensor<1x24x128x128xbf16>) -> tensor<24x128x128xbf16>
      %81 = stablehlo.dot_general %76, %80, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
      %82 = stablehlo.convert %81 : (tensor<24x1x128xbf16>) -> tensor<24x1x128xf32>
      %83 = stablehlo.reshape %82 : (tensor<24x1x128xf32>) -> tensor<1x24x1x128xf32>
      %84 = stablehlo.broadcast_in_dim %arg39, dims = [] : (tensor<f32>) -> tensor<1x24x1x128xf32>
      %85 = stablehlo.multiply %83, %84 : tensor<1x24x1x128xf32>
      %86 = stablehlo.convert %85 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
      %87 = stablehlo.convert %arg38 : (tensor<1x128xbf16>) -> tensor<1x128xf32>
      %88 = stablehlo.reshape %arg37 : (tensor<128xi64>) -> tensor<1x128xi64>
      %89 = stablehlo.broadcast_in_dim %arg22, dims = [0] : (tensor<1xi64>) -> tensor<1x128xi64>
      %90 = stablehlo.compare  GT, %88, %89 : (tensor<1x128xi64>, tensor<1x128xi64>) -> tensor<1x128xi1>
      %91 = stablehlo.convert %90 : (tensor<1x128xi1>) -> tensor<1x128xf32>
      %92 = stablehlo.multiply %87, %91 : tensor<1x128xf32>
      %93 = stablehlo.convert %92 : (tensor<1x128xf32>) -> tensor<1x128xbf16>
      %94 = stablehlo.reshape %93 : (tensor<1x128xbf16>) -> tensor<1x1x128xbf16>
      %95 = stablehlo.broadcast_in_dim %94, dims = [0, 2, 3] : (tensor<1x1x128xbf16>) -> tensor<1x24x1x128xbf16>
      %96 = stablehlo.add %86, %95 : tensor<1x24x1x128xbf16>
      %97 = stablehlo.convert %96 : (tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xf32>
      %98 = stablehlo.reduce(%97 init: %cst_0) applies stablehlo.maximum across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
      %99 = stablehlo.broadcast_in_dim %98, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
      %100 = stablehlo.subtract %97, %99 : tensor<1x24x1x128xf32>
      %101 = stablehlo.exponential %100 : tensor<1x24x1x128xf32>
      %102 = stablehlo.reduce(%101 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<1x24x1x128xf32>, tensor<f32>) -> tensor<1x24x1xf32>
      %103 = stablehlo.broadcast_in_dim %102, dims = [0, 1, 2] : (tensor<1x24x1xf32>) -> tensor<1x24x1x128xf32>
      %104 = stablehlo.divide %101, %103 : tensor<1x24x1x128xf32>
      %105 = stablehlo.convert %104 : (tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xbf16>
      %106 = stablehlo.reshape %105 : (tensor<1x24x1x128xbf16>) -> tensor<24x1x128xbf16>
      %107 = stablehlo.broadcast_in_dim %58, dims = [0, 1, 3, 4] : (tensor<1x8x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
      %108 = stablehlo.reshape %107 : (tensor<1x8x3x128x128xbf16>) -> tensor<24x128x128xbf16>
      %109 = stablehlo.dot_general %106, %108, batching_dims = [0] x [0], contracting_dims = [2] x [1] : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
      %110 = stablehlo.reshape %109 : (tensor<24x1x128xbf16>) -> tensor<1x3072xbf16>
      %111 = stablehlo.dot_general %110, %arg36, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
      %112 = stablehlo.reshape %111 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %113 = stablehlo.add %12, %112 : tensor<1x1x3072xbf16>
      %114 = stablehlo.convert %arg41 : (tensor<3072xbf16>) -> tensor<3072xf32>
      %115 = stablehlo.reshape %114 : (tensor<3072xf32>) -> tensor<1x1x3072xf32>
      %116 = stablehlo.convert %113 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %117 = stablehlo.power %116, %1 : tensor<1x1x3072xf32>
      %118 = stablehlo.reduce(%117 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %119 = stablehlo.multiply %118, %cst_1 : tensor<1x1xf32>
      %120 = stablehlo.reshape %119 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %121 = stablehlo.add %120, %18 : tensor<1x1x1xf32>
      %122 = stablehlo.rsqrt %121 : tensor<1x1x1xf32>
      %123 = stablehlo.reshape %122 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %124 = stablehlo.broadcast_in_dim %123, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %125 = stablehlo.multiply %116, %124 : tensor<1x1x3072xf32>
      %126 = stablehlo.convert %125 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %127 = stablehlo.convert %126 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %128 = stablehlo.multiply %115, %127 : tensor<1x1x3072xf32>
      %129 = stablehlo.convert %128 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %130 = stablehlo.reshape %129 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %131 = stablehlo.dot_general %130, %arg42, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
      %132 = stablehlo.reshape %131 : (tensor<1x8192xbf16>) -> tensor<1x1x8192xbf16>
      %133 = stablehlo.convert %132 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
      %134 = stablehlo.logistic %132 : tensor<1x1x8192xbf16>
      %135 = stablehlo.convert %134 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
      %136 = stablehlo.multiply %133, %135 : tensor<1x1x8192xf32>
      %137 = stablehlo.convert %136 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
      %138 = stablehlo.convert %137 : (tensor<1x1x8192xbf16>) -> tensor<1x1x8192xf32>
      %139 = stablehlo.dot_general %130, %arg35, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
      %140 = stablehlo.convert %139 : (tensor<1x8192xbf16>) -> tensor<1x8192xf32>
      %141 = stablehlo.reshape %140 : (tensor<1x8192xf32>) -> tensor<1x1x8192xf32>
      %142 = stablehlo.multiply %138, %141 : tensor<1x1x8192xf32>
      %143 = stablehlo.convert %142 : (tensor<1x1x8192xf32>) -> tensor<1x1x8192xbf16>
      %144 = stablehlo.reshape %143 : (tensor<1x1x8192xbf16>) -> tensor<1x8192xbf16>
      %145 = stablehlo.dot_general %144, %arg34, contracting_dims = [1] x [0] : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
      %146 = stablehlo.reshape %145 : (tensor<1x3072xbf16>) -> tensor<1x1x3072xbf16>
      %147 = stablehlo.add %113, %146 : tensor<1x1x3072xbf16>
      %148 = stablehlo.convert %147 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %149 = stablehlo.power %148, %1 : tensor<1x1x3072xf32>
      %150 = stablehlo.reduce(%149 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<1x1x3072xf32>, tensor<f32>) -> tensor<1x1xf32>
      %151 = stablehlo.multiply %150, %cst_1 : tensor<1x1xf32>
      %152 = stablehlo.reshape %151 : (tensor<1x1xf32>) -> tensor<1x1x1xf32>
      %153 = stablehlo.add %152, %18 : tensor<1x1x1xf32>
      %154 = stablehlo.rsqrt %153 : tensor<1x1x1xf32>
      %155 = stablehlo.reshape %154 : (tensor<1x1x1xf32>) -> tensor<1x1xf32>
      %156 = stablehlo.broadcast_in_dim %155, dims = [0, 1] : (tensor<1x1xf32>) -> tensor<1x1x3072xf32>
      %157 = stablehlo.multiply %148, %156 : tensor<1x1x3072xf32>
      %158 = stablehlo.convert %157 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %159 = stablehlo.convert %158 : (tensor<1x1x3072xbf16>) -> tensor<1x1x3072xf32>
      %160 = stablehlo.multiply %60, %159 : tensor<1x1x3072xf32>
      %161 = stablehlo.convert %160 : (tensor<1x1x3072xf32>) -> tensor<1x1x3072xbf16>
      %162 = stablehlo.reshape %161 : (tensor<1x1x3072xbf16>) -> tensor<1x3072xbf16>
      %163 = stablehlo.dot_general %162, %arg33, contracting_dims = [1] x [0] : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
      %164 = stablehlo.reshape %163 : (tensor<1x128256xbf16>) -> tensor<1x1x128256xbf16>
      sdy.return %55, %58, %163, %164 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
    } : (tensor<1xi64>, tensor<1x64x1xf32>, tensor<3072x1024xbf16>, tensor<f32>, tensor<1x1xi64>, tensor<128256x3072xbf16>, tensor<3072xbf16>, tensor<i64>, tensor<1x8x128x128xbf16>, tensor<3072x1024xbf16>, tensor<1x8x128x128xbf16>, tensor<3072x128256xbf16>, tensor<8192x3072xbf16>, tensor<3072x8192xbf16>, tensor<3072x3072xbf16>, tensor<128xi64>, tensor<1x128xbf16>, tensor<f32>, tensor<3072x3072xbf16>, tensor<3072xbf16>, tensor<3072x8192xbf16>, tensor<3072xbf16>) -> (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>)
    return %0#0, %0#1, %0#2, %0#3 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
2025-08-13 23:25:45.645 (  43.723s) [        CA99B1C0]      module_builder.cc:452      1| TTIR Module:
module @SyncTensorsGraph.358 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  func.func @main(%arg0: tensor<1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<1x64x1xf32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x1xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<i64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<3072x1024xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<128xi64> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<1x128xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<f32> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072x3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072x8192xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<3072xbf16> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
    %0 = ttir.empty() : tensor<1xi64>
    %1 = "ttir.mesh_shard"(%arg0, %0) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %2 = ttir.empty() : tensor<1x64x1xf32>
    %3 = "ttir.mesh_shard"(%arg1, %2) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x64x1xf32>, tensor<1x64x1xf32>) -> tensor<1x64x1xf32>
    %4 = ttir.empty() : tensor<3072x1024xbf16>
    %5 = "ttir.mesh_shard"(%arg2, %4) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x1024xbf16>, tensor<3072x1024xbf16>) -> tensor<3072x1024xbf16>
    %6 = ttir.empty() : tensor<f32>
    %7 = "ttir.mesh_shard"(%arg3, %6) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %8 = ttir.empty() : tensor<1x1xi64>
    %9 = "ttir.mesh_shard"(%arg4, %8) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %10 = ttir.empty() : tensor<64128x3072xbf16>
    %11 = "ttir.mesh_shard"(%arg5, %10) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16>, tensor<64128x3072xbf16>) -> tensor<64128x3072xbf16>
    %12 = ttir.empty() : tensor<3072xbf16>
    %13 = "ttir.mesh_shard"(%arg6, %12) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %14 = ttir.empty() : tensor<i64>
    %15 = "ttir.mesh_shard"(%arg7, %14) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<i64>, tensor<i64>) -> tensor<i64>
    %16 = ttir.empty() : tensor<1x8x128x128xbf16>
    %17 = "ttir.mesh_shard"(%arg8, %16) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %18 = ttir.empty() : tensor<3072x1024xbf16>
    %19 = "ttir.mesh_shard"(%arg9, %18) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x1024xbf16>, tensor<3072x1024xbf16>) -> tensor<3072x1024xbf16>
    %20 = ttir.empty() : tensor<1x8x128x128xbf16>
    %21 = "ttir.mesh_shard"(%arg10, %20) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %22 = ttir.empty() : tensor<3072x128256xbf16>
    %23 = "ttir.mesh_shard"(%arg11, %22) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x128256xbf16>, tensor<3072x128256xbf16>) -> tensor<3072x128256xbf16>
    %24 = ttir.empty() : tensor<8192x3072xbf16>
    %25 = "ttir.mesh_shard"(%arg12, %24) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16>, tensor<8192x3072xbf16>) -> tensor<8192x3072xbf16>
    %26 = ttir.empty() : tensor<3072x8192xbf16>
    %27 = "ttir.mesh_shard"(%arg13, %26) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16>, tensor<3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %28 = ttir.empty() : tensor<3072x3072xbf16>
    %29 = "ttir.mesh_shard"(%arg14, %28) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %30 = ttir.empty() : tensor<128xi64>
    %31 = "ttir.mesh_shard"(%arg15, %30) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128xi64>, tensor<128xi64>) -> tensor<128xi64>
    %32 = ttir.empty() : tensor<1x128xbf16>
    %33 = "ttir.mesh_shard"(%arg16, %32) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x128xbf16>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %34 = ttir.empty() : tensor<f32>
    %35 = "ttir.mesh_shard"(%arg17, %34) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %36 = ttir.empty() : tensor<3072x3072xbf16>
    %37 = "ttir.mesh_shard"(%arg18, %36) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<3072x3072xbf16>
    %38 = ttir.empty() : tensor<3072xbf16>
    %39 = "ttir.mesh_shard"(%arg19, %38) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %40 = ttir.empty() : tensor<3072x8192xbf16>
    %41 = "ttir.mesh_shard"(%arg20, %40) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16>, tensor<3072x8192xbf16>) -> tensor<3072x8192xbf16>
    %42 = ttir.empty() : tensor<3072xbf16>
    %43 = "ttir.mesh_shard"(%arg21, %42) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16>, tensor<3072xbf16>) -> tensor<3072xbf16>
    %44 = "ttir.constant"() <{value = dense<0> : tensor<1xi64>}> : () -> tensor<1xi64>
    %45 = "ttir.constant"() <{value = dense<0.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %46 = "ttir.constant"() <{value = dense<0xFF800000> : tensor<f32>}> : () -> tensor<f32>
    %47 = "ttir.constant"() <{value = dense<3.25520843E-4> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %48 = "ttir.constant"() <{value = dense<2.000000e+00> : tensor<f32>}> : () -> tensor<f32>
    %49 = ttir.empty() : tensor<1x1x1xf32>
    %50 = "ttir.reshape"(%48, %49) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %51 = ttir.empty() : tensor<1x1x3072xf32>
    %52 = "ttir.broadcast"(%50, %51) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %53 = ttir.empty() : tensor<1xi1>
    %54 = "ttir.lt"(%1, %44, %53) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi1>) -> tensor<1xi1>
    %55 = ttir.empty() : tensor<1xi64>
    %56 = "ttir.reshape"(%15, %55) <{shape = [1 : i32]}> : (tensor<i64>, tensor<1xi64>) -> tensor<1xi64>
    %57 = ttir.empty() : tensor<1xi64>
    %58 = "ttir.add"(%1, %56, %57) : (tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %59 = ttir.empty() : tensor<1xi64>
    %60 = "ttir.where"(%54, %58, %1, %59) : (tensor<1xi1>, tensor<1xi64>, tensor<1xi64>, tensor<1xi64>) -> tensor<1xi64>
    %61 = ttir.empty() : tensor<1x1xi64>
    %62 = "ttir.reshape"(%60, %61) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %63 = ttir.empty() : tensor<3072xf32>
    %64 = "ttir.typecast"(%13, %63) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %65 = ttir.empty() : tensor<1x1x3072xf32>
    %66 = "ttir.reshape"(%64, %65) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %67 = ttir.empty() : tensor<1x1xui32>
    %68 = "ttir.typecast"(%9, %67) <{conservative_folding = false}> : (tensor<1x1xi64>, tensor<1x1xui32>) -> tensor<1x1xui32>
    %69 = ttir.empty() : tensor<1xui32>
    %70 = "ttir.reshape"(%68, %69) <{shape = [1 : i32]}> : (tensor<1x1xui32>, tensor<1xui32>) -> tensor<1xui32>
    %71 = ttir.empty() : tensor<1x3072xbf16>
    %72 = "ttir.gather"(%11, %70, %71) <{collapsed_slice_dims = array<i64: 0>, index_vector_dim = 1 : si64, indices_are_sorted = false, offset_dims = array<i64: 1>, operand_batching_dims = array<i64>, slice_sizes = array<i64: 1, 3072>, start_index_map = array<i64: 0>, start_indices_batching_dims = array<i64>}> : (tensor<64128x3072xbf16>, tensor<1xui32>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %73 = ttir.empty() : tensor<1x1x3072xbf16>
    %74 = "ttir.reshape"(%72, %73) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %75 = ttir.empty() : tensor<1x1x3072xf32>
    %76 = "ttir.typecast"(%74, %75) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %77 = ttir.empty() : tensor<1x1x3072xf32>
    %78 = "ttir.pow"(%76, %52, %77) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %79 = ttir.empty() : tensor<1x1xf32>
    %80 = "ttir.sum"(%78, %79) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %81 = ttir.empty() : tensor<1x1xf32>
    %82 = "ttir.multiply"(%80, %47, %81) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %83 = ttir.empty() : tensor<1x1x1xf32>
    %84 = "ttir.reshape"(%82, %83) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %85 = ttir.empty() : tensor<1x1x1xf32>
    %86 = "ttir.reshape"(%7, %85) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %87 = ttir.empty() : tensor<1x1x1xf32>
    %88 = "ttir.add"(%84, %86, %87) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %89 = ttir.empty() : tensor<1x1x1xf32>
    %90 = "ttir.rsqrt"(%88, %89) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %91 = ttir.empty() : tensor<1x1xf32>
    %92 = "ttir.reshape"(%90, %91) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %93 = ttir.empty() : tensor<1x1x1xf32>
    %94 = "ttir.reshape"(%92, %93) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %95 = ttir.empty() : tensor<1x1x3072xf32>
    %96 = "ttir.broadcast"(%94, %95) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %97 = ttir.empty() : tensor<1x1x3072xf32>
    %98 = "ttir.multiply"(%76, %96, %97) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %99 = ttir.empty() : tensor<1x1x3072xbf16>
    %100 = "ttir.typecast"(%98, %99) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %101 = ttir.empty() : tensor<1x1x3072xf32>
    %102 = "ttir.typecast"(%100, %101) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %103 = ttir.empty() : tensor<1x1x3072xf32>
    %104 = "ttir.multiply"(%66, %102, %103) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %105 = ttir.empty() : tensor<1x1x3072xbf16>
    %106 = "ttir.typecast"(%104, %105) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %107 = ttir.empty() : tensor<1x3072xbf16>
    %108 = "ttir.reshape"(%106, %107) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %109 = "ttir.dot_general"(%108, %5) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %110 = ttir.empty() : tensor<1x8x1x128xbf16>
    %111 = "ttir.reshape"(%109, %110) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %112 = ttir.empty() : tensor<1x8x1x128xf32>
    %113 = "ttir.typecast"(%111, %112) <{conservative_folding = false}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %114 = ttir.empty() : tensor<1xf32>
    %115 = "ttir.typecast"(%1, %114) <{conservative_folding = false}> : (tensor<1xi64>, tensor<1xf32>) -> tensor<1xf32>
    %116 = ttir.empty() : tensor<1x1x1xf32>
    %117 = "ttir.reshape"(%115, %116) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %118 = "ttir.dot_general"(%3, %117) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<1x64x1xf32>, tensor<1x1x1xf32>) -> tensor<1x64x1xf32>
    %119 = ttir.empty() : tensor<1x1x64xf32>
    %120 = "ttir.reshape"(%118, %119) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32>, tensor<1x1x64xf32>) -> tensor<1x1x64xf32>
    %121 = ttir.empty() : tensor<1x1x128xf32>
    %122 = "ttir.concat"(%120, %120, %121) <{dim = 2 : si32}> : (tensor<1x1x64xf32>, tensor<1x1x64xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %123 = ttir.empty() : tensor<1x1x128xf32>
    %124 = "ttir.cos"(%122, %123) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %125 = ttir.empty() : tensor<1x1x128xbf16>
    %126 = "ttir.typecast"(%124, %125) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %127 = ttir.empty() : tensor<1x1x128xf32>
    %128 = "ttir.typecast"(%126, %127) <{conservative_folding = false}> : (tensor<1x1x128xbf16>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %129 = ttir.empty() : tensor<1x1x1x128xf32>
    %130 = "ttir.reshape"(%128, %129) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %131 = ttir.empty() : tensor<1x8x1x128xf32>
    %132 = "ttir.broadcast"(%130, %131) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %133 = ttir.empty() : tensor<1x8x1x128xf32>
    %134 = "ttir.multiply"(%113, %132, %133) : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %135 = ttir.empty() : tensor<1x8x1x128xbf16>
    %136 = "ttir.typecast"(%134, %135) <{conservative_folding = false}> : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %137 = ttir.empty() : tensor<1x8x1x64xbf16>
    %138 = "ttir.slice"(%111, %137) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
    %139 = ttir.empty() : tensor<1x8x1x64xbf16>
    %140 = "ttir.neg"(%138, %139) : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
    %141 = ttir.empty() : tensor<1x8x1x64xbf16>
    %142 = "ttir.slice"(%111, %141) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x64xbf16>) -> tensor<1x8x1x64xbf16>
    %143 = ttir.empty() : tensor<1x8x1x128xbf16>
    %144 = "ttir.concat"(%140, %142, %143) <{dim = 3 : si32}> : (tensor<1x8x1x64xbf16>, tensor<1x8x1x64xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %145 = ttir.empty() : tensor<1x8x1x128xf32>
    %146 = "ttir.typecast"(%144, %145) <{conservative_folding = false}> : (tensor<1x8x1x128xbf16>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %147 = ttir.empty() : tensor<1x1x128xf32>
    %148 = "ttir.sin"(%122, %147) : (tensor<1x1x128xf32>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %149 = ttir.empty() : tensor<1x1x128xbf16>
    %150 = "ttir.typecast"(%148, %149) <{conservative_folding = false}> : (tensor<1x1x128xf32>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %151 = ttir.empty() : tensor<1x1x128xf32>
    %152 = "ttir.typecast"(%150, %151) <{conservative_folding = false}> : (tensor<1x1x128xbf16>, tensor<1x1x128xf32>) -> tensor<1x1x128xf32>
    %153 = ttir.empty() : tensor<1x1x1x128xf32>
    %154 = "ttir.reshape"(%152, %153) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %155 = ttir.empty() : tensor<1x8x1x128xf32>
    %156 = "ttir.broadcast"(%154, %155) <{broadcast_dimensions = array<i64: 1, 8, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %157 = ttir.empty() : tensor<1x8x1x128xf32>
    %158 = "ttir.multiply"(%146, %156, %157) : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>) -> tensor<1x8x1x128xf32>
    %159 = ttir.empty() : tensor<1x8x1x128xbf16>
    %160 = "ttir.typecast"(%158, %159) <{conservative_folding = false}> : (tensor<1x8x1x128xf32>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %161 = ttir.empty() : tensor<1x8x1x128xbf16>
    %162 = "ttir.add"(%136, %160, %161) : (tensor<1x8x1x128xbf16>, tensor<1x8x1x128xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %163 = ttir.empty() : tensor<1x8x128x128xbf16>
    %164 = "ttir.scatter"(%17, %62, %162, %163) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %165 = "ttir.dot_general"(%108, %19) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x1024xbf16>) -> tensor<1x1024xbf16>
    %166 = ttir.empty() : tensor<1x8x1x128xbf16>
    %167 = "ttir.reshape"(%165, %166) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16>, tensor<1x8x1x128xbf16>) -> tensor<1x8x1x128xbf16>
    %168 = ttir.empty() : tensor<1x8x128x128xbf16>
    %169 = "ttir.scatter"(%21, %62, %167, %168) <{index_vector_dim = 1 : i32, indices_are_sorted = false, input_batching_dims = array<i32>, inserted_window_dims = array<i32: 2>, scatter_dims_to_operand_dims = array<i32: 2>, scatter_indices_batching_dims = array<i32>, unique_indices = false, update_window_dims = array<i32: 0, 1, 3>}> : (tensor<1x8x128x128xbf16>, tensor<1x1xi64>, tensor<1x8x1x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %170 = ttir.empty() : tensor<3072xf32>
    %171 = "ttir.typecast"(%43, %170) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %172 = ttir.empty() : tensor<1x1x3072xf32>
    %173 = "ttir.reshape"(%171, %172) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %174 = "ttir.dot_general"(%108, %37) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %175 = ttir.empty() : tensor<1x24x1x128xbf16>
    %176 = "ttir.reshape"(%174, %175) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x3072xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %177 = ttir.empty() : tensor<1x24x1x128xf32>
    %178 = "ttir.typecast"(%176, %177) <{conservative_folding = false}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %179 = ttir.empty() : tensor<1x1x1x128xf32>
    %180 = "ttir.reshape"(%128, %179) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %181 = ttir.empty() : tensor<1x24x1x128xf32>
    %182 = "ttir.broadcast"(%180, %181) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %183 = ttir.empty() : tensor<1x24x1x128xf32>
    %184 = "ttir.multiply"(%178, %182, %183) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %185 = ttir.empty() : tensor<1x24x1x128xbf16>
    %186 = "ttir.typecast"(%184, %185) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %187 = ttir.empty() : tensor<1x24x1x64xbf16>
    %188 = "ttir.slice"(%176, %187) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x64xbf16>
    %189 = ttir.empty() : tensor<1x24x1x64xbf16>
    %190 = "ttir.neg"(%188, %189) : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x64xbf16>
    %191 = ttir.empty() : tensor<1x24x1x64xbf16>
    %192 = "ttir.slice"(%176, %191) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x64xbf16>) -> tensor<1x24x1x64xbf16>
    %193 = ttir.empty() : tensor<1x24x1x128xbf16>
    %194 = "ttir.concat"(%190, %192, %193) <{dim = 3 : si32}> : (tensor<1x24x1x64xbf16>, tensor<1x24x1x64xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %195 = ttir.empty() : tensor<1x24x1x128xf32>
    %196 = "ttir.typecast"(%194, %195) <{conservative_folding = false}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %197 = ttir.empty() : tensor<1x1x1x128xf32>
    %198 = "ttir.reshape"(%152, %197) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32>, tensor<1x1x1x128xf32>) -> tensor<1x1x1x128xf32>
    %199 = ttir.empty() : tensor<1x24x1x128xf32>
    %200 = "ttir.broadcast"(%198, %199) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %201 = ttir.empty() : tensor<1x24x1x128xf32>
    %202 = "ttir.multiply"(%196, %200, %201) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %203 = ttir.empty() : tensor<1x24x1x128xbf16>
    %204 = "ttir.typecast"(%202, %203) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %205 = ttir.empty() : tensor<1x24x1x128xbf16>
    %206 = "ttir.add"(%186, %204, %205) : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %207 = ttir.empty() : tensor<24x1x128xbf16>
    %208 = "ttir.reshape"(%206, %207) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %209 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %210 = "ttir.reshape"(%164, %209) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %211 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %212 = "ttir.broadcast"(%210, %211) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %213 = ttir.empty() : tensor<1x24x128x128xbf16>
    %214 = "ttir.reshape"(%212, %213) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %215 = ttir.empty() : tensor<1x24x128x128xbf16>
    %216 = "ttir.permute"(%214, %215) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16>, tensor<1x24x128x128xbf16>) -> tensor<1x24x128x128xbf16>
    %217 = ttir.empty() : tensor<24x128x128xbf16>
    %218 = "ttir.reshape"(%216, %217) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %219 = "ttir.dot_general"(%208, %218) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %220 = ttir.empty() : tensor<24x1x128xf32>
    %221 = "ttir.typecast"(%219, %220) <{conservative_folding = false}> : (tensor<24x1x128xbf16>, tensor<24x1x128xf32>) -> tensor<24x1x128xf32>
    %222 = ttir.empty() : tensor<1x24x1x128xf32>
    %223 = "ttir.reshape"(%221, %222) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %224 = ttir.empty() : tensor<1x1x1x1xf32>
    %225 = "ttir.reshape"(%35, %224) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32>, tensor<1x1x1x1xf32>) -> tensor<1x1x1x1xf32>
    %226 = ttir.empty() : tensor<1x24x1x128xf32>
    %227 = "ttir.broadcast"(%225, %226) <{broadcast_dimensions = array<i64: 1, 24, 1, 128>}> : (tensor<1x1x1x1xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %228 = ttir.empty() : tensor<1x24x1x128xf32>
    %229 = "ttir.multiply"(%223, %227, %228) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %230 = ttir.empty() : tensor<1x24x1x128xbf16>
    %231 = "ttir.typecast"(%229, %230) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %232 = ttir.empty() : tensor<1x128xf32>
    %233 = "ttir.typecast"(%33, %232) <{conservative_folding = false}> : (tensor<1x128xbf16>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %234 = ttir.empty() : tensor<1x128xi64>
    %235 = "ttir.reshape"(%31, %234) <{shape = [1 : i32, 128 : i32]}> : (tensor<128xi64>, tensor<1x128xi64>) -> tensor<1x128xi64>
    %236 = ttir.empty() : tensor<1x1xi64>
    %237 = "ttir.reshape"(%1, %236) <{shape = [1 : i32, 1 : i32]}> : (tensor<1xi64>, tensor<1x1xi64>) -> tensor<1x1xi64>
    %238 = ttir.empty() : tensor<1x128xi64>
    %239 = "ttir.broadcast"(%237, %238) <{broadcast_dimensions = array<i64: 1, 128>}> : (tensor<1x1xi64>, tensor<1x128xi64>) -> tensor<1x128xi64>
    %240 = ttir.empty() : tensor<1x128xi1>
    %241 = "ttir.gt"(%235, %239, %240) : (tensor<1x128xi64>, tensor<1x128xi64>, tensor<1x128xi1>) -> tensor<1x128xi1>
    %242 = ttir.empty() : tensor<1x128xf32>
    %243 = "ttir.typecast"(%241, %242) <{conservative_folding = false}> : (tensor<1x128xi1>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %244 = ttir.empty() : tensor<1x128xf32>
    %245 = "ttir.multiply"(%233, %243, %244) : (tensor<1x128xf32>, tensor<1x128xf32>, tensor<1x128xf32>) -> tensor<1x128xf32>
    %246 = ttir.empty() : tensor<1x128xbf16>
    %247 = "ttir.typecast"(%245, %246) <{conservative_folding = false}> : (tensor<1x128xf32>, tensor<1x128xbf16>) -> tensor<1x128xbf16>
    %248 = ttir.empty() : tensor<1x1x128xbf16>
    %249 = "ttir.reshape"(%247, %248) <{shape = [1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128xbf16>, tensor<1x1x128xbf16>) -> tensor<1x1x128xbf16>
    %250 = ttir.empty() : tensor<1x1x1x128xbf16>
    %251 = "ttir.reshape"(%249, %250) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xbf16>, tensor<1x1x1x128xbf16>) -> tensor<1x1x1x128xbf16>
    %252 = ttir.empty() : tensor<1x24x1x128xbf16>
    %253 = "ttir.broadcast"(%251, %252) <{broadcast_dimensions = array<i64: 1, 24, 1, 1>}> : (tensor<1x1x1x128xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %254 = ttir.empty() : tensor<1x24x1x128xbf16>
    %255 = "ttir.add"(%231, %253, %254) : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %256 = ttir.empty() : tensor<1x24x1x128xf32>
    %257 = "ttir.typecast"(%255, %256) <{conservative_folding = false}> : (tensor<1x24x1x128xbf16>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %258 = ttir.empty() : tensor<1x24x1xf32>
    %259 = "ttir.max"(%257, %258) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1xf32>) -> tensor<1x24x1xf32>
    %260 = ttir.empty() : tensor<1x24x1x1xf32>
    %261 = "ttir.reshape"(%259, %260) <{shape = [1 : i32, 24 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1xf32>, tensor<1x24x1x1xf32>) -> tensor<1x24x1x1xf32>
    %262 = ttir.empty() : tensor<1x24x1x128xf32>
    %263 = "ttir.broadcast"(%261, %262) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x1x1xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %264 = ttir.empty() : tensor<1x24x1x128xf32>
    %265 = "ttir.subtract"(%257, %263, %264) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %266 = ttir.empty() : tensor<1x24x1x128xf32>
    %267 = "ttir.exp"(%265, %266) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %268 = ttir.empty() : tensor<1x24x1xf32>
    %269 = "ttir.sum"(%267, %268) <{dim_arg = [3 : i32], keep_dim = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1xf32>) -> tensor<1x24x1xf32>
    %270 = ttir.empty() : tensor<1x24x1x1xf32>
    %271 = "ttir.reshape"(%269, %270) <{shape = [1 : i32, 24 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1xf32>, tensor<1x24x1x1xf32>) -> tensor<1x24x1x1xf32>
    %272 = ttir.empty() : tensor<1x24x1x128xf32>
    %273 = "ttir.broadcast"(%271, %272) <{broadcast_dimensions = array<i64: 1, 1, 1, 128>}> : (tensor<1x24x1x1xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %274 = ttir.empty() : tensor<1x24x1x128xf32>
    %275 = "ttir.div"(%267, %273, %274) : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>, tensor<1x24x1x128xf32>) -> tensor<1x24x1x128xf32>
    %276 = ttir.empty() : tensor<1x24x1x128xbf16>
    %277 = "ttir.typecast"(%275, %276) <{conservative_folding = false}> : (tensor<1x24x1x128xf32>, tensor<1x24x1x128xbf16>) -> tensor<1x24x1x128xbf16>
    %278 = ttir.empty() : tensor<24x1x128xbf16>
    %279 = "ttir.reshape"(%277, %278) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x24x1x128xbf16>, tensor<24x1x128xbf16>) -> tensor<24x1x128xbf16>
    %280 = ttir.empty() : tensor<1x8x1x128x128xbf16>
    %281 = "ttir.reshape"(%169, %280) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16>, tensor<1x8x1x128x128xbf16>) -> tensor<1x8x1x128x128xbf16>
    %282 = ttir.empty() : tensor<1x8x3x128x128xbf16>
    %283 = "ttir.broadcast"(%281, %282) <{broadcast_dimensions = array<i64: 1, 1, 3, 1, 1>}> : (tensor<1x8x1x128x128xbf16>, tensor<1x8x3x128x128xbf16>) -> tensor<1x8x3x128x128xbf16>
    %284 = ttir.empty() : tensor<24x128x128xbf16>
    %285 = "ttir.reshape"(%283, %284) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x128x128xbf16>
    %286 = "ttir.dot_general"(%279, %285) <{batch_dims_lhs = array<i64: 0>, batch_dims_rhs = array<i64: 0>, contract_dims_lhs = array<i64: 2>, contract_dims_rhs = array<i64: 1>}> : (tensor<24x1x128xbf16>, tensor<24x128x128xbf16>) -> tensor<24x1x128xbf16>
    %287 = ttir.empty() : tensor<1x3072xbf16>
    %288 = "ttir.reshape"(%286, %287) <{shape = [1 : i32, 3072 : i32]}> : (tensor<24x1x128xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %289 = "ttir.dot_general"(%288, %29) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x3072xbf16>) -> tensor<1x3072xbf16>
    %290 = ttir.empty() : tensor<1x1x3072xbf16>
    %291 = "ttir.reshape"(%289, %290) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %292 = ttir.empty() : tensor<1x1x3072xbf16>
    %293 = "ttir.add"(%74, %291, %292) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %294 = ttir.empty() : tensor<3072xf32>
    %295 = "ttir.typecast"(%39, %294) <{conservative_folding = false}> : (tensor<3072xbf16>, tensor<3072xf32>) -> tensor<3072xf32>
    %296 = ttir.empty() : tensor<1x1x3072xf32>
    %297 = "ttir.reshape"(%295, %296) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %298 = ttir.empty() : tensor<1x1x3072xf32>
    %299 = "ttir.typecast"(%293, %298) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %300 = ttir.empty() : tensor<1x1x3072xf32>
    %301 = "ttir.pow"(%299, %52, %300) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %302 = ttir.empty() : tensor<1x1xf32>
    %303 = "ttir.sum"(%301, %302) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %304 = ttir.empty() : tensor<1x1xf32>
    %305 = "ttir.multiply"(%303, %47, %304) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %306 = ttir.empty() : tensor<1x1x1xf32>
    %307 = "ttir.reshape"(%305, %306) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %308 = ttir.empty() : tensor<1x1x1xf32>
    %309 = "ttir.add"(%307, %86, %308) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %310 = ttir.empty() : tensor<1x1x1xf32>
    %311 = "ttir.rsqrt"(%309, %310) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %312 = ttir.empty() : tensor<1x1xf32>
    %313 = "ttir.reshape"(%311, %312) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %314 = ttir.empty() : tensor<1x1x1xf32>
    %315 = "ttir.reshape"(%313, %314) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %316 = ttir.empty() : tensor<1x1x3072xf32>
    %317 = "ttir.broadcast"(%315, %316) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %318 = ttir.empty() : tensor<1x1x3072xf32>
    %319 = "ttir.multiply"(%299, %317, %318) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %320 = ttir.empty() : tensor<1x1x3072xbf16>
    %321 = "ttir.typecast"(%319, %320) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %322 = ttir.empty() : tensor<1x1x3072xf32>
    %323 = "ttir.typecast"(%321, %322) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %324 = ttir.empty() : tensor<1x1x3072xf32>
    %325 = "ttir.multiply"(%297, %323, %324) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %326 = ttir.empty() : tensor<1x1x3072xbf16>
    %327 = "ttir.typecast"(%325, %326) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %328 = ttir.empty() : tensor<1x3072xbf16>
    %329 = "ttir.reshape"(%327, %328) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %330 = "ttir.dot_general"(%329, %41) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %331 = ttir.empty() : tensor<1x1x8192xbf16>
    %332 = "ttir.reshape"(%330, %331) <{shape = [1 : i32, 1 : i32, 8192 : i32]}> : (tensor<1x8192xbf16>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %333 = ttir.empty() : tensor<1x1x8192xf32>
    %334 = "ttir.typecast"(%332, %333) <{conservative_folding = false}> : (tensor<1x1x8192xbf16>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %335 = ttir.empty() : tensor<1x1x8192xbf16>
    %336 = "ttir.sigmoid"(%332, %335) : (tensor<1x1x8192xbf16>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %337 = ttir.empty() : tensor<1x1x8192xf32>
    %338 = "ttir.typecast"(%336, %337) <{conservative_folding = false}> : (tensor<1x1x8192xbf16>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %339 = ttir.empty() : tensor<1x1x8192xf32>
    %340 = "ttir.multiply"(%334, %338, %339) : (tensor<1x1x8192xf32>, tensor<1x1x8192xf32>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %341 = ttir.empty() : tensor<1x1x8192xbf16>
    %342 = "ttir.typecast"(%340, %341) <{conservative_folding = false}> : (tensor<1x1x8192xf32>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %343 = ttir.empty() : tensor<1x1x8192xf32>
    %344 = "ttir.typecast"(%342, %343) <{conservative_folding = false}> : (tensor<1x1x8192xbf16>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %345 = "ttir.dot_general"(%329, %27) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x8192xbf16>) -> tensor<1x8192xbf16>
    %346 = ttir.empty() : tensor<1x8192xf32>
    %347 = "ttir.typecast"(%345, %346) <{conservative_folding = false}> : (tensor<1x8192xbf16>, tensor<1x8192xf32>) -> tensor<1x8192xf32>
    %348 = ttir.empty() : tensor<1x1x8192xf32>
    %349 = "ttir.reshape"(%347, %348) <{shape = [1 : i32, 1 : i32, 8192 : i32]}> : (tensor<1x8192xf32>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %350 = ttir.empty() : tensor<1x1x8192xf32>
    %351 = "ttir.multiply"(%344, %349, %350) : (tensor<1x1x8192xf32>, tensor<1x1x8192xf32>, tensor<1x1x8192xf32>) -> tensor<1x1x8192xf32>
    %352 = ttir.empty() : tensor<1x1x8192xbf16>
    %353 = "ttir.typecast"(%351, %352) <{conservative_folding = false}> : (tensor<1x1x8192xf32>, tensor<1x1x8192xbf16>) -> tensor<1x1x8192xbf16>
    %354 = ttir.empty() : tensor<1x8192xbf16>
    %355 = "ttir.reshape"(%353, %354) <{shape = [1 : i32, 8192 : i32]}> : (tensor<1x1x8192xbf16>, tensor<1x8192xbf16>) -> tensor<1x8192xbf16>
    %356 = "ttir.dot_general"(%355, %25) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x8192xbf16>, tensor<8192x3072xbf16>) -> tensor<1x3072xbf16>
    %357 = ttir.empty() : tensor<1x1x3072xbf16>
    %358 = "ttir.reshape"(%356, %357) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %359 = ttir.empty() : tensor<1x1x3072xbf16>
    %360 = "ttir.add"(%293, %358, %359) : (tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %361 = ttir.empty() : tensor<1x1x3072xf32>
    %362 = "ttir.typecast"(%360, %361) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %363 = ttir.empty() : tensor<1x1x3072xf32>
    %364 = "ttir.pow"(%362, %52, %363) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %365 = ttir.empty() : tensor<1x1xf32>
    %366 = "ttir.sum"(%364, %365) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %367 = ttir.empty() : tensor<1x1xf32>
    %368 = "ttir.multiply"(%366, %47, %367) : (tensor<1x1xf32>, tensor<1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %369 = ttir.empty() : tensor<1x1x1xf32>
    %370 = "ttir.reshape"(%368, %369) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %371 = ttir.empty() : tensor<1x1x1xf32>
    %372 = "ttir.add"(%370, %86, %371) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %373 = ttir.empty() : tensor<1x1x1xf32>
    %374 = "ttir.rsqrt"(%372, %373) : (tensor<1x1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %375 = ttir.empty() : tensor<1x1xf32>
    %376 = "ttir.reshape"(%374, %375) <{shape = [1 : i32, 1 : i32]}> : (tensor<1x1x1xf32>, tensor<1x1xf32>) -> tensor<1x1xf32>
    %377 = ttir.empty() : tensor<1x1x1xf32>
    %378 = "ttir.reshape"(%376, %377) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1xf32>, tensor<1x1x1xf32>) -> tensor<1x1x1xf32>
    %379 = ttir.empty() : tensor<1x1x3072xf32>
    %380 = "ttir.broadcast"(%378, %379) <{broadcast_dimensions = array<i64: 1, 1, 3072>}> : (tensor<1x1x1xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %381 = ttir.empty() : tensor<1x1x3072xf32>
    %382 = "ttir.multiply"(%362, %380, %381) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %383 = ttir.empty() : tensor<1x1x3072xbf16>
    %384 = "ttir.typecast"(%382, %383) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %385 = ttir.empty() : tensor<1x1x3072xf32>
    %386 = "ttir.typecast"(%384, %385) <{conservative_folding = false}> : (tensor<1x1x3072xbf16>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %387 = ttir.empty() : tensor<1x1x3072xf32>
    %388 = "ttir.multiply"(%173, %386, %387) : (tensor<1x1x3072xf32>, tensor<1x1x3072xf32>, tensor<1x1x3072xf32>) -> tensor<1x1x3072xf32>
    %389 = ttir.empty() : tensor<1x1x3072xbf16>
    %390 = "ttir.typecast"(%388, %389) <{conservative_folding = false}> : (tensor<1x1x3072xf32>, tensor<1x1x3072xbf16>) -> tensor<1x1x3072xbf16>
    %391 = ttir.empty() : tensor<1x3072xbf16>
    %392 = "ttir.reshape"(%390, %391) <{shape = [1 : i32, 3072 : i32]}> : (tensor<1x1x3072xbf16>, tensor<1x3072xbf16>) -> tensor<1x3072xbf16>
    %393 = "ttir.dot_general"(%392, %23) <{batch_dims_lhs = array<i64>, batch_dims_rhs = array<i64>, contract_dims_lhs = array<i64: 1>, contract_dims_rhs = array<i64: 0>}> : (tensor<1x3072xbf16>, tensor<3072x128256xbf16>) -> tensor<1x128256xbf16>
    %394 = ttir.empty() : tensor<1x1x128256xbf16>
    %395 = "ttir.reshape"(%393, %394) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16>, tensor<1x1x128256xbf16>) -> tensor<1x1x128256xbf16>
    %396 = ttir.empty() : tensor<1x8x128x128xbf16>
    %397 = "ttir.mesh_shard"(%164, %396) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %398 = ttir.empty() : tensor<1x8x128x128xbf16>
    %399 = "ttir.mesh_shard"(%169, %398) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>) -> tensor<1x8x128x128xbf16>
    %400 = ttir.empty() : tensor<1x128256xbf16>
    %401 = "ttir.mesh_shard"(%393, %400) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x128256xbf16>, tensor<1x128256xbf16>) -> tensor<1x128256xbf16>
    %402 = ttir.empty() : tensor<1x1x128256xbf16>
    %403 = "ttir.mesh_shard"(%395, %402) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16>, tensor<1x1x128256xbf16>) -> tensor<1x1x128256xbf16>
    return %397, %399, %401, %403 : tensor<1x8x128x128xbf16>, tensor<1x8x128x128xbf16>, tensor<1x128256xbf16>, tensor<1x1x128256xbf16>
  }
}
2025-08-13 23:25:45.656 (  43.735s) [        CA99B1C0]      module_builder.cc:506   WARN| `mhlo.num_partitions` attribute not found, assuming default number of partitions: 1
2025-08-13 23:25:45.656 (  43.735s) [        CA99B1C0]      module_builder.cc:520   WARN| `mhlo.num_replicas` attribute not found, assuming default number of replicas: 1
2025-08-13 23:25:45.656 (  43.735s) [        CA99B1C0]      module_builder.cc:528   WARN| Num replicas and num partitions are not set, inferring the number of devices from mesh shape
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.109")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.109")
CacheFillUpdatePattern: Successfully fusing ScatterOp into UpdateCacheOp
CacheFillUpdatePattern: Attempting to match ScatterOp at loc("scatter.132")
getCacheUpdatePositions: Checking ScatterOp at loc("scatter.132")
CacheFillUpdatePattern: Successfully fusing ScatterOp into UpdateCacheOp
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
TTIRFusingPass: Starting pass on operation
TTIRFusingPass: Successfully applied Conv2dTagWeights patterns
TTIRFusingPass: Successfully applied all fusion patterns
TTIRFusingPass: Completed pass on operation
2025-08-13 23:25:45.708 (  43.786s) [        CA99B1C0]      module_builder.cc:588      1| TTNN Module:
module @SyncTensorsGraph.358 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>} {
  ttcore.device_module {
    builtin.module @SyncTensorsGraph.358 attributes {mhlo.cross_program_prefetches = [], mhlo.input_output_alias = [], mhlo.is_dynamic = false, mhlo.use_auto_spmd_partitioning = false, ttcore.meshes = #ttcore.meshes<[<"mesh" = 1x2>]>, ttcore.system_desc = #ttcore.system_desc<[{role = host, target_triple = "x86_64-pc-linux"}], [{arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073184896, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}, {arch = <wormhole_b0>, grid = 8x8, coord_translation_offsets = 18x18, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 99744, erisc_l1_unreserved_base = 98304, dram_unreserved_base = 32, dram_unreserved_end = 1073193216, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>, <si32>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], dst_register_size_tiles = 8, num_cbs = 32, num_compute_threads = 1, num_datamovement_threads = 2}], [0, 1], [1 : i32, 0 : i32], [ 0x0x0x0]>} {
      ttcore.device @default_device = <workerGrid = #ttcore.grid<8x16, (d0, d1) -> (d1 floordiv 8, d0, d1 mod 2)>, l1Map = (d0, d1, d2)[s0] -> (d1 floordiv 8, d0, d1 mod 2, d2 + s0), dramMap = (d0, d1, d2)[s0, s1, s2, s3, s4, s5] -> (0, 0, (((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv s4) mod 12, ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) floordiv (s4 * 12) + ((d0 * s1) * (s2 * s3) + d1 * (s2 * s3) + d2) mod s4 + s5), meshShape = 1x2, chipIds = [0, 1]>
      func.func @main_const_eval_0() -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> attributes {const_eval} {
        %0 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %1 = "ttnn.full"(%0) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 2.000000e+00 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<>}> : (!ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %2 = "ttnn.reshape"(%1) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%1) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        return %2 : tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
      }
      func.func @main(%arg0: tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg1: tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg2: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg3: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg4: tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg5: tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg6: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg7: tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg8: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg9: tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg10: tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg11: tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg12: tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg13: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg14: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg15: tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg16: tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg17: tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg18: tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg19: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg20: tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}, %arg21: tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>> {ttcore.shard_status = #ttcore.shard_status<presharded>}) -> (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}, tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>> {ttcore.shard_status = #ttcore.shard_status<unsharded>}) {
        %0 = ttcore.load_cached(@main_const_eval_0, []) : () -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg7) <{force = false}> : (tensor<si32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %1 = "ttnn.get_device"() <{mesh_offset = #ttnn<mesh_offset 0x0>, mesh_shape = #ttnn<mesh_shape 1x2>}> : () -> !ttnn.device
        %2 = "ttnn.full"(%1) <{dtype = #ttcore.supportedDataTypes<f32>, fill_value = 3.25520843E-4 : f32, layout = #ttnn.layout<tile>, shape = #ttnn.shape<1x1>}> : (!ttnn.device) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %3 = "ttnn.mesh_shard"(%arg0, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %4 = "ttnn.mesh_shard"(%arg1, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg1) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %5 = "ttnn.mesh_shard"(%arg2, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg2) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %6 = "ttnn.mesh_shard"(%arg3, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg3) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %7 = "ttnn.mesh_shard"(%arg4, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg4) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %8 = "ttnn.mesh_shard"(%arg5, %1) <{shard_dims = array<i64: -1, 0>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 2, 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<64128x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2004x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg5) <{force = false}> : (tensor<128256x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<4008x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %9 = "ttnn.mesh_shard"(%arg6, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg6) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %10 = "ttnn.mesh_shard"(%arg8, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg8) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %11 = "ttnn.mesh_shard"(%arg9, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg9) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %12 = "ttnn.mesh_shard"(%arg10, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg10) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %13 = "ttnn.mesh_shard"(%arg11, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg11) <{force = false}> : (tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %14 = "ttnn.mesh_shard"(%arg12, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg12) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %15 = "ttnn.mesh_shard"(%arg13, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg13) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %16 = "ttnn.mesh_shard"(%arg14, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg14) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %17 = "ttnn.mesh_shard"(%arg15, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg15) <{force = false}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %18 = "ttnn.mesh_shard"(%arg16, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg16) <{force = false}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %19 = "ttnn.mesh_shard"(%arg17, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg17) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %20 = "ttnn.mesh_shard"(%arg18, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg18) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %21 = "ttnn.mesh_shard"(%arg19, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg19) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %22 = "ttnn.mesh_shard"(%arg20, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg20) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %23 = "ttnn.mesh_shard"(%arg21, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<full_to_shard>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<identity>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, !ttnn.device) -> tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg21) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %24 = "ttnn.typecast"(%9) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%9) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %25 = "ttnn.reshape"(%24) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%24) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %26 = "ttnn.typecast"(%7) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%7) <{force = false}> : (tensor<1x1xsi32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %27 = "ttnn.reshape"(%26) <{shape = [1 : i32]}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x1xui32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %28 = "ttnn.from_device"(%27) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%27) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %29 = "ttnn.to_layout"(%28) <{layout = #ttnn.layout<row_major>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%28) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<system_memory>>>>) -> ()
        %30 = "ttnn.to_device"(%29, %1) <{memory_config = #ttnn.memory_config<<dram>, <interleaved>>}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%29) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<system_memory>>>>) -> ()
        %31 = "ttnn.embedding"(%30, %8) : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<64128x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2004x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%30) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xui32, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%8) <{force = false}> : (tensor<64128x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<2004x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %32 = "ttnn.typecast"(%31) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %33 = "ttnn.reshape"(%32) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %34 = "ttnn.pow"(%33, %0) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %35 = "ttnn.sum"(%34) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%34) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %36 = "ttnn.multiply"(%35, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %37 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %38 = "ttnn.add"(%36, %37) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %39 = "ttnn.rsqrt"(%38) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %40 = "ttnn.multiply"(%32, %39) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%39) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%32) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %41 = "ttnn.multiply"(%25, %40) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %42 = "ttnn.typecast"(%41) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %43 = "ttnn.matmul"(%42, %5) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%5) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %44 = "ttnn.reshape"(%43) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %45 = "ttnn.typecast"(%44) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %46 = "ttnn.typecast"(%3) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %47 = "ttnn.reshape"(%46) <{shape = [1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%46) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %48 = "ttnn.matmul"(%4, %47) <{transpose_a = false, transpose_b = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%47) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%4) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %49 = "ttnn.reshape"(%48) <{shape = [1 : i32, 1 : i32, 64 : i32]}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x64x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 64 + d1, d2), <1x1>, memref<2x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %50 = "ttnn.concat"(%49, %49) <{dim = 2 : si32}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%49) <{force = false}> : (tensor<1x1x64xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x2x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %51 = "ttnn.cos"(%50) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %52 = "ttnn.reshape"(%51) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %53 = "ttnn.multiply"(%45, %52) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%52) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %54 = "ttnn.typecast"(%53) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%53) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %55 = "ttnn.slice"(%44) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %56 = "ttnn.neg"(%55) : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %57 = "ttnn.slice"(%44) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 8 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%44) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %58 = "ttnn.concat"(%56, %57) <{dim = 3 : si32}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x8x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %59 = "ttnn.typecast"(%58) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%58) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %60 = "ttnn.sin"(%50) : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%50) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %61 = "ttnn.reshape"(%60) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %62 = "ttnn.multiply"(%59, %61) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %63 = "ttnn.typecast"(%62) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x8x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %64 = "ttnn.add"(%54, %63) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%63) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%54) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %65 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.update_cache"(%10, %64, %65) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%65) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %66 = "ttnn.matmul"(%42, %11) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%11) <{force = false}> : (tensor<3072x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %67 = "ttnn.reshape"(%66) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32]}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%66) <{force = false}> : (tensor<1x1024xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x32x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %68 = "ttnn.typecast"(%arg0) <{dtype = #ttcore.supportedDataTypes<u32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%arg0) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.update_cache"(%12, %67, %68) <{batch_offset = 0 : i32}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%68) <{force = false}> : (tensor<1xui32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, u32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x8x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 * 32 + d2, d3), <1x1>, memref<8x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %69 = "ttnn.typecast"(%23) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%23) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %70 = "ttnn.reshape"(%69) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%69) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %71 = "ttnn.matmul"(%42, %20) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%42) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%20) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %72 = "ttnn.reshape"(%71) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %73 = "ttnn.typecast"(%71) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %74 = "ttnn.reshape"(%73) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%73) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %75 = "ttnn.multiply"(%74, %51) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%74) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%51) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %76 = "ttnn.typecast"(%75) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%75) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %77 = "ttnn.slice"(%72) <{begins = [0 : i32, 0 : i32, 0 : i32, 64 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 128 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %78 = "ttnn.neg"(%77) : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %79 = "ttnn.reshape"(%78) <{shape = [24 : i32, 1 : i32, 64 : i32]}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%78) <{force = false}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %80 = "ttnn.slice"(%72) <{begins = [0 : i32, 0 : i32, 0 : i32, 0 : i32], ends = [1 : i32, 24 : i32, 1 : i32, 64 : i32], step = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %81 = "ttnn.reshape"(%80) <{shape = [24 : i32, 1 : i32, 64 : i32]}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%80) <{force = false}> : (tensor<1x24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %82 = "ttnn.concat"(%79, %81) <{dim = 2 : si32}> : (tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%81) <{force = false}> : (tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%79) <{force = false}> : (tensor<24x1x64xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x2x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %83 = "ttnn.typecast"(%82) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%82) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %84 = "ttnn.multiply"(%83, %60) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%83) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %85 = "ttnn.typecast"(%84) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%84) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %86 = "ttnn.add"(%76, %85) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%85) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%76) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %87 = "ttnn.reshape"(%10) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %88 = "ttnn.repeat"(%87) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%87) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %89 = "ttnn.reshape"(%88) <{shape = [1 : i32, 24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%88) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %90 = "ttnn.permute"(%89) <{permutation = array<i64: 0, 1, 3, 2>}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%89) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %91 = "ttnn.reshape"(%90) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3072 + d1 * 128 + d2, d3), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %92 = "ttnn.matmul"(%86, %91) <{transpose_a = false, transpose_b = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%91) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%86) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %93 = "ttnn.typecast"(%92) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%92) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %94 = "ttnn.reshape"(%93) <{shape = [1 : i32, 24 : i32, 1 : i32, 128 : i32]}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%93) <{force = false}> : (tensor<24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %95 = "ttnn.reshape"(%19) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%19) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %96 = "ttnn.multiply"(%94, %95) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%95) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %97 = "ttnn.typecast"(%96) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%96) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %98 = "ttnn.typecast"(%18) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x128xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %99 = "ttnn.reshape"(%98) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%98) <{force = false}> : (tensor<1x128xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %100 = "ttnn.typecast"(%3) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%3) <{force = false}> : (tensor<1xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %101 = "ttnn.reshape"(%100) <{shape = [1 : i32, 1 : i32, 1 : i32, 1 : i32]}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%100) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %102 = "ttnn.typecast"(%17) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%17) <{force = false}> : (tensor<128xsi32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, si32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %103 = "ttnn.reshape"(%102) <{shape = [1 : i32, 1 : i32, 1 : i32, 128 : i32]}> : (tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%102) <{force = false}> : (tensor<128xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %104 = "ttnn.typecast"(%103) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%103) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %105 = "ttnn.typecast"(%101) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %106 = "ttnn.gt"(%104, %105) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%105) <{force = false}> : (tensor<1x1x1x1xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x1x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%104) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %107 = "ttnn.typecast"(%106) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%106) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %108 = "ttnn.multiply"(%99, %107) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%99) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %109 = "ttnn.typecast"(%108) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x1x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %110 = "ttnn.add"(%97, %109) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%109) <{force = false}> : (tensor<1x1x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 32 + d1 * 32 + d2, d3), <1x1>, memref<1x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%97) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %111 = "ttnn.typecast"(%110) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%110) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %112 = "ttnn.max"(%111) <{dim_arg = [3 : i32], keep_dim = true}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %113 = "ttnn.neg"(%112) : (tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%112) <{force = false}> : (tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %114 = "ttnn.add"(%111, %113) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%113) <{force = false}> : (tensor<1x24x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%111) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %115 = "ttnn.softmax"(%114) <{dimension = 3 : si32}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %116 = "ttnn.typecast"(%115) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x24x1x128xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %117 = "ttnn.reshape"(%116) <{shape = [24 : i32, 1 : i32, 128 : i32]}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%116) <{force = false}> : (tensor<1x24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 32 + d2, d3), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %118 = "ttnn.reshape"(%12) <{shape = [1 : i32, 8 : i32, 1 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %119 = "ttnn.repeat"(%118) <{repeat_dims = #ttnn.shape<1x1x3x1x1>}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%118) <{force = false}> : (tensor<1x8x1x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 1024 + d1 * 128 + d2 * 128 + d3, d4), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %120 = "ttnn.reshape"(%119) <{shape = [24 : i32, 128 : i32, 128 : i32]}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%119) <{force = false}> : (tensor<1x8x3x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3, d4) -> (d0 * 3072 + d1 * 384 + d2 * 128 + d3, d4), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %121 = "ttnn.matmul"(%117, %120) <{transpose_a = false, transpose_b = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%120) <{force = false}> : (tensor<24x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 128 + d1, d2), <1x1>, memref<96x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%117) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %122 = "ttnn.reshape"(%121) <{shape = [1 : i32, 3072 : i32]}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%121) <{force = false}> : (tensor<24x1x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<24x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %123 = "ttnn.matmul"(%122, %16) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%122) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%16) <{force = false}> : (tensor<3072x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %124 = "ttnn.add"(%31, %123) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%123) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%31) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %125 = "ttnn.typecast"(%21) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%21) <{force = false}> : (tensor<3072xbf16, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %126 = "ttnn.reshape"(%125) <{shape = [1 : i32, 3072 : i32]}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%125) <{force = false}> : (tensor<3072xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %127 = "ttnn.typecast"(%124) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %128 = "ttnn.reshape"(%127) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %129 = "ttnn.pow"(%128, %0) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%128) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %130 = "ttnn.sum"(%129) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%129) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %131 = "ttnn.multiply"(%130, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%130) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %132 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %133 = "ttnn.add"(%131, %132) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%132) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%131) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %134 = "ttnn.rsqrt"(%133) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%133) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %135 = "ttnn.multiply"(%127, %134) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%134) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%127) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %136 = "ttnn.multiply"(%126, %135) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%135) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%126) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %137 = "ttnn.typecast"(%136) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%136) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %138 = "ttnn.matmul"(%137, %22) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%22) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %139 = "ttnn.typecast"(%138) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %140 = "ttnn.sigmoid"(%138) : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%138) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %141 = "ttnn.typecast"(%140) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%140) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %142 = "ttnn.multiply"(%139, %141) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%141) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%139) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %143 = "ttnn.matmul"(%137, %15) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%137) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%15) <{force = false}> : (tensor<3072x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %144 = "ttnn.typecast"(%143) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%143) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %145 = "ttnn.multiply"(%142, %144) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%144) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%142) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %146 = "ttnn.typecast"(%145) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%145) <{force = false}> : (tensor<1x8192xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %147 = "ttnn.matmul"(%146, %14) <{transpose_a = false, transpose_b = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%146) <{force = false}> : (tensor<1x8192xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x256x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%14) <{force = false}> : (tensor<8192x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<256x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %148 = "ttnn.add"(%124, %147) <{output_dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%147) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%124) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %149 = "ttnn.typecast"(%148) <{dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%148) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %150 = "ttnn.reshape"(%149) <{shape = [1 : i32, 1 : i32, 3072 : i32]}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %151 = "ttnn.pow"(%150, %0) : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%150) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%0) <{force = false}> : (tensor<1x1x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %152 = "ttnn.sum"(%151) <{dim_arg = [2 : i32], keep_dim = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%151) <{force = false}> : (tensor<1x1x3072xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %153 = "ttnn.multiply"(%152, %2) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%152) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %154 = "ttnn.reshape"(%6) <{shape = [1 : i32, 1 : i32]}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%6) <{force = false}> : (tensor<f32, #ttnn.ttnn_layout<() -> (0, 0), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %155 = "ttnn.add"(%153, %154) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%154) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%153) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %156 = "ttnn.rsqrt"(%155) : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%155) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %157 = "ttnn.multiply"(%149, %156) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%156) <{force = false}> : (tensor<1x1xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%149) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %158 = "ttnn.multiply"(%70, %157) <{output_dtype = #ttcore.supportedDataTypes<f32>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%157) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %159 = "ttnn.typecast"(%158) <{dtype = #ttcore.supportedDataTypes<bf16>}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%158) <{force = false}> : (tensor<1x3072xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %160 = "ttnn.matmul"(%159, %13) <{transpose_a = false, transpose_b = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%159) <{force = false}> : (tensor<1x3072xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x96x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        "ttnn.deallocate"(%13) <{force = false}> : (tensor<3072x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<96x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %161 = "ttnn.reshape"(%160) <{shape = [1 : i32, 1 : i32, 128256 : i32]}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>
        %162 = "ttnn.to_layout"(%10) <{layout = #ttnn.layout<row_major>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %163 = "ttnn.from_device"(%162) : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%162) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %164 = "ttnn.mesh_shard"(%163, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%163) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %165 = "ttnn.to_layout"(%12) <{layout = #ttnn.layout<row_major>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<32x4x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %166 = "ttnn.from_device"(%165) : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%165) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %167 = "ttnn.mesh_shard"(%166, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%166) <{force = false}> : (tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %168 = "ttnn.to_layout"(%160) <{layout = #ttnn.layout<row_major>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%160) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %169 = "ttnn.from_device"(%168) : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%168) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %170 = "ttnn.mesh_shard"(%169, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%169) <{force = false}> : (tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        %171 = "ttnn.to_layout"(%161) <{layout = #ttnn.layout<row_major>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>
        "ttnn.deallocate"(%161) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 32 + d1, d2), <1x1>, memref<1x4008x!ttcore.tile<32x32, bf16>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %172 = "ttnn.from_device"(%171) : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%171) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
        %173 = "ttnn.mesh_shard"(%172, %1) <{shard_dims = array<i64: -1>, shard_direction = #ttcore.shard_direction<shard_to_full>, shard_shape = array<i64: 1>, shard_type = #ttcore.shard_type<replicate>}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, !ttnn.device) -> tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
        "ttnn.deallocate"(%172) <{force = false}> : (tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>) -> ()
        return %164, %167, %170, %173 : tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x8x128x128xbf16, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 * 128 + d2, d3), <1x1>, memref<1024x128xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x128256xbf16, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>, tensor<1x1x128256xbf16, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x128256xbf16, #ttnn.buffer_type<system_memory>>>>
      }
    }
  }
}
2025-08-13 23:25:45.739 (  43.817s) [        CA99B1C0]loaded_executable_insta:98       1| [LIFECYCLE] LoadedExecutableInstance constructor - instance created: 0x5572df905e10
2025-08-13 23:25:45.739 (  43.817s) [        CA99B1C0]loaded_executable_insta:516      1| LoadedExecutableInstance::PJRT_LoadedExecutable_GetExecutable
2025-08-13 23:25:45.739 (  43.817s) [        CA99B1C0]loaded_executable_insta:535      1| LoadedExecutableInstance::PJRT_LoadedExecutable_AddressableDevices
2025-08-13 23:25:45.739 (  43.818s) [        CA99B1C0]              stubs.inc:76    WARN| STUB: PJRT_Executable_GetCompiledMemoryStats
2025-08-13 23:25:45.739 (  43.818s) [        CA99B1C0]      error_instance.cc:49       1| ErrorInstance::PJRT_Error_Message
2025-08-13 23:25:45.739 (  43.818s) [        CA99B1C0]      error_instance.cc:58       1| ErrorInstance::PJRT_Error_GetCode
2025-08-13 23:25:45.739 (  43.818s) [        CA99B1C0]      error_instance.cc:43       1| ErrorInstance::PJRT_Error_Destroy
2025-08-13 23:25:45.739 (  43.818s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:45.739 (  43.818s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:45.749 (  43.827s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:45.749 (  43.827s) [        CA99B1C0] executable_instance.cc:106      1| ExecutableInstance::PJRT_Executable_OptimizedProgram
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]  device_description.cc:44       1| DeviceDescription::PJRT_DeviceDescription_Id
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        337FE640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        337FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        337FE640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        337FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        337FE640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        337FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.835s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        337FE640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.835s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        337FE640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        8AFFD640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        8AFFD640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        327FC640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     buffer_instance.cc:578      1| BufferInstance::PJRT_Buffer_Device
2025-08-13 23:25:45.757 (  43.836s) [        31FFB640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.757 (  43.836s) [        33FFF640]     device_instance.cc:53       1| DeviceInstance::PJRT_Device_IsAddressable
2025-08-13 23:25:45.758 (  43.836s) [        8AFFD640] executable_instance.cc:138      1| ExecutableInstance::PJRT_Executable_NumOutputs
2025-08-13 23:25:45.758 (  43.836s) [        8AFFD640]loaded_executable_insta:571      1| LoadedExecutableInstance::PJRT_LoadedExecutable_Execute
2025-08-13 23:25:45.758 (  43.836s) [        8AFFD640]loaded_executable_insta:95       1| LoadedExecutableInstance::Execute
2025-08-13 23:25:45.758 (  43.836s) [        8AFFD640]loaded_executable_insta:125      1| [DEVICE] Runtime device already opened, reusing existing device
2025-08-13 23:25:45.758 (  43.836s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 0)
2025-08-13 23:25:45.759 (  43.837s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 1)
2025-08-13 23:25:45.759 (  43.838s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 2)
2025-08-13 23:25:45.762 (  43.840s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 3)
2025-08-13 23:25:45.762 (  43.840s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 4)
2025-08-13 23:25:45.762 (  43.841s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 5)
2025-08-13 23:25:45.822 (  43.900s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 6)
2025-08-13 23:25:45.823 (  43.901s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 7)
2025-08-13 23:25:45.823 (  43.901s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 8)
2025-08-13 23:25:45.824 (  43.902s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 9)
2025-08-13 23:25:45.828 (  43.906s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 10)
2025-08-13 23:25:45.829 (  43.907s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 11)
2025-08-13 23:25:45.955 (  44.034s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 12)
2025-08-13 23:25:45.972 (  44.050s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 13)
2025-08-13 23:25:45.982 (  44.061s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 14)
2025-08-13 23:25:45.986 (  44.064s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 15)
2025-08-13 23:25:45.986 (  44.065s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 16)
2025-08-13 23:25:45.987 (  44.065s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 17)
2025-08-13 23:25:45.987 (  44.066s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 18)
2025-08-13 23:25:45.991 (  44.069s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 19)
2025-08-13 23:25:45.992 (  44.070s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 20)
2025-08-13 23:25:46.001 (  44.080s) [        8AFFD640]loaded_executable_insta:337      1| [LAYOUT] Converting layout for tensor handle 0x7ff048cb33d0 (arg 21)
2025-08-13 23:25:48.215 | critical |          Always | Can't get a single buffer from host storage distributed over mesh shape MeshShape([1, 2]) (assert.hpp:107)
2025-08-13 23:25:48.240 (  46.319s) [        8AFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-13 23:25:48.241 (  46.319s) [        8AFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-13 23:25:48.241 (  46.319s) [        8AFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
2025-08-13 23:25:48.241 (  46.319s) [        8AFFD640]     buffer_instance.cc:427      1| BufferInstance::PJRT_Buffer_Destroy
Fatal Python error: Segmentation fault

Thread 0x00007ff5227fc640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Thread 0x00007ffa1d85f640 (most recent call first):
  File "/usr/lib/python3.10/threading.py", line 324 in wait
  File "/usr/lib/python3.10/threading.py", line 607 in wait
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/tqdm/_monitor.py", line 60 in run
  File "/usr/lib/python3.10/threading.py", line 1016 in _bootstrap_inner
  File "/usr/lib/python3.10/threading.py", line 973 in _bootstrap

Current thread 0x00007ffbca99b1c0 (most recent call first):
  File "/localdev/jameszianxu/gen_mc/tt-torch/tt_torch/dynamo/experimental/xla_backend.py", line 920 in __call__
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838 in _fn
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/transformers/utils/generic.py", line 953 in wrapper
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762 in _call_impl
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751 in _wrapped_call_impl
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 655 in _fn
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762 in _call_impl
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751 in _wrapped_call_impl
  File "/localdev/jameszianxu/gen_mc/tt-torch/tests/models/llama/test_llama3_generative.py", line 217 in test_llama3_generate
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116 in decorate_context
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/python.py", line 157 in pytest_pyfunc_call
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/python.py", line 1671 in runtest
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/runner.py", line 178 in pytest_runtest_call
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/runner.py", line 246 in <lambda>
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/runner.py", line 344 in from_call
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/runner.py", line 245 in call_and_report
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/runner.py", line 136 in runtestprotocol
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/runner.py", line 117 in pytest_runtest_protocol
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/main.py", line 367 in pytest_runtestloop
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/main.py", line 343 in _main
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/main.py", line 289 in wrap_session
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/main.py", line 336 in pytest_cmdline_main
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_callers.py", line 121 in _multicall
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_manager.py", line 120 in _hookexec
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/pluggy/_hooks.py", line 512 in __call__
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/config/__init__.py", line 175 in main
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/lib/python3.10/site-packages/_pytest/config/__init__.py", line 201 in console_main
  File "/localdev/jameszianxu/gen_mc/tt-torch/env/venv/bin/pytest", line 7 in <module>

Extension modules: numpy._core._multiarray_umath, numpy.linalg._umath_linalg, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, charset_normalizer.md, requests.packages.charset_normalizer.md, requests.packages.chardet.md, yaml._yaml, markupsafe._speedups, regex._regex, PIL._imaging, sklearn.__check_build._check_build, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg._matfuncs_expm, scipy.linalg._linalg_pythran, scipy.linalg.cython_blas, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack, scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, psutil._psutil_linux, psutil._psutil_posix, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize._cython_nnls, scipy._lib._uarray._uarray, scipy.linalg._decomp_interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._dierckx, scipy.interpolate._ppoly, scipy.interpolate._interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.interpolate._bspl, scipy.special.cython_special, scipy.stats._stats, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._biasedurn, scipy.stats._stats_pythran, scipy.stats._levy_stable.levyst, scipy.stats._ansari_swilk_statistics, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.ndimage._nd_image, scipy.ndimage._rank_filter_1d, _ni_label, scipy.ndimage._ni_label, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.tslibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.tslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._libs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pandas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join, pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, _cyutility, sklearn._cyutility, sklearn.utils._isfinite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.preprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._base, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distances_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, sklearn.metrics._pairwise_fast, PIL._imagingft, jaxlib.cpu_feature_guard (total: 182)
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
